I0113 11:33:13.684124 25204 caffe.cpp:185] Using GPUs 0
I0113 11:33:13.854332 25204 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1430
test_interval: 1000
base_lr: 0.001
display: 200
max_iter: 70000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 10000
snapshot_prefix: "krnet_full"
solver_mode: GPU
device_id: 0
net: "krnet_full_train_test.prototxt"
snapshot_format: HDF5
I0113 11:33:13.854460 25204 solver.cpp:91] Creating training net from net file: krnet_full_train_test.prototxt
I0113 11:33:13.854929 25204 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0113 11:33:13.854946 25204 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0113 11:33:13.855139 25204 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "train_lmdb"
    batch_size: 100
    backend: LMDB
    prefetch: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0113 11:33:13.855247 25204 layer_factory.hpp:77] Creating layer cifar
I0113 11:33:13.856184 25204 net.cpp:106] Creating Layer cifar
I0113 11:33:13.856194 25204 net.cpp:411] cifar -> data
I0113 11:33:13.856209 25204 net.cpp:411] cifar -> label
I0113 11:33:13.856222 25204 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I0113 11:33:13.856719 25208 db_lmdb.cpp:38] Opened lmdb train_lmdb
I0113 11:33:13.869551 25204 data_layer.cpp:41] output data size: 100,3,100,100
I0113 11:33:13.882328 25204 net.cpp:150] Setting up cifar
I0113 11:33:13.882369 25204 net.cpp:157] Top shape: 100 3 100 100 (3000000)
I0113 11:33:13.882382 25204 net.cpp:157] Top shape: 100 (100)
I0113 11:33:13.882385 25204 net.cpp:165] Memory required for data: 12000400
I0113 11:33:13.882400 25204 layer_factory.hpp:77] Creating layer conv1
I0113 11:33:13.882427 25204 net.cpp:106] Creating Layer conv1
I0113 11:33:13.882433 25204 net.cpp:454] conv1 <- data
I0113 11:33:13.882452 25204 net.cpp:411] conv1 -> conv1
I0113 11:33:13.884836 25204 net.cpp:150] Setting up conv1
I0113 11:33:13.884865 25204 net.cpp:157] Top shape: 100 32 100 100 (32000000)
I0113 11:33:13.884868 25204 net.cpp:165] Memory required for data: 140000400
I0113 11:33:13.884891 25204 layer_factory.hpp:77] Creating layer pool1
I0113 11:33:13.884910 25204 net.cpp:106] Creating Layer pool1
I0113 11:33:13.884915 25204 net.cpp:454] pool1 <- conv1
I0113 11:33:13.884923 25204 net.cpp:411] pool1 -> pool1
I0113 11:33:13.884963 25204 net.cpp:150] Setting up pool1
I0113 11:33:13.884969 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.884971 25204 net.cpp:165] Memory required for data: 172000400
I0113 11:33:13.884974 25204 layer_factory.hpp:77] Creating layer relu1
I0113 11:33:13.884982 25204 net.cpp:106] Creating Layer relu1
I0113 11:33:13.884985 25204 net.cpp:454] relu1 <- pool1
I0113 11:33:13.884991 25204 net.cpp:397] relu1 -> pool1 (in-place)
I0113 11:33:13.884996 25204 net.cpp:150] Setting up relu1
I0113 11:33:13.885000 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.885002 25204 net.cpp:165] Memory required for data: 204000400
I0113 11:33:13.885005 25204 layer_factory.hpp:77] Creating layer norm1
I0113 11:33:13.885013 25204 net.cpp:106] Creating Layer norm1
I0113 11:33:13.885016 25204 net.cpp:454] norm1 <- pool1
I0113 11:33:13.885022 25204 net.cpp:411] norm1 -> norm1
I0113 11:33:13.885107 25204 net.cpp:150] Setting up norm1
I0113 11:33:13.885115 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.885118 25204 net.cpp:165] Memory required for data: 236000400
I0113 11:33:13.885121 25204 layer_factory.hpp:77] Creating layer conv2
I0113 11:33:13.885134 25204 net.cpp:106] Creating Layer conv2
I0113 11:33:13.885138 25204 net.cpp:454] conv2 <- norm1
I0113 11:33:13.885145 25204 net.cpp:411] conv2 -> conv2
I0113 11:33:13.888238 25204 net.cpp:150] Setting up conv2
I0113 11:33:13.888269 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.888273 25204 net.cpp:165] Memory required for data: 268000400
I0113 11:33:13.888291 25204 layer_factory.hpp:77] Creating layer relu2
I0113 11:33:13.888305 25204 net.cpp:106] Creating Layer relu2
I0113 11:33:13.888310 25204 net.cpp:454] relu2 <- conv2
I0113 11:33:13.888319 25204 net.cpp:397] relu2 -> conv2 (in-place)
I0113 11:33:13.888327 25204 net.cpp:150] Setting up relu2
I0113 11:33:13.888330 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.888334 25204 net.cpp:165] Memory required for data: 300000400
I0113 11:33:13.888335 25204 layer_factory.hpp:77] Creating layer pool2
I0113 11:33:13.888345 25204 net.cpp:106] Creating Layer pool2
I0113 11:33:13.888347 25204 net.cpp:454] pool2 <- conv2
I0113 11:33:13.888353 25204 net.cpp:411] pool2 -> pool2
I0113 11:33:13.888376 25204 net.cpp:150] Setting up pool2
I0113 11:33:13.888381 25204 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 11:33:13.888383 25204 net.cpp:165] Memory required for data: 308000400
I0113 11:33:13.888386 25204 layer_factory.hpp:77] Creating layer norm2
I0113 11:33:13.888399 25204 net.cpp:106] Creating Layer norm2
I0113 11:33:13.888402 25204 net.cpp:454] norm2 <- pool2
I0113 11:33:13.888408 25204 net.cpp:411] norm2 -> norm2
I0113 11:33:13.888483 25204 net.cpp:150] Setting up norm2
I0113 11:33:13.888489 25204 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 11:33:13.888491 25204 net.cpp:165] Memory required for data: 316000400
I0113 11:33:13.888494 25204 layer_factory.hpp:77] Creating layer conv3
I0113 11:33:13.888507 25204 net.cpp:106] Creating Layer conv3
I0113 11:33:13.888511 25204 net.cpp:454] conv3 <- norm2
I0113 11:33:13.888519 25204 net.cpp:411] conv3 -> conv3
I0113 11:33:13.895179 25204 net.cpp:150] Setting up conv3
I0113 11:33:13.895225 25204 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 11:33:13.895228 25204 net.cpp:165] Memory required for data: 332000400
I0113 11:33:13.895248 25204 layer_factory.hpp:77] Creating layer relu3
I0113 11:33:13.895264 25204 net.cpp:106] Creating Layer relu3
I0113 11:33:13.895270 25204 net.cpp:454] relu3 <- conv3
I0113 11:33:13.895278 25204 net.cpp:397] relu3 -> conv3 (in-place)
I0113 11:33:13.895287 25204 net.cpp:150] Setting up relu3
I0113 11:33:13.895290 25204 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 11:33:13.895293 25204 net.cpp:165] Memory required for data: 348000400
I0113 11:33:13.895295 25204 layer_factory.hpp:77] Creating layer pool3
I0113 11:33:13.895304 25204 net.cpp:106] Creating Layer pool3
I0113 11:33:13.895308 25204 net.cpp:454] pool3 <- conv3
I0113 11:33:13.895313 25204 net.cpp:411] pool3 -> pool3
I0113 11:33:13.895341 25204 net.cpp:150] Setting up pool3
I0113 11:33:13.895347 25204 net.cpp:157] Top shape: 100 64 12 12 (921600)
I0113 11:33:13.895349 25204 net.cpp:165] Memory required for data: 351686800
I0113 11:33:13.895351 25204 layer_factory.hpp:77] Creating layer ip1
I0113 11:33:13.895364 25204 net.cpp:106] Creating Layer ip1
I0113 11:33:13.895366 25204 net.cpp:454] ip1 <- pool3
I0113 11:33:13.895375 25204 net.cpp:411] ip1 -> ip1
I0113 11:33:13.898615 25204 net.cpp:150] Setting up ip1
I0113 11:33:13.898646 25204 net.cpp:157] Top shape: 100 3 (300)
I0113 11:33:13.898649 25204 net.cpp:165] Memory required for data: 351688000
I0113 11:33:13.898660 25204 layer_factory.hpp:77] Creating layer loss
I0113 11:33:13.898675 25204 net.cpp:106] Creating Layer loss
I0113 11:33:13.898680 25204 net.cpp:454] loss <- ip1
I0113 11:33:13.898689 25204 net.cpp:454] loss <- label
I0113 11:33:13.898695 25204 net.cpp:411] loss -> loss
I0113 11:33:13.898710 25204 layer_factory.hpp:77] Creating layer loss
I0113 11:33:13.898787 25204 net.cpp:150] Setting up loss
I0113 11:33:13.898794 25204 net.cpp:157] Top shape: (1)
I0113 11:33:13.898797 25204 net.cpp:160]     with loss weight 1
I0113 11:33:13.898825 25204 net.cpp:165] Memory required for data: 351688004
I0113 11:33:13.898829 25204 net.cpp:226] loss needs backward computation.
I0113 11:33:13.898833 25204 net.cpp:226] ip1 needs backward computation.
I0113 11:33:13.898836 25204 net.cpp:226] pool3 needs backward computation.
I0113 11:33:13.898839 25204 net.cpp:226] relu3 needs backward computation.
I0113 11:33:13.898843 25204 net.cpp:226] conv3 needs backward computation.
I0113 11:33:13.898845 25204 net.cpp:226] norm2 needs backward computation.
I0113 11:33:13.898847 25204 net.cpp:226] pool2 needs backward computation.
I0113 11:33:13.898850 25204 net.cpp:226] relu2 needs backward computation.
I0113 11:33:13.898854 25204 net.cpp:226] conv2 needs backward computation.
I0113 11:33:13.898856 25204 net.cpp:226] norm1 needs backward computation.
I0113 11:33:13.898859 25204 net.cpp:226] relu1 needs backward computation.
I0113 11:33:13.898861 25204 net.cpp:226] pool1 needs backward computation.
I0113 11:33:13.898864 25204 net.cpp:226] conv1 needs backward computation.
I0113 11:33:13.898867 25204 net.cpp:228] cifar does not need backward computation.
I0113 11:33:13.898869 25204 net.cpp:270] This network produces output loss
I0113 11:33:13.898885 25204 net.cpp:283] Network initialization done.
I0113 11:33:13.899329 25204 solver.cpp:181] Creating test net (#0) specified by net file: krnet_full_train_test.prototxt
I0113 11:33:13.899384 25204 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0113 11:33:13.899613 25204 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "test_lmdb"
    batch_size: 100
    backend: LMDB
    prefetch: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0113 11:33:13.899740 25204 layer_factory.hpp:77] Creating layer cifar
I0113 11:33:13.900499 25204 net.cpp:106] Creating Layer cifar
I0113 11:33:13.900512 25204 net.cpp:411] cifar -> data
I0113 11:33:13.900527 25204 net.cpp:411] cifar -> label
I0113 11:33:13.900535 25204 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I0113 11:33:13.901101 25210 db_lmdb.cpp:38] Opened lmdb test_lmdb
I0113 11:33:13.901600 25204 data_layer.cpp:41] output data size: 100,3,100,100
I0113 11:33:13.916446 25204 net.cpp:150] Setting up cifar
I0113 11:33:13.916497 25204 net.cpp:157] Top shape: 100 3 100 100 (3000000)
I0113 11:33:13.916507 25204 net.cpp:157] Top shape: 100 (100)
I0113 11:33:13.916512 25204 net.cpp:165] Memory required for data: 12000400
I0113 11:33:13.916522 25204 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0113 11:33:13.916548 25204 net.cpp:106] Creating Layer label_cifar_1_split
I0113 11:33:13.916556 25204 net.cpp:454] label_cifar_1_split <- label
I0113 11:33:13.916573 25204 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0113 11:33:13.916587 25204 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0113 11:33:13.916643 25204 net.cpp:150] Setting up label_cifar_1_split
I0113 11:33:13.916651 25204 net.cpp:157] Top shape: 100 (100)
I0113 11:33:13.916657 25204 net.cpp:157] Top shape: 100 (100)
I0113 11:33:13.916661 25204 net.cpp:165] Memory required for data: 12001200
I0113 11:33:13.916664 25204 layer_factory.hpp:77] Creating layer conv1
I0113 11:33:13.916687 25204 net.cpp:106] Creating Layer conv1
I0113 11:33:13.916693 25204 net.cpp:454] conv1 <- data
I0113 11:33:13.916704 25204 net.cpp:411] conv1 -> conv1
I0113 11:33:13.918571 25204 net.cpp:150] Setting up conv1
I0113 11:33:13.918638 25204 net.cpp:157] Top shape: 100 32 100 100 (32000000)
I0113 11:33:13.918644 25204 net.cpp:165] Memory required for data: 140001200
I0113 11:33:13.918676 25204 layer_factory.hpp:77] Creating layer pool1
I0113 11:33:13.918700 25204 net.cpp:106] Creating Layer pool1
I0113 11:33:13.918709 25204 net.cpp:454] pool1 <- conv1
I0113 11:33:13.918720 25204 net.cpp:411] pool1 -> pool1
I0113 11:33:13.918771 25204 net.cpp:150] Setting up pool1
I0113 11:33:13.918781 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.918784 25204 net.cpp:165] Memory required for data: 172001200
I0113 11:33:13.918789 25204 layer_factory.hpp:77] Creating layer relu1
I0113 11:33:13.918802 25204 net.cpp:106] Creating Layer relu1
I0113 11:33:13.918805 25204 net.cpp:454] relu1 <- pool1
I0113 11:33:13.918814 25204 net.cpp:397] relu1 -> pool1 (in-place)
I0113 11:33:13.918823 25204 net.cpp:150] Setting up relu1
I0113 11:33:13.918829 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.918833 25204 net.cpp:165] Memory required for data: 204001200
I0113 11:33:13.918836 25204 layer_factory.hpp:77] Creating layer norm1
I0113 11:33:13.918848 25204 net.cpp:106] Creating Layer norm1
I0113 11:33:13.918853 25204 net.cpp:454] norm1 <- pool1
I0113 11:33:13.918860 25204 net.cpp:411] norm1 -> norm1
I0113 11:33:13.918962 25204 net.cpp:150] Setting up norm1
I0113 11:33:13.918972 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.918975 25204 net.cpp:165] Memory required for data: 236001200
I0113 11:33:13.918979 25204 layer_factory.hpp:77] Creating layer conv2
I0113 11:33:13.918999 25204 net.cpp:106] Creating Layer conv2
I0113 11:33:13.919004 25204 net.cpp:454] conv2 <- norm1
I0113 11:33:13.919015 25204 net.cpp:411] conv2 -> conv2
I0113 11:33:13.923626 25204 net.cpp:150] Setting up conv2
I0113 11:33:13.923673 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.923678 25204 net.cpp:165] Memory required for data: 268001200
I0113 11:33:13.923707 25204 layer_factory.hpp:77] Creating layer relu2
I0113 11:33:13.923725 25204 net.cpp:106] Creating Layer relu2
I0113 11:33:13.923733 25204 net.cpp:454] relu2 <- conv2
I0113 11:33:13.923745 25204 net.cpp:397] relu2 -> conv2 (in-place)
I0113 11:33:13.923758 25204 net.cpp:150] Setting up relu2
I0113 11:33:13.923764 25204 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 11:33:13.923768 25204 net.cpp:165] Memory required for data: 300001200
I0113 11:33:13.923773 25204 layer_factory.hpp:77] Creating layer pool2
I0113 11:33:13.923789 25204 net.cpp:106] Creating Layer pool2
I0113 11:33:13.923792 25204 net.cpp:454] pool2 <- conv2
I0113 11:33:13.923801 25204 net.cpp:411] pool2 -> pool2
I0113 11:33:13.923833 25204 net.cpp:150] Setting up pool2
I0113 11:33:13.923844 25204 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 11:33:13.923847 25204 net.cpp:165] Memory required for data: 308001200
I0113 11:33:13.923852 25204 layer_factory.hpp:77] Creating layer norm2
I0113 11:33:13.923863 25204 net.cpp:106] Creating Layer norm2
I0113 11:33:13.923868 25204 net.cpp:454] norm2 <- pool2
I0113 11:33:13.923877 25204 net.cpp:411] norm2 -> norm2
I0113 11:33:13.923979 25204 net.cpp:150] Setting up norm2
I0113 11:33:13.923987 25204 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 11:33:13.923991 25204 net.cpp:165] Memory required for data: 316001200
I0113 11:33:13.923995 25204 layer_factory.hpp:77] Creating layer conv3
I0113 11:33:13.924013 25204 net.cpp:106] Creating Layer conv3
I0113 11:33:13.924018 25204 net.cpp:454] conv3 <- norm2
I0113 11:33:13.924028 25204 net.cpp:411] conv3 -> conv3
I0113 11:33:13.932903 25204 net.cpp:150] Setting up conv3
I0113 11:33:13.932952 25204 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 11:33:13.932956 25204 net.cpp:165] Memory required for data: 332001200
I0113 11:33:13.932984 25204 layer_factory.hpp:77] Creating layer relu3
I0113 11:33:13.933003 25204 net.cpp:106] Creating Layer relu3
I0113 11:33:13.933012 25204 net.cpp:454] relu3 <- conv3
I0113 11:33:13.933034 25204 net.cpp:397] relu3 -> conv3 (in-place)
I0113 11:33:13.933060 25204 net.cpp:150] Setting up relu3
I0113 11:33:13.933066 25204 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 11:33:13.933070 25204 net.cpp:165] Memory required for data: 348001200
I0113 11:33:13.933074 25204 layer_factory.hpp:77] Creating layer pool3
I0113 11:33:13.933086 25204 net.cpp:106] Creating Layer pool3
I0113 11:33:13.933090 25204 net.cpp:454] pool3 <- conv3
I0113 11:33:13.933099 25204 net.cpp:411] pool3 -> pool3
I0113 11:33:13.933130 25204 net.cpp:150] Setting up pool3
I0113 11:33:13.933138 25204 net.cpp:157] Top shape: 100 64 12 12 (921600)
I0113 11:33:13.933141 25204 net.cpp:165] Memory required for data: 351687600
I0113 11:33:13.933145 25204 layer_factory.hpp:77] Creating layer ip1
I0113 11:33:13.933159 25204 net.cpp:106] Creating Layer ip1
I0113 11:33:13.933164 25204 net.cpp:454] ip1 <- pool3
I0113 11:33:13.933174 25204 net.cpp:411] ip1 -> ip1
I0113 11:33:13.937963 25204 net.cpp:150] Setting up ip1
I0113 11:33:13.938012 25204 net.cpp:157] Top shape: 100 3 (300)
I0113 11:33:13.938016 25204 net.cpp:165] Memory required for data: 351688800
I0113 11:33:13.938032 25204 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I0113 11:33:13.938050 25204 net.cpp:106] Creating Layer ip1_ip1_0_split
I0113 11:33:13.938058 25204 net.cpp:454] ip1_ip1_0_split <- ip1
I0113 11:33:13.938072 25204 net.cpp:411] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0113 11:33:13.938087 25204 net.cpp:411] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0113 11:33:13.938128 25204 net.cpp:150] Setting up ip1_ip1_0_split
I0113 11:33:13.938136 25204 net.cpp:157] Top shape: 100 3 (300)
I0113 11:33:13.938143 25204 net.cpp:157] Top shape: 100 3 (300)
I0113 11:33:13.938145 25204 net.cpp:165] Memory required for data: 351691200
I0113 11:33:13.938150 25204 layer_factory.hpp:77] Creating layer accuracy
I0113 11:33:13.938166 25204 net.cpp:106] Creating Layer accuracy
I0113 11:33:13.938171 25204 net.cpp:454] accuracy <- ip1_ip1_0_split_0
I0113 11:33:13.938179 25204 net.cpp:454] accuracy <- label_cifar_1_split_0
I0113 11:33:13.938189 25204 net.cpp:411] accuracy -> accuracy
I0113 11:33:13.938200 25204 net.cpp:150] Setting up accuracy
I0113 11:33:13.938206 25204 net.cpp:157] Top shape: (1)
I0113 11:33:13.938210 25204 net.cpp:165] Memory required for data: 351691204
I0113 11:33:13.938215 25204 layer_factory.hpp:77] Creating layer loss
I0113 11:33:13.938237 25204 net.cpp:106] Creating Layer loss
I0113 11:33:13.938242 25204 net.cpp:454] loss <- ip1_ip1_0_split_1
I0113 11:33:13.938249 25204 net.cpp:454] loss <- label_cifar_1_split_1
I0113 11:33:13.938257 25204 net.cpp:411] loss -> loss
I0113 11:33:13.938273 25204 layer_factory.hpp:77] Creating layer loss
I0113 11:33:13.938364 25204 net.cpp:150] Setting up loss
I0113 11:33:13.938374 25204 net.cpp:157] Top shape: (1)
I0113 11:33:13.938377 25204 net.cpp:160]     with loss weight 1
I0113 11:33:13.938392 25204 net.cpp:165] Memory required for data: 351691208
I0113 11:33:13.938397 25204 net.cpp:226] loss needs backward computation.
I0113 11:33:13.938402 25204 net.cpp:228] accuracy does not need backward computation.
I0113 11:33:13.938407 25204 net.cpp:226] ip1_ip1_0_split needs backward computation.
I0113 11:33:13.938411 25204 net.cpp:226] ip1 needs backward computation.
I0113 11:33:13.938416 25204 net.cpp:226] pool3 needs backward computation.
I0113 11:33:13.938421 25204 net.cpp:226] relu3 needs backward computation.
I0113 11:33:13.938426 25204 net.cpp:226] conv3 needs backward computation.
I0113 11:33:13.938429 25204 net.cpp:226] norm2 needs backward computation.
I0113 11:33:13.938434 25204 net.cpp:226] pool2 needs backward computation.
I0113 11:33:13.938438 25204 net.cpp:226] relu2 needs backward computation.
I0113 11:33:13.938442 25204 net.cpp:226] conv2 needs backward computation.
I0113 11:33:13.938446 25204 net.cpp:226] norm1 needs backward computation.
I0113 11:33:13.938452 25204 net.cpp:226] relu1 needs backward computation.
I0113 11:33:13.938457 25204 net.cpp:226] pool1 needs backward computation.
I0113 11:33:13.938460 25204 net.cpp:226] conv1 needs backward computation.
I0113 11:33:13.938488 25204 net.cpp:228] label_cifar_1_split does not need backward computation.
I0113 11:33:13.938493 25204 net.cpp:228] cifar does not need backward computation.
I0113 11:33:13.938498 25204 net.cpp:270] This network produces output accuracy
I0113 11:33:13.938503 25204 net.cpp:270] This network produces output loss
I0113 11:33:13.938525 25204 net.cpp:283] Network initialization done.
I0113 11:33:13.938602 25204 solver.cpp:60] Solver scaffolding done.
I0113 11:33:13.938858 25204 caffe.cpp:213] Starting Optimization
I0113 11:33:13.938863 25204 solver.cpp:280] Solving CIFAR10_full
I0113 11:33:13.938870 25204 solver.cpp:281] Learning Rate Policy: fixed
I0113 11:33:13.939743 25204 solver.cpp:338] Iteration 0, Testing net (#0)
I0113 11:33:13.939915 25204 blocking_queue.cpp:50] Data layer prefetch queue empty
I0113 11:34:00.183156 25204 solver.cpp:406]     Test net output #0: accuracy = 0.172385
I0113 11:34:00.183248 25204 solver.cpp:406]     Test net output #1: loss = 1.09881 (* 1 = 1.09881 loss)
I0113 11:34:00.242408 25204 solver.cpp:229] Iteration 0, loss = 1.09903
I0113 11:34:00.242455 25204 solver.cpp:245]     Train net output #0: loss = 1.09903 (* 1 = 1.09903 loss)
I0113 11:34:00.242461 25204 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0113 11:34:21.044090 25204 solver.cpp:229] Iteration 200, loss = 0.76401
I0113 11:34:21.044134 25204 solver.cpp:245]     Train net output #0: loss = 0.76401 (* 1 = 0.76401 loss)
I0113 11:34:21.044142 25204 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I0113 11:34:41.887423 25204 solver.cpp:229] Iteration 400, loss = 0.633688
I0113 11:34:41.887495 25204 solver.cpp:245]     Train net output #0: loss = 0.633688 (* 1 = 0.633688 loss)
I0113 11:34:41.887502 25204 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I0113 11:35:02.698801 25204 solver.cpp:229] Iteration 600, loss = 0.7824
I0113 11:35:02.698842 25204 solver.cpp:245]     Train net output #0: loss = 0.7824 (* 1 = 0.7824 loss)
I0113 11:35:02.698848 25204 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I0113 11:35:23.518128 25204 solver.cpp:229] Iteration 800, loss = 1.09397
I0113 11:35:23.518227 25204 solver.cpp:245]     Train net output #0: loss = 1.09397 (* 1 = 1.09397 loss)
I0113 11:35:23.518235 25204 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I0113 11:35:44.230562 25204 solver.cpp:338] Iteration 1000, Testing net (#0)
I0113 11:36:30.541709 25204 solver.cpp:406]     Test net output #0: accuracy = 0.672412
I0113 11:36:30.541810 25204 solver.cpp:406]     Test net output #1: loss = 0.859814 (* 1 = 0.859814 loss)
I0113 11:36:30.597390 25204 solver.cpp:229] Iteration 1000, loss = 0.432636
I0113 11:36:30.597430 25204 solver.cpp:245]     Train net output #0: loss = 0.432636 (* 1 = 0.432636 loss)
I0113 11:36:30.597436 25204 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I0113 11:36:51.413396 25204 solver.cpp:229] Iteration 1200, loss = 1.13253
I0113 11:36:51.413439 25204 solver.cpp:245]     Train net output #0: loss = 1.13253 (* 1 = 1.13253 loss)
I0113 11:36:51.413444 25204 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I0113 11:37:12.232650 25204 solver.cpp:229] Iteration 1400, loss = 1.2668
I0113 11:37:12.232735 25204 solver.cpp:245]     Train net output #0: loss = 1.2668 (* 1 = 1.2668 loss)
I0113 11:37:12.232753 25204 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I0113 11:37:33.050191 25204 solver.cpp:229] Iteration 1600, loss = 0.434732
I0113 11:37:33.050235 25204 solver.cpp:245]     Train net output #0: loss = 0.434732 (* 1 = 0.434732 loss)
I0113 11:37:33.050240 25204 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I0113 11:37:53.870106 25204 solver.cpp:229] Iteration 1800, loss = 0.777617
I0113 11:37:53.870185 25204 solver.cpp:245]     Train net output #0: loss = 0.777617 (* 1 = 0.777617 loss)
I0113 11:37:53.870192 25204 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I0113 11:38:14.591205 25204 solver.cpp:338] Iteration 2000, Testing net (#0)
I0113 11:39:00.881834 25204 solver.cpp:406]     Test net output #0: accuracy = 0.674188
I0113 11:39:00.881959 25204 solver.cpp:406]     Test net output #1: loss = 0.885088 (* 1 = 0.885088 loss)
I0113 11:39:00.937527 25204 solver.cpp:229] Iteration 2000, loss = 1.27139
I0113 11:39:00.937566 25204 solver.cpp:245]     Train net output #0: loss = 1.27139 (* 1 = 1.27139 loss)
I0113 11:39:00.937572 25204 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I0113 11:39:21.752897 25204 solver.cpp:229] Iteration 2200, loss = 0.66388
I0113 11:39:21.752938 25204 solver.cpp:245]     Train net output #0: loss = 0.663879 (* 1 = 0.663879 loss)
I0113 11:39:21.752943 25204 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I0113 11:39:42.564780 25204 solver.cpp:229] Iteration 2400, loss = 0.95662
I0113 11:39:42.564867 25204 solver.cpp:245]     Train net output #0: loss = 0.956619 (* 1 = 0.956619 loss)
I0113 11:39:42.564873 25204 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I0113 11:40:03.374708 25204 solver.cpp:229] Iteration 2600, loss = 1.67961
I0113 11:40:03.374747 25204 solver.cpp:245]     Train net output #0: loss = 1.67961 (* 1 = 1.67961 loss)
I0113 11:40:03.374752 25204 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I0113 11:40:24.170241 25204 solver.cpp:229] Iteration 2800, loss = 0.14484
I0113 11:40:24.170333 25204 solver.cpp:245]     Train net output #0: loss = 0.144839 (* 1 = 0.144839 loss)
I0113 11:40:24.170339 25204 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I0113 11:40:44.881516 25204 solver.cpp:338] Iteration 3000, Testing net (#0)
I0113 11:41:31.177772 25204 solver.cpp:406]     Test net output #0: accuracy = 0.680552
I0113 11:41:31.177825 25204 solver.cpp:406]     Test net output #1: loss = 0.900041 (* 1 = 0.900041 loss)
I0113 11:41:31.233302 25204 solver.cpp:229] Iteration 3000, loss = 1.31616
I0113 11:41:31.233340 25204 solver.cpp:245]     Train net output #0: loss = 1.31615 (* 1 = 1.31615 loss)
I0113 11:41:31.233345 25204 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I0113 11:41:52.029417 25204 solver.cpp:229] Iteration 3200, loss = 1.32046
I0113 11:41:52.029458 25204 solver.cpp:245]     Train net output #0: loss = 1.32046 (* 1 = 1.32046 loss)
I0113 11:41:52.029462 25204 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I0113 11:42:12.824115 25204 solver.cpp:229] Iteration 3400, loss = 1.85573
I0113 11:42:12.824182 25204 solver.cpp:245]     Train net output #0: loss = 1.85573 (* 1 = 1.85573 loss)
I0113 11:42:12.824188 25204 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I0113 11:42:33.630096 25204 solver.cpp:229] Iteration 3600, loss = 1.17416
I0113 11:42:33.630136 25204 solver.cpp:245]     Train net output #0: loss = 1.17416 (* 1 = 1.17416 loss)
I0113 11:42:33.630142 25204 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I0113 11:42:54.438105 25204 solver.cpp:229] Iteration 3800, loss = 0.556069
I0113 11:42:54.438156 25204 solver.cpp:245]     Train net output #0: loss = 0.556067 (* 1 = 0.556067 loss)
I0113 11:42:54.438161 25204 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I0113 11:43:15.148458 25204 solver.cpp:338] Iteration 4000, Testing net (#0)
I0113 11:44:01.450661 25204 solver.cpp:406]     Test net output #0: accuracy = 0.675105
I0113 11:44:01.450721 25204 solver.cpp:406]     Test net output #1: loss = 0.888623 (* 1 = 0.888623 loss)
I0113 11:44:01.506260 25204 solver.cpp:229] Iteration 4000, loss = 1.84549
I0113 11:44:01.506295 25204 solver.cpp:245]     Train net output #0: loss = 1.84549 (* 1 = 1.84549 loss)
I0113 11:44:01.506300 25204 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I0113 11:44:22.316098 25204 solver.cpp:229] Iteration 4200, loss = 1.18542
I0113 11:44:22.316138 25204 solver.cpp:245]     Train net output #0: loss = 1.18542 (* 1 = 1.18542 loss)
I0113 11:44:22.316143 25204 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I0113 11:44:43.127975 25204 solver.cpp:229] Iteration 4400, loss = 0.225154
I0113 11:44:43.128026 25204 solver.cpp:245]     Train net output #0: loss = 0.225153 (* 1 = 0.225153 loss)
I0113 11:44:43.128032 25204 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I0113 11:45:03.947748 25204 solver.cpp:229] Iteration 4600, loss = 0.763072
I0113 11:45:03.947787 25204 solver.cpp:245]     Train net output #0: loss = 0.76307 (* 1 = 0.76307 loss)
I0113 11:45:03.947800 25204 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I0113 11:45:24.748944 25204 solver.cpp:229] Iteration 4800, loss = 0.40263
I0113 11:45:24.749030 25204 solver.cpp:245]     Train net output #0: loss = 0.402629 (* 1 = 0.402629 loss)
I0113 11:45:24.749037 25204 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I0113 11:45:45.460358 25204 solver.cpp:338] Iteration 5000, Testing net (#0)
I0113 11:46:31.757186 25204 solver.cpp:406]     Test net output #0: accuracy = 0.672223
I0113 11:46:31.757251 25204 solver.cpp:406]     Test net output #1: loss = 0.971139 (* 1 = 0.971139 loss)
I0113 11:46:31.812803 25204 solver.cpp:229] Iteration 5000, loss = 0.153583
I0113 11:46:31.812840 25204 solver.cpp:245]     Train net output #0: loss = 0.153581 (* 1 = 0.153581 loss)
I0113 11:46:31.812845 25204 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I0113 11:46:52.626796 25204 solver.cpp:229] Iteration 5200, loss = 1.39133
I0113 11:46:52.626834 25204 solver.cpp:245]     Train net output #0: loss = 1.39133 (* 1 = 1.39133 loss)
I0113 11:46:52.626840 25204 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I0113 11:47:13.445405 25204 solver.cpp:229] Iteration 5400, loss = 1.38308
I0113 11:47:13.445472 25204 solver.cpp:245]     Train net output #0: loss = 1.38308 (* 1 = 1.38308 loss)
I0113 11:47:13.445478 25204 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I0113 11:47:34.260972 25204 solver.cpp:229] Iteration 5600, loss = 1.61946
I0113 11:47:34.261010 25204 solver.cpp:245]     Train net output #0: loss = 1.61946 (* 1 = 1.61946 loss)
I0113 11:47:34.261016 25204 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I0113 11:47:55.075848 25204 solver.cpp:229] Iteration 5800, loss = 1.4822
I0113 11:47:55.075897 25204 solver.cpp:245]     Train net output #0: loss = 1.4822 (* 1 = 1.4822 loss)
I0113 11:47:55.075902 25204 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I0113 11:48:15.795123 25204 solver.cpp:338] Iteration 6000, Testing net (#0)
I0113 11:49:02.093447 25204 solver.cpp:406]     Test net output #0: accuracy = 0.676986
I0113 11:49:02.093499 25204 solver.cpp:406]     Test net output #1: loss = 0.857878 (* 1 = 0.857878 loss)
I0113 11:49:02.149063 25204 solver.cpp:229] Iteration 6000, loss = 1.34924
I0113 11:49:02.149101 25204 solver.cpp:245]     Train net output #0: loss = 1.34924 (* 1 = 1.34924 loss)
I0113 11:49:02.149106 25204 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I0113 11:49:22.965821 25204 solver.cpp:229] Iteration 6200, loss = 1.27116
I0113 11:49:22.965862 25204 solver.cpp:245]     Train net output #0: loss = 1.27116 (* 1 = 1.27116 loss)
I0113 11:49:22.965867 25204 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I0113 11:49:43.778908 25204 solver.cpp:229] Iteration 6400, loss = 0.928215
I0113 11:49:43.778980 25204 solver.cpp:245]     Train net output #0: loss = 0.928214 (* 1 = 0.928214 loss)
I0113 11:49:43.778985 25204 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I0113 11:50:04.602386 25204 solver.cpp:229] Iteration 6600, loss = 0.198583
I0113 11:50:04.602425 25204 solver.cpp:245]     Train net output #0: loss = 0.198582 (* 1 = 0.198582 loss)
I0113 11:50:04.602432 25204 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I0113 11:50:25.419668 25204 solver.cpp:229] Iteration 6800, loss = 1.38193
I0113 11:50:25.419718 25204 solver.cpp:245]     Train net output #0: loss = 1.38192 (* 1 = 1.38192 loss)
I0113 11:50:25.419723 25204 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I0113 11:50:46.134943 25204 solver.cpp:338] Iteration 7000, Testing net (#0)
I0113 11:51:32.439862 25204 solver.cpp:406]     Test net output #0: accuracy = 0.680769
I0113 11:51:32.439913 25204 solver.cpp:406]     Test net output #1: loss = 0.849162 (* 1 = 0.849162 loss)
I0113 11:51:32.495532 25204 solver.cpp:229] Iteration 7000, loss = 0.661956
I0113 11:51:32.495569 25204 solver.cpp:245]     Train net output #0: loss = 0.661955 (* 1 = 0.661955 loss)
I0113 11:51:32.495575 25204 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I0113 11:51:53.314272 25204 solver.cpp:229] Iteration 7200, loss = 0.686422
I0113 11:51:53.314311 25204 solver.cpp:245]     Train net output #0: loss = 0.68642 (* 1 = 0.68642 loss)
I0113 11:51:53.314328 25204 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I0113 11:52:14.131546 25204 solver.cpp:229] Iteration 7400, loss = 1.17844
I0113 11:52:14.131649 25204 solver.cpp:245]     Train net output #0: loss = 1.17844 (* 1 = 1.17844 loss)
I0113 11:52:14.131655 25204 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I0113 11:52:34.949159 25204 solver.cpp:229] Iteration 7600, loss = 1.64528
I0113 11:52:34.949200 25204 solver.cpp:245]     Train net output #0: loss = 1.64528 (* 1 = 1.64528 loss)
I0113 11:52:34.949206 25204 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I0113 11:52:55.766830 25204 solver.cpp:229] Iteration 7800, loss = 1.59846
I0113 11:52:55.766883 25204 solver.cpp:245]     Train net output #0: loss = 1.59846 (* 1 = 1.59846 loss)
I0113 11:52:55.766888 25204 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I0113 11:53:16.489193 25204 solver.cpp:338] Iteration 8000, Testing net (#0)
I0113 11:54:02.773838 25204 solver.cpp:406]     Test net output #0: accuracy = 0.67272
I0113 11:54:02.773898 25204 solver.cpp:406]     Test net output #1: loss = 0.858916 (* 1 = 0.858916 loss)
I0113 11:54:02.829382 25204 solver.cpp:229] Iteration 8000, loss = 0.957984
I0113 11:54:02.829419 25204 solver.cpp:245]     Train net output #0: loss = 0.957983 (* 1 = 0.957983 loss)
I0113 11:54:02.829424 25204 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I0113 11:54:23.629106 25204 solver.cpp:229] Iteration 8200, loss = 0.812128
I0113 11:54:23.629145 25204 solver.cpp:245]     Train net output #0: loss = 0.812127 (* 1 = 0.812127 loss)
I0113 11:54:23.629151 25204 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I0113 11:54:44.445365 25204 solver.cpp:229] Iteration 8400, loss = 1.61742
I0113 11:54:44.445415 25204 solver.cpp:245]     Train net output #0: loss = 1.61742 (* 1 = 1.61742 loss)
I0113 11:54:44.445420 25204 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I0113 11:55:05.260341 25204 solver.cpp:229] Iteration 8600, loss = 1.00743
I0113 11:55:05.260380 25204 solver.cpp:245]     Train net output #0: loss = 1.00743 (* 1 = 1.00743 loss)
I0113 11:55:05.260385 25204 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I0113 11:55:26.075446 25204 solver.cpp:229] Iteration 8800, loss = 0.219752
I0113 11:55:26.075497 25204 solver.cpp:245]     Train net output #0: loss = 0.21975 (* 1 = 0.21975 loss)
I0113 11:55:26.075502 25204 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I0113 11:55:46.790473 25204 solver.cpp:338] Iteration 9000, Testing net (#0)
I0113 11:56:33.072434 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673665
I0113 11:56:33.072486 25204 solver.cpp:406]     Test net output #1: loss = 0.881989 (* 1 = 0.881989 loss)
I0113 11:56:33.128077 25204 solver.cpp:229] Iteration 9000, loss = 0.669754
I0113 11:56:33.128115 25204 solver.cpp:245]     Train net output #0: loss = 0.669753 (* 1 = 0.669753 loss)
I0113 11:56:33.128121 25204 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I0113 11:56:53.950561 25204 solver.cpp:229] Iteration 9200, loss = 0.757956
I0113 11:56:53.950599 25204 solver.cpp:245]     Train net output #0: loss = 0.757955 (* 1 = 0.757955 loss)
I0113 11:56:53.950604 25204 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I0113 11:57:14.745302 25204 solver.cpp:229] Iteration 9400, loss = 0.214996
I0113 11:57:14.745353 25204 solver.cpp:245]     Train net output #0: loss = 0.214995 (* 1 = 0.214995 loss)
I0113 11:57:14.745359 25204 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I0113 11:57:35.562173 25204 solver.cpp:229] Iteration 9600, loss = 0.662532
I0113 11:57:35.562213 25204 solver.cpp:245]     Train net output #0: loss = 0.662531 (* 1 = 0.662531 loss)
I0113 11:57:35.562222 25204 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I0113 11:57:56.378825 25204 solver.cpp:229] Iteration 9800, loss = 0.482832
I0113 11:57:56.378896 25204 solver.cpp:245]     Train net output #0: loss = 0.482831 (* 1 = 0.482831 loss)
I0113 11:57:56.378902 25204 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I0113 11:58:17.089540 25204 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_10000.caffemodel.h5
I0113 11:58:17.138953 25204 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_10000.solverstate.h5
I0113 11:58:17.139828 25204 solver.cpp:338] Iteration 10000, Testing net (#0)
I0113 11:59:03.363567 25204 solver.cpp:406]     Test net output #0: accuracy = 0.679265
I0113 11:59:03.363672 25204 solver.cpp:406]     Test net output #1: loss = 0.941338 (* 1 = 0.941338 loss)
I0113 11:59:03.419196 25204 solver.cpp:229] Iteration 10000, loss = 0.377916
I0113 11:59:03.419236 25204 solver.cpp:245]     Train net output #0: loss = 0.377915 (* 1 = 0.377915 loss)
I0113 11:59:03.419244 25204 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I0113 11:59:24.240449 25204 solver.cpp:229] Iteration 10200, loss = 1.46274
I0113 11:59:24.240489 25204 solver.cpp:245]     Train net output #0: loss = 1.46274 (* 1 = 1.46274 loss)
I0113 11:59:24.240494 25204 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I0113 11:59:45.043182 25204 solver.cpp:229] Iteration 10400, loss = 0.478336
I0113 11:59:45.043236 25204 solver.cpp:245]     Train net output #0: loss = 0.478335 (* 1 = 0.478335 loss)
I0113 11:59:45.043242 25204 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I0113 12:00:05.867348 25204 solver.cpp:229] Iteration 10600, loss = 0.280358
I0113 12:00:05.867389 25204 solver.cpp:245]     Train net output #0: loss = 0.280358 (* 1 = 0.280358 loss)
I0113 12:00:05.867394 25204 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I0113 12:00:26.692204 25204 solver.cpp:229] Iteration 10800, loss = 1.31712
I0113 12:00:26.692255 25204 solver.cpp:245]     Train net output #0: loss = 1.31712 (* 1 = 1.31712 loss)
I0113 12:00:26.692260 25204 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I0113 12:00:47.392454 25204 solver.cpp:338] Iteration 11000, Testing net (#0)
I0113 12:01:33.674016 25204 solver.cpp:406]     Test net output #0: accuracy = 0.676573
I0113 12:01:33.674093 25204 solver.cpp:406]     Test net output #1: loss = 0.883985 (* 1 = 0.883985 loss)
I0113 12:01:33.729666 25204 solver.cpp:229] Iteration 11000, loss = 0.244123
I0113 12:01:33.729704 25204 solver.cpp:245]     Train net output #0: loss = 0.244122 (* 1 = 0.244122 loss)
I0113 12:01:33.729709 25204 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I0113 12:01:54.556083 25204 solver.cpp:229] Iteration 11200, loss = 0.881591
I0113 12:01:54.556123 25204 solver.cpp:245]     Train net output #0: loss = 0.88159 (* 1 = 0.88159 loss)
I0113 12:01:54.556128 25204 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I0113 12:02:15.384333 25204 solver.cpp:229] Iteration 11400, loss = 1.26203
I0113 12:02:15.384385 25204 solver.cpp:245]     Train net output #0: loss = 1.26203 (* 1 = 1.26203 loss)
I0113 12:02:15.384392 25204 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I0113 12:02:36.184824 25204 solver.cpp:229] Iteration 11600, loss = 0.239175
I0113 12:02:36.184864 25204 solver.cpp:245]     Train net output #0: loss = 0.239175 (* 1 = 0.239175 loss)
I0113 12:02:36.184870 25204 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I0113 12:02:57.003378 25204 solver.cpp:229] Iteration 11800, loss = 0.910864
I0113 12:02:57.003451 25204 solver.cpp:245]     Train net output #0: loss = 0.910864 (* 1 = 0.910864 loss)
I0113 12:02:57.003458 25204 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I0113 12:03:17.715771 25204 solver.cpp:338] Iteration 12000, Testing net (#0)
I0113 12:04:04.005141 25204 solver.cpp:406]     Test net output #0: accuracy = 0.671979
I0113 12:04:04.005197 25204 solver.cpp:406]     Test net output #1: loss = 0.859418 (* 1 = 0.859418 loss)
I0113 12:04:04.060694 25204 solver.cpp:229] Iteration 12000, loss = 0.701777
I0113 12:04:04.060735 25204 solver.cpp:245]     Train net output #0: loss = 0.701776 (* 1 = 0.701776 loss)
I0113 12:04:04.060741 25204 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I0113 12:04:24.870874 25204 solver.cpp:229] Iteration 12200, loss = 1.64259
I0113 12:04:24.870914 25204 solver.cpp:245]     Train net output #0: loss = 1.64259 (* 1 = 1.64259 loss)
I0113 12:04:24.870920 25204 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I0113 12:04:45.695590 25204 solver.cpp:229] Iteration 12400, loss = 1.18212
I0113 12:04:45.695704 25204 solver.cpp:245]     Train net output #0: loss = 1.18212 (* 1 = 1.18212 loss)
I0113 12:04:45.695711 25204 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I0113 12:05:06.487900 25204 solver.cpp:229] Iteration 12600, loss = 0.850788
I0113 12:05:06.487938 25204 solver.cpp:245]     Train net output #0: loss = 0.850788 (* 1 = 0.850788 loss)
I0113 12:05:06.487944 25204 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I0113 12:05:27.303678 25204 solver.cpp:229] Iteration 12800, loss = 0.306876
I0113 12:05:27.303731 25204 solver.cpp:245]     Train net output #0: loss = 0.306875 (* 1 = 0.306875 loss)
I0113 12:05:27.303736 25204 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I0113 12:05:48.018761 25204 solver.cpp:338] Iteration 13000, Testing net (#0)
I0113 12:06:34.301067 25204 solver.cpp:406]     Test net output #0: accuracy = 0.675406
I0113 12:06:34.301121 25204 solver.cpp:406]     Test net output #1: loss = 0.915181 (* 1 = 0.915181 loss)
I0113 12:06:34.356662 25204 solver.cpp:229] Iteration 13000, loss = 0.800185
I0113 12:06:34.356701 25204 solver.cpp:245]     Train net output #0: loss = 0.800184 (* 1 = 0.800184 loss)
I0113 12:06:34.356708 25204 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I0113 12:06:55.167011 25204 solver.cpp:229] Iteration 13200, loss = 0.268328
I0113 12:06:55.167052 25204 solver.cpp:245]     Train net output #0: loss = 0.268328 (* 1 = 0.268328 loss)
I0113 12:06:55.167057 25204 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I0113 12:07:15.991472 25204 solver.cpp:229] Iteration 13400, loss = 0.83572
I0113 12:07:15.991519 25204 solver.cpp:245]     Train net output #0: loss = 0.835719 (* 1 = 0.835719 loss)
I0113 12:07:15.991524 25204 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I0113 12:07:36.809201 25204 solver.cpp:229] Iteration 13600, loss = 1.36327
I0113 12:07:36.809244 25204 solver.cpp:245]     Train net output #0: loss = 1.36327 (* 1 = 1.36327 loss)
I0113 12:07:36.809250 25204 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I0113 12:07:57.605464 25204 solver.cpp:229] Iteration 13800, loss = 0.170425
I0113 12:07:57.605512 25204 solver.cpp:245]     Train net output #0: loss = 0.170424 (* 1 = 0.170424 loss)
I0113 12:07:57.605518 25204 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I0113 12:08:18.321147 25204 solver.cpp:338] Iteration 14000, Testing net (#0)
I0113 12:09:04.596127 25204 solver.cpp:406]     Test net output #0: accuracy = 0.681279
I0113 12:09:04.596210 25204 solver.cpp:406]     Test net output #1: loss = 0.870239 (* 1 = 0.870239 loss)
I0113 12:09:04.651684 25204 solver.cpp:229] Iteration 14000, loss = 1.63082
I0113 12:09:04.651722 25204 solver.cpp:245]     Train net output #0: loss = 1.63082 (* 1 = 1.63082 loss)
I0113 12:09:04.651728 25204 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I0113 12:09:25.475023 25204 solver.cpp:229] Iteration 14200, loss = 0.401074
I0113 12:09:25.475064 25204 solver.cpp:245]     Train net output #0: loss = 0.401073 (* 1 = 0.401073 loss)
I0113 12:09:25.475070 25204 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I0113 12:09:46.295914 25204 solver.cpp:229] Iteration 14400, loss = 0.16215
I0113 12:09:46.295987 25204 solver.cpp:245]     Train net output #0: loss = 0.162149 (* 1 = 0.162149 loss)
I0113 12:09:46.295994 25204 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I0113 12:10:07.121275 25204 solver.cpp:229] Iteration 14600, loss = 0.429521
I0113 12:10:07.121316 25204 solver.cpp:245]     Train net output #0: loss = 0.429521 (* 1 = 0.429521 loss)
I0113 12:10:07.121321 25204 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I0113 12:10:27.912482 25204 solver.cpp:229] Iteration 14800, loss = 1.39745
I0113 12:10:27.912554 25204 solver.cpp:245]     Train net output #0: loss = 1.39745 (* 1 = 1.39745 loss)
I0113 12:10:27.912560 25204 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I0113 12:10:48.619503 25204 solver.cpp:338] Iteration 15000, Testing net (#0)
I0113 12:11:34.883755 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673559
I0113 12:11:34.883867 25204 solver.cpp:406]     Test net output #1: loss = 0.865994 (* 1 = 0.865994 loss)
I0113 12:11:34.939458 25204 solver.cpp:229] Iteration 15000, loss = 0.324797
I0113 12:11:34.939498 25204 solver.cpp:245]     Train net output #0: loss = 0.324796 (* 1 = 0.324796 loss)
I0113 12:11:34.939504 25204 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I0113 12:11:55.762665 25204 solver.cpp:229] Iteration 15200, loss = 1.14929
I0113 12:11:55.762707 25204 solver.cpp:245]     Train net output #0: loss = 1.14928 (* 1 = 1.14928 loss)
I0113 12:11:55.762712 25204 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I0113 12:12:16.564625 25204 solver.cpp:229] Iteration 15400, loss = 0.401208
I0113 12:12:16.564679 25204 solver.cpp:245]     Train net output #0: loss = 0.401207 (* 1 = 0.401207 loss)
I0113 12:12:16.564685 25204 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I0113 12:12:37.391383 25204 solver.cpp:229] Iteration 15600, loss = 0.619149
I0113 12:12:37.391424 25204 solver.cpp:245]     Train net output #0: loss = 0.619148 (* 1 = 0.619148 loss)
I0113 12:12:37.391430 25204 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I0113 12:12:58.209671 25204 solver.cpp:229] Iteration 15800, loss = 0.51143
I0113 12:12:58.209743 25204 solver.cpp:245]     Train net output #0: loss = 0.511428 (* 1 = 0.511428 loss)
I0113 12:12:58.209748 25204 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I0113 12:13:18.922858 25204 solver.cpp:338] Iteration 16000, Testing net (#0)
I0113 12:14:05.193349 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673167
I0113 12:14:05.193403 25204 solver.cpp:406]     Test net output #1: loss = 0.9499 (* 1 = 0.9499 loss)
I0113 12:14:05.248862 25204 solver.cpp:229] Iteration 16000, loss = 1.39912
I0113 12:14:05.248899 25204 solver.cpp:245]     Train net output #0: loss = 1.39912 (* 1 = 1.39912 loss)
I0113 12:14:05.248904 25204 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I0113 12:14:26.076063 25204 solver.cpp:229] Iteration 16200, loss = 1.17436
I0113 12:14:26.076102 25204 solver.cpp:245]     Train net output #0: loss = 1.17436 (* 1 = 1.17436 loss)
I0113 12:14:26.076107 25204 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I0113 12:14:46.900169 25204 solver.cpp:229] Iteration 16400, loss = 1.0401
I0113 12:14:46.900221 25204 solver.cpp:245]     Train net output #0: loss = 1.04009 (* 1 = 1.04009 loss)
I0113 12:14:46.900228 25204 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I0113 12:15:07.722393 25204 solver.cpp:229] Iteration 16600, loss = 0.168501
I0113 12:15:07.722431 25204 solver.cpp:245]     Train net output #0: loss = 0.168499 (* 1 = 0.168499 loss)
I0113 12:15:07.722437 25204 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I0113 12:15:28.542179 25204 solver.cpp:229] Iteration 16800, loss = 1.71748
I0113 12:15:28.542279 25204 solver.cpp:245]     Train net output #0: loss = 1.71748 (* 1 = 1.71748 loss)
I0113 12:15:28.542285 25204 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I0113 12:15:49.252965 25204 solver.cpp:338] Iteration 17000, Testing net (#0)
I0113 12:16:35.492832 25204 solver.cpp:406]     Test net output #0: accuracy = 0.678685
I0113 12:16:35.492904 25204 solver.cpp:406]     Test net output #1: loss = 0.853447 (* 1 = 0.853447 loss)
I0113 12:16:35.548383 25204 solver.cpp:229] Iteration 17000, loss = 1.05353
I0113 12:16:35.548421 25204 solver.cpp:245]     Train net output #0: loss = 1.05352 (* 1 = 1.05352 loss)
I0113 12:16:35.548426 25204 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I0113 12:16:56.371932 25204 solver.cpp:229] Iteration 17200, loss = 0.457503
I0113 12:16:56.371971 25204 solver.cpp:245]     Train net output #0: loss = 0.457501 (* 1 = 0.457501 loss)
I0113 12:16:56.371978 25204 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I0113 12:17:17.203614 25204 solver.cpp:229] Iteration 17400, loss = 1.21561
I0113 12:17:17.203660 25204 solver.cpp:245]     Train net output #0: loss = 1.21561 (* 1 = 1.21561 loss)
I0113 12:17:17.203665 25204 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I0113 12:17:38.022301 25204 solver.cpp:229] Iteration 17600, loss = 0.942475
I0113 12:17:38.022341 25204 solver.cpp:245]     Train net output #0: loss = 0.942473 (* 1 = 0.942473 loss)
I0113 12:17:38.022356 25204 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I0113 12:17:58.840584 25204 solver.cpp:229] Iteration 17800, loss = 1.3895
I0113 12:17:58.840690 25204 solver.cpp:245]     Train net output #0: loss = 1.3895 (* 1 = 1.3895 loss)
I0113 12:17:58.840698 25204 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I0113 12:18:19.560550 25204 solver.cpp:338] Iteration 18000, Testing net (#0)
I0113 12:19:05.802755 25204 solver.cpp:406]     Test net output #0: accuracy = 0.677832
I0113 12:19:05.802814 25204 solver.cpp:406]     Test net output #1: loss = 0.872785 (* 1 = 0.872785 loss)
I0113 12:19:05.858395 25204 solver.cpp:229] Iteration 18000, loss = 0.543827
I0113 12:19:05.858435 25204 solver.cpp:245]     Train net output #0: loss = 0.543825 (* 1 = 0.543825 loss)
I0113 12:19:05.858440 25204 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0113 12:19:26.620424 25204 solver.cpp:229] Iteration 18200, loss = 0.216542
I0113 12:19:26.620465 25204 solver.cpp:245]     Train net output #0: loss = 0.21654 (* 1 = 0.21654 loss)
I0113 12:19:26.620471 25204 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I0113 12:19:47.447499 25204 solver.cpp:229] Iteration 18400, loss = 1.29461
I0113 12:19:47.447548 25204 solver.cpp:245]     Train net output #0: loss = 1.29461 (* 1 = 1.29461 loss)
I0113 12:19:47.447554 25204 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I0113 12:20:08.268306 25204 solver.cpp:229] Iteration 18600, loss = 0.678813
I0113 12:20:08.268347 25204 solver.cpp:245]     Train net output #0: loss = 0.678811 (* 1 = 0.678811 loss)
I0113 12:20:08.268353 25204 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I0113 12:20:29.054833 25204 solver.cpp:229] Iteration 18800, loss = 0.166483
I0113 12:20:29.054883 25204 solver.cpp:245]     Train net output #0: loss = 0.166481 (* 1 = 0.166481 loss)
I0113 12:20:29.054889 25204 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I0113 12:20:49.770889 25204 solver.cpp:338] Iteration 19000, Testing net (#0)
I0113 12:21:36.028015 25204 solver.cpp:406]     Test net output #0: accuracy = 0.67221
I0113 12:21:36.028069 25204 solver.cpp:406]     Test net output #1: loss = 0.863023 (* 1 = 0.863023 loss)
I0113 12:21:36.083534 25204 solver.cpp:229] Iteration 19000, loss = 0.957237
I0113 12:21:36.083571 25204 solver.cpp:245]     Train net output #0: loss = 0.957235 (* 1 = 0.957235 loss)
I0113 12:21:36.083577 25204 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I0113 12:21:56.904687 25204 solver.cpp:229] Iteration 19200, loss = 0.838466
I0113 12:21:56.904726 25204 solver.cpp:245]     Train net output #0: loss = 0.838464 (* 1 = 0.838464 loss)
I0113 12:21:56.904732 25204 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I0113 12:22:17.729384 25204 solver.cpp:229] Iteration 19400, loss = 0.321979
I0113 12:22:17.729454 25204 solver.cpp:245]     Train net output #0: loss = 0.321977 (* 1 = 0.321977 loss)
I0113 12:22:17.729460 25204 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I0113 12:22:38.556296 25204 solver.cpp:229] Iteration 19600, loss = 0.816027
I0113 12:22:38.556337 25204 solver.cpp:245]     Train net output #0: loss = 0.816025 (* 1 = 0.816025 loss)
I0113 12:22:38.556342 25204 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I0113 12:22:59.379907 25204 solver.cpp:229] Iteration 19800, loss = 0.632927
I0113 12:22:59.379977 25204 solver.cpp:245]     Train net output #0: loss = 0.632925 (* 1 = 0.632925 loss)
I0113 12:22:59.379983 25204 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I0113 12:23:20.098891 25204 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_20000.caffemodel.h5
I0113 12:23:20.148193 25204 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_20000.solverstate.h5
I0113 12:23:20.148809 25204 solver.cpp:338] Iteration 20000, Testing net (#0)
I0113 12:24:06.370533 25204 solver.cpp:406]     Test net output #0: accuracy = 0.674398
I0113 12:24:06.370611 25204 solver.cpp:406]     Test net output #1: loss = 0.865256 (* 1 = 0.865256 loss)
I0113 12:24:06.426174 25204 solver.cpp:229] Iteration 20000, loss = 1.40566
I0113 12:24:06.426230 25204 solver.cpp:245]     Train net output #0: loss = 1.40566 (* 1 = 1.40566 loss)
I0113 12:24:06.426237 25204 sgd_solver.cpp:106] Iteration 20000, lr = 0.001
I0113 12:24:27.253399 25204 solver.cpp:229] Iteration 20200, loss = 0.95008
I0113 12:24:27.253438 25204 solver.cpp:245]     Train net output #0: loss = 0.950077 (* 1 = 0.950077 loss)
I0113 12:24:27.253444 25204 sgd_solver.cpp:106] Iteration 20200, lr = 0.001
I0113 12:24:48.081346 25204 solver.cpp:229] Iteration 20400, loss = 0.189829
I0113 12:24:48.081430 25204 solver.cpp:245]     Train net output #0: loss = 0.189826 (* 1 = 0.189826 loss)
I0113 12:24:48.081436 25204 sgd_solver.cpp:106] Iteration 20400, lr = 0.001
I0113 12:25:08.908205 25204 solver.cpp:229] Iteration 20600, loss = 1.46358
I0113 12:25:08.908246 25204 solver.cpp:245]     Train net output #0: loss = 1.46358 (* 1 = 1.46358 loss)
I0113 12:25:08.908251 25204 sgd_solver.cpp:106] Iteration 20600, lr = 0.001
I0113 12:25:29.728782 25204 solver.cpp:229] Iteration 20800, loss = 1.01661
I0113 12:25:29.728834 25204 solver.cpp:245]     Train net output #0: loss = 1.0166 (* 1 = 1.0166 loss)
I0113 12:25:29.728840 25204 sgd_solver.cpp:106] Iteration 20800, lr = 0.001
I0113 12:25:50.422173 25204 solver.cpp:338] Iteration 21000, Testing net (#0)
I0113 12:26:36.686658 25204 solver.cpp:406]     Test net output #0: accuracy = 0.681594
I0113 12:26:36.686710 25204 solver.cpp:406]     Test net output #1: loss = 0.949844 (* 1 = 0.949844 loss)
I0113 12:26:36.742219 25204 solver.cpp:229] Iteration 21000, loss = 0.15104
I0113 12:26:36.742257 25204 solver.cpp:245]     Train net output #0: loss = 0.151038 (* 1 = 0.151038 loss)
I0113 12:26:36.742262 25204 sgd_solver.cpp:106] Iteration 21000, lr = 0.001
I0113 12:26:57.571233 25204 solver.cpp:229] Iteration 21200, loss = 1.38663
I0113 12:26:57.571295 25204 solver.cpp:245]     Train net output #0: loss = 1.38663 (* 1 = 1.38663 loss)
I0113 12:26:57.571300 25204 sgd_solver.cpp:106] Iteration 21200, lr = 0.001
I0113 12:27:18.402995 25204 solver.cpp:229] Iteration 21400, loss = 1.00789
I0113 12:27:18.403046 25204 solver.cpp:245]     Train net output #0: loss = 1.00789 (* 1 = 1.00789 loss)
I0113 12:27:18.403051 25204 sgd_solver.cpp:106] Iteration 21400, lr = 0.001
I0113 12:27:39.230885 25204 solver.cpp:229] Iteration 21600, loss = 0.296631
I0113 12:27:39.230926 25204 solver.cpp:245]     Train net output #0: loss = 0.296628 (* 1 = 0.296628 loss)
I0113 12:27:39.230931 25204 sgd_solver.cpp:106] Iteration 21600, lr = 0.001
I0113 12:28:00.060807 25204 solver.cpp:229] Iteration 21800, loss = 0.622476
I0113 12:28:00.060868 25204 solver.cpp:245]     Train net output #0: loss = 0.622473 (* 1 = 0.622473 loss)
I0113 12:28:00.060873 25204 sgd_solver.cpp:106] Iteration 21800, lr = 0.001
I0113 12:28:20.772383 25204 solver.cpp:338] Iteration 22000, Testing net (#0)
I0113 12:29:07.030387 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673916
I0113 12:29:07.030472 25204 solver.cpp:406]     Test net output #1: loss = 0.857425 (* 1 = 0.857425 loss)
I0113 12:29:07.085955 25204 solver.cpp:229] Iteration 22000, loss = 0.3758
I0113 12:29:07.085993 25204 solver.cpp:245]     Train net output #0: loss = 0.375798 (* 1 = 0.375798 loss)
I0113 12:29:07.085999 25204 sgd_solver.cpp:106] Iteration 22000, lr = 0.001
I0113 12:29:27.910153 25204 solver.cpp:229] Iteration 22200, loss = 1.57614
I0113 12:29:27.910194 25204 solver.cpp:245]     Train net output #0: loss = 1.57614 (* 1 = 1.57614 loss)
I0113 12:29:27.910199 25204 sgd_solver.cpp:106] Iteration 22200, lr = 0.001
I0113 12:29:48.737751 25204 solver.cpp:229] Iteration 22400, loss = 0.836996
I0113 12:29:48.737812 25204 solver.cpp:245]     Train net output #0: loss = 0.836993 (* 1 = 0.836993 loss)
I0113 12:29:48.737818 25204 sgd_solver.cpp:106] Iteration 22400, lr = 0.001
I0113 12:30:09.566357 25204 solver.cpp:229] Iteration 22600, loss = 0.191476
I0113 12:30:09.566397 25204 solver.cpp:245]     Train net output #0: loss = 0.191473 (* 1 = 0.191473 loss)
I0113 12:30:09.566402 25204 sgd_solver.cpp:106] Iteration 22600, lr = 0.001
I0113 12:30:30.387112 25204 solver.cpp:229] Iteration 22800, loss = 1.49071
I0113 12:30:30.387202 25204 solver.cpp:245]     Train net output #0: loss = 1.49071 (* 1 = 1.49071 loss)
I0113 12:30:30.387207 25204 sgd_solver.cpp:106] Iteration 22800, lr = 0.001
I0113 12:30:51.100246 25204 solver.cpp:338] Iteration 23000, Testing net (#0)
I0113 12:31:37.343708 25204 solver.cpp:406]     Test net output #0: accuracy = 0.672915
I0113 12:31:37.343773 25204 solver.cpp:406]     Test net output #1: loss = 0.85848 (* 1 = 0.85848 loss)
I0113 12:31:37.399250 25204 solver.cpp:229] Iteration 23000, loss = 0.932813
I0113 12:31:37.399287 25204 solver.cpp:245]     Train net output #0: loss = 0.932811 (* 1 = 0.932811 loss)
I0113 12:31:37.399293 25204 sgd_solver.cpp:106] Iteration 23000, lr = 0.001
I0113 12:31:58.198451 25204 solver.cpp:229] Iteration 23200, loss = 1.82794
I0113 12:31:58.198490 25204 solver.cpp:245]     Train net output #0: loss = 1.82794 (* 1 = 1.82794 loss)
I0113 12:31:58.198496 25204 sgd_solver.cpp:106] Iteration 23200, lr = 0.001
I0113 12:32:19.019376 25204 solver.cpp:229] Iteration 23400, loss = 0.588279
I0113 12:32:19.019425 25204 solver.cpp:245]     Train net output #0: loss = 0.588277 (* 1 = 0.588277 loss)
I0113 12:32:19.019430 25204 sgd_solver.cpp:106] Iteration 23400, lr = 0.001
I0113 12:32:39.832159 25204 solver.cpp:229] Iteration 23600, loss = 0.675761
I0113 12:32:39.832197 25204 solver.cpp:245]     Train net output #0: loss = 0.675758 (* 1 = 0.675758 loss)
I0113 12:32:39.832202 25204 sgd_solver.cpp:106] Iteration 23600, lr = 0.001
I0113 12:33:00.647621 25204 solver.cpp:229] Iteration 23800, loss = 1.52252
I0113 12:33:00.647693 25204 solver.cpp:245]     Train net output #0: loss = 1.52252 (* 1 = 1.52252 loss)
I0113 12:33:00.647698 25204 sgd_solver.cpp:106] Iteration 23800, lr = 0.001
I0113 12:33:21.369704 25204 solver.cpp:338] Iteration 24000, Testing net (#0)
I0113 12:34:07.611879 25204 solver.cpp:406]     Test net output #0: accuracy = 0.677643
I0113 12:34:07.611933 25204 solver.cpp:406]     Test net output #1: loss = 0.880347 (* 1 = 0.880347 loss)
I0113 12:34:07.667341 25204 solver.cpp:229] Iteration 24000, loss = 1.32747
I0113 12:34:07.667379 25204 solver.cpp:245]     Train net output #0: loss = 1.32747 (* 1 = 1.32747 loss)
I0113 12:34:07.667385 25204 sgd_solver.cpp:106] Iteration 24000, lr = 0.001
I0113 12:34:28.490424 25204 solver.cpp:229] Iteration 24200, loss = 0.401948
I0113 12:34:28.490463 25204 solver.cpp:245]     Train net output #0: loss = 0.401946 (* 1 = 0.401946 loss)
I0113 12:34:28.490468 25204 sgd_solver.cpp:106] Iteration 24200, lr = 0.001
I0113 12:34:49.310420 25204 solver.cpp:229] Iteration 24400, loss = 1.49001
I0113 12:34:49.310469 25204 solver.cpp:245]     Train net output #0: loss = 1.49001 (* 1 = 1.49001 loss)
I0113 12:34:49.310475 25204 sgd_solver.cpp:106] Iteration 24400, lr = 0.001
I0113 12:35:10.135700 25204 solver.cpp:229] Iteration 24600, loss = 1.26249
I0113 12:35:10.135741 25204 solver.cpp:245]     Train net output #0: loss = 1.26249 (* 1 = 1.26249 loss)
I0113 12:35:10.135746 25204 sgd_solver.cpp:106] Iteration 24600, lr = 0.001
I0113 12:35:30.945219 25204 solver.cpp:229] Iteration 24800, loss = 0.187069
I0113 12:35:30.945293 25204 solver.cpp:245]     Train net output #0: loss = 0.187067 (* 1 = 0.187067 loss)
I0113 12:35:30.945299 25204 sgd_solver.cpp:106] Iteration 24800, lr = 0.001
I0113 12:35:51.660096 25204 solver.cpp:338] Iteration 25000, Testing net (#0)
I0113 12:36:37.929841 25204 solver.cpp:406]     Test net output #0: accuracy = 0.679727
I0113 12:36:37.929893 25204 solver.cpp:406]     Test net output #1: loss = 0.861793 (* 1 = 0.861793 loss)
I0113 12:36:37.985436 25204 solver.cpp:229] Iteration 25000, loss = 1.55388
I0113 12:36:37.985476 25204 solver.cpp:245]     Train net output #0: loss = 1.55388 (* 1 = 1.55388 loss)
I0113 12:36:37.985481 25204 sgd_solver.cpp:106] Iteration 25000, lr = 0.001
I0113 12:36:58.803315 25204 solver.cpp:229] Iteration 25200, loss = 0.864743
I0113 12:36:58.803354 25204 solver.cpp:245]     Train net output #0: loss = 0.864741 (* 1 = 0.864741 loss)
I0113 12:36:58.803359 25204 sgd_solver.cpp:106] Iteration 25200, lr = 0.001
I0113 12:37:19.590386 25204 solver.cpp:229] Iteration 25400, loss = 0.1331
I0113 12:37:19.590497 25204 solver.cpp:245]     Train net output #0: loss = 0.133098 (* 1 = 0.133098 loss)
I0113 12:37:19.590502 25204 sgd_solver.cpp:106] Iteration 25400, lr = 0.001
I0113 12:37:40.408326 25204 solver.cpp:229] Iteration 25600, loss = 0.640022
I0113 12:37:40.408367 25204 solver.cpp:245]     Train net output #0: loss = 0.64002 (* 1 = 0.64002 loss)
I0113 12:37:40.408372 25204 sgd_solver.cpp:106] Iteration 25600, lr = 0.001
I0113 12:38:01.215472 25204 solver.cpp:229] Iteration 25800, loss = 0.836016
I0113 12:38:01.215545 25204 solver.cpp:245]     Train net output #0: loss = 0.836014 (* 1 = 0.836014 loss)
I0113 12:38:01.215551 25204 sgd_solver.cpp:106] Iteration 25800, lr = 0.001
I0113 12:38:21.916476 25204 solver.cpp:338] Iteration 26000, Testing net (#0)
I0113 12:39:08.206941 25204 solver.cpp:406]     Test net output #0: accuracy = 0.672168
I0113 12:39:08.207015 25204 solver.cpp:406]     Test net output #1: loss = 0.894607 (* 1 = 0.894607 loss)
I0113 12:39:08.262533 25204 solver.cpp:229] Iteration 26000, loss = 0.751371
I0113 12:39:08.262569 25204 solver.cpp:245]     Train net output #0: loss = 0.751369 (* 1 = 0.751369 loss)
I0113 12:39:08.262575 25204 sgd_solver.cpp:106] Iteration 26000, lr = 0.001
I0113 12:39:29.084138 25204 solver.cpp:229] Iteration 26200, loss = 0.837043
I0113 12:39:29.084177 25204 solver.cpp:245]     Train net output #0: loss = 0.837041 (* 1 = 0.837041 loss)
I0113 12:39:29.084183 25204 sgd_solver.cpp:106] Iteration 26200, lr = 0.001
I0113 12:39:49.874886 25204 solver.cpp:229] Iteration 26400, loss = 0.447287
I0113 12:39:49.874936 25204 solver.cpp:245]     Train net output #0: loss = 0.447285 (* 1 = 0.447285 loss)
I0113 12:39:49.874941 25204 sgd_solver.cpp:106] Iteration 26400, lr = 0.001
I0113 12:40:10.682364 25204 solver.cpp:229] Iteration 26600, loss = 0.52639
I0113 12:40:10.682404 25204 solver.cpp:245]     Train net output #0: loss = 0.526388 (* 1 = 0.526388 loss)
I0113 12:40:10.682410 25204 sgd_solver.cpp:106] Iteration 26600, lr = 0.001
I0113 12:40:31.506956 25204 solver.cpp:229] Iteration 26800, loss = 1.04339
I0113 12:40:31.507009 25204 solver.cpp:245]     Train net output #0: loss = 1.04339 (* 1 = 1.04339 loss)
I0113 12:40:31.507014 25204 sgd_solver.cpp:106] Iteration 26800, lr = 0.001
I0113 12:40:52.223438 25204 solver.cpp:338] Iteration 27000, Testing net (#0)
I0113 12:41:38.504186 25204 solver.cpp:406]     Test net output #0: accuracy = 0.674195
I0113 12:41:38.504262 25204 solver.cpp:406]     Test net output #1: loss = 0.939125 (* 1 = 0.939125 loss)
I0113 12:41:38.559708 25204 solver.cpp:229] Iteration 27000, loss = 0.766001
I0113 12:41:38.559746 25204 solver.cpp:245]     Train net output #0: loss = 0.765999 (* 1 = 0.765999 loss)
I0113 12:41:38.559751 25204 sgd_solver.cpp:106] Iteration 27000, lr = 0.001
I0113 12:41:59.390538 25204 solver.cpp:229] Iteration 27200, loss = 0.56907
I0113 12:41:59.390578 25204 solver.cpp:245]     Train net output #0: loss = 0.569069 (* 1 = 0.569069 loss)
I0113 12:41:59.390583 25204 sgd_solver.cpp:106] Iteration 27200, lr = 0.001
I0113 12:42:20.220888 25204 solver.cpp:229] Iteration 27400, loss = 0.804577
I0113 12:42:20.220937 25204 solver.cpp:245]     Train net output #0: loss = 0.804575 (* 1 = 0.804575 loss)
I0113 12:42:20.220943 25204 sgd_solver.cpp:106] Iteration 27400, lr = 0.001
I0113 12:42:41.040390 25204 solver.cpp:229] Iteration 27600, loss = 0.142144
I0113 12:42:41.040428 25204 solver.cpp:245]     Train net output #0: loss = 0.142142 (* 1 = 0.142142 loss)
I0113 12:42:41.040434 25204 sgd_solver.cpp:106] Iteration 27600, lr = 0.001
I0113 12:43:01.864785 25204 solver.cpp:229] Iteration 27800, loss = 0.76366
I0113 12:43:01.864836 25204 solver.cpp:245]     Train net output #0: loss = 0.763658 (* 1 = 0.763658 loss)
I0113 12:43:01.864842 25204 sgd_solver.cpp:106] Iteration 27800, lr = 0.001
I0113 12:43:22.582231 25204 solver.cpp:338] Iteration 28000, Testing net (#0)
I0113 12:44:08.862166 25204 solver.cpp:406]     Test net output #0: accuracy = 0.680223
I0113 12:44:08.862282 25204 solver.cpp:406]     Test net output #1: loss = 0.848647 (* 1 = 0.848647 loss)
I0113 12:44:08.917786 25204 solver.cpp:229] Iteration 28000, loss = 0.699325
I0113 12:44:08.917824 25204 solver.cpp:245]     Train net output #0: loss = 0.699324 (* 1 = 0.699324 loss)
I0113 12:44:08.917830 25204 sgd_solver.cpp:106] Iteration 28000, lr = 0.001
I0113 12:44:29.744840 25204 solver.cpp:229] Iteration 28200, loss = 0.342373
I0113 12:44:29.744879 25204 solver.cpp:245]     Train net output #0: loss = 0.342371 (* 1 = 0.342371 loss)
I0113 12:44:29.744885 25204 sgd_solver.cpp:106] Iteration 28200, lr = 0.001
I0113 12:44:50.571518 25204 solver.cpp:229] Iteration 28400, loss = 0.994579
I0113 12:44:50.571619 25204 solver.cpp:245]     Train net output #0: loss = 0.994577 (* 1 = 0.994577 loss)
I0113 12:44:50.571625 25204 sgd_solver.cpp:106] Iteration 28400, lr = 0.001
I0113 12:45:11.398849 25204 solver.cpp:229] Iteration 28600, loss = 0.451404
I0113 12:45:11.398887 25204 solver.cpp:245]     Train net output #0: loss = 0.451402 (* 1 = 0.451402 loss)
I0113 12:45:11.398893 25204 sgd_solver.cpp:106] Iteration 28600, lr = 0.001
I0113 12:45:32.222494 25204 solver.cpp:229] Iteration 28800, loss = 2.17772
I0113 12:45:32.222564 25204 solver.cpp:245]     Train net output #0: loss = 2.17772 (* 1 = 2.17772 loss)
I0113 12:45:32.222570 25204 sgd_solver.cpp:106] Iteration 28800, lr = 0.001
I0113 12:45:52.944985 25204 solver.cpp:338] Iteration 29000, Testing net (#0)
I0113 12:46:39.206362 25204 solver.cpp:406]     Test net output #0: accuracy = 0.675664
I0113 12:46:39.206418 25204 solver.cpp:406]     Test net output #1: loss = 0.885876 (* 1 = 0.885876 loss)
I0113 12:46:39.261927 25204 solver.cpp:229] Iteration 29000, loss = 0.992961
I0113 12:46:39.261965 25204 solver.cpp:245]     Train net output #0: loss = 0.992959 (* 1 = 0.992959 loss)
I0113 12:46:39.261972 25204 sgd_solver.cpp:106] Iteration 29000, lr = 0.001
I0113 12:47:00.091367 25204 solver.cpp:229] Iteration 29200, loss = 1.16158
I0113 12:47:00.091408 25204 solver.cpp:245]     Train net output #0: loss = 1.16158 (* 1 = 1.16158 loss)
I0113 12:47:00.091413 25204 sgd_solver.cpp:106] Iteration 29200, lr = 0.001
I0113 12:47:20.924209 25204 solver.cpp:229] Iteration 29400, loss = 0.966081
I0113 12:47:20.924259 25204 solver.cpp:245]     Train net output #0: loss = 0.966079 (* 1 = 0.966079 loss)
I0113 12:47:20.924264 25204 sgd_solver.cpp:106] Iteration 29400, lr = 0.001
I0113 12:47:41.752266 25204 solver.cpp:229] Iteration 29600, loss = 0.400221
I0113 12:47:41.752305 25204 solver.cpp:245]     Train net output #0: loss = 0.400219 (* 1 = 0.400219 loss)
I0113 12:47:41.752311 25204 sgd_solver.cpp:106] Iteration 29600, lr = 0.001
I0113 12:48:02.583356 25204 solver.cpp:229] Iteration 29800, loss = 0.227007
I0113 12:48:02.583406 25204 solver.cpp:245]     Train net output #0: loss = 0.227005 (* 1 = 0.227005 loss)
I0113 12:48:02.583412 25204 sgd_solver.cpp:106] Iteration 29800, lr = 0.001
I0113 12:48:23.306135 25204 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_30000.caffemodel.h5
I0113 12:48:23.355433 25204 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_30000.solverstate.h5
I0113 12:48:23.356047 25204 solver.cpp:338] Iteration 30000, Testing net (#0)
I0113 12:49:09.557406 25204 solver.cpp:406]     Test net output #0: accuracy = 0.671797
I0113 12:49:09.557462 25204 solver.cpp:406]     Test net output #1: loss = 0.903351 (* 1 = 0.903351 loss)
I0113 12:49:09.612962 25204 solver.cpp:229] Iteration 30000, loss = 0.990892
I0113 12:49:09.612999 25204 solver.cpp:245]     Train net output #0: loss = 0.99089 (* 1 = 0.99089 loss)
I0113 12:49:09.613004 25204 sgd_solver.cpp:106] Iteration 30000, lr = 0.001
I0113 12:49:30.440909 25204 solver.cpp:229] Iteration 30200, loss = 1.32946
I0113 12:49:30.440949 25204 solver.cpp:245]     Train net output #0: loss = 1.32946 (* 1 = 1.32946 loss)
I0113 12:49:30.440955 25204 sgd_solver.cpp:106] Iteration 30200, lr = 0.001
I0113 12:49:51.269292 25204 solver.cpp:229] Iteration 30400, loss = 0.202939
I0113 12:49:51.269378 25204 solver.cpp:245]     Train net output #0: loss = 0.202937 (* 1 = 0.202937 loss)
I0113 12:49:51.269384 25204 sgd_solver.cpp:106] Iteration 30400, lr = 0.001
I0113 12:50:12.108611 25204 solver.cpp:229] Iteration 30600, loss = 1.50194
I0113 12:50:12.108651 25204 solver.cpp:245]     Train net output #0: loss = 1.50194 (* 1 = 1.50194 loss)
I0113 12:50:12.108656 25204 sgd_solver.cpp:106] Iteration 30600, lr = 0.001
I0113 12:50:32.937362 25204 solver.cpp:229] Iteration 30800, loss = 0.561398
I0113 12:50:32.937417 25204 solver.cpp:245]     Train net output #0: loss = 0.561396 (* 1 = 0.561396 loss)
I0113 12:50:32.937422 25204 sgd_solver.cpp:106] Iteration 30800, lr = 0.001
I0113 12:50:53.664652 25204 solver.cpp:338] Iteration 31000, Testing net (#0)
I0113 12:51:39.913489 25204 solver.cpp:406]     Test net output #0: accuracy = 0.676888
I0113 12:51:39.913542 25204 solver.cpp:406]     Test net output #1: loss = 0.888308 (* 1 = 0.888308 loss)
I0113 12:51:39.969053 25204 solver.cpp:229] Iteration 31000, loss = 1.04789
I0113 12:51:39.969092 25204 solver.cpp:245]     Train net output #0: loss = 1.04789 (* 1 = 1.04789 loss)
I0113 12:51:39.969097 25204 sgd_solver.cpp:106] Iteration 31000, lr = 0.001
I0113 12:52:00.802423 25204 solver.cpp:229] Iteration 31200, loss = 1.18204
I0113 12:52:00.802462 25204 solver.cpp:245]     Train net output #0: loss = 1.18204 (* 1 = 1.18204 loss)
I0113 12:52:00.802469 25204 sgd_solver.cpp:106] Iteration 31200, lr = 0.001
I0113 12:52:21.631018 25204 solver.cpp:229] Iteration 31400, loss = 0.190512
I0113 12:52:21.631089 25204 solver.cpp:245]     Train net output #0: loss = 0.19051 (* 1 = 0.19051 loss)
I0113 12:52:21.631095 25204 sgd_solver.cpp:106] Iteration 31400, lr = 0.001
I0113 12:52:42.472060 25204 solver.cpp:229] Iteration 31600, loss = 0.868209
I0113 12:52:42.472100 25204 solver.cpp:245]     Train net output #0: loss = 0.868208 (* 1 = 0.868208 loss)
I0113 12:52:42.472105 25204 sgd_solver.cpp:106] Iteration 31600, lr = 0.001
I0113 12:53:03.316629 25204 solver.cpp:229] Iteration 31800, loss = 0.405953
I0113 12:53:03.316679 25204 solver.cpp:245]     Train net output #0: loss = 0.405951 (* 1 = 0.405951 loss)
I0113 12:53:03.316685 25204 sgd_solver.cpp:106] Iteration 31800, lr = 0.001
I0113 12:53:24.047399 25204 solver.cpp:338] Iteration 32000, Testing net (#0)
I0113 12:54:10.289903 25204 solver.cpp:406]     Test net output #0: accuracy = 0.680734
I0113 12:54:10.289955 25204 solver.cpp:406]     Test net output #1: loss = 0.942928 (* 1 = 0.942928 loss)
I0113 12:54:10.345561 25204 solver.cpp:229] Iteration 32000, loss = 0.159273
I0113 12:54:10.345599 25204 solver.cpp:245]     Train net output #0: loss = 0.159271 (* 1 = 0.159271 loss)
I0113 12:54:10.345604 25204 sgd_solver.cpp:106] Iteration 32000, lr = 0.001
I0113 12:54:31.191511 25204 solver.cpp:229] Iteration 32200, loss = 0.973325
I0113 12:54:31.191551 25204 solver.cpp:245]     Train net output #0: loss = 0.973324 (* 1 = 0.973324 loss)
I0113 12:54:31.191557 25204 sgd_solver.cpp:106] Iteration 32200, lr = 0.001
I0113 12:54:52.033457 25204 solver.cpp:229] Iteration 32400, loss = 0.574297
I0113 12:54:52.033507 25204 solver.cpp:245]     Train net output #0: loss = 0.574296 (* 1 = 0.574296 loss)
I0113 12:54:52.033514 25204 sgd_solver.cpp:106] Iteration 32400, lr = 0.001
I0113 12:55:12.879894 25204 solver.cpp:229] Iteration 32600, loss = 0.872428
I0113 12:55:12.879952 25204 solver.cpp:245]     Train net output #0: loss = 0.872426 (* 1 = 0.872426 loss)
I0113 12:55:12.879958 25204 sgd_solver.cpp:106] Iteration 32600, lr = 0.001
I0113 12:55:33.740037 25204 solver.cpp:229] Iteration 32800, loss = 1.77427
I0113 12:55:33.740087 25204 solver.cpp:245]     Train net output #0: loss = 1.77427 (* 1 = 1.77427 loss)
I0113 12:55:33.740093 25204 sgd_solver.cpp:106] Iteration 32800, lr = 0.001
I0113 12:55:54.503772 25204 solver.cpp:338] Iteration 33000, Testing net (#0)
I0113 12:56:40.771441 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673307
I0113 12:56:40.771534 25204 solver.cpp:406]     Test net output #1: loss = 0.862989 (* 1 = 0.862989 loss)
I0113 12:56:40.827268 25204 solver.cpp:229] Iteration 33000, loss = 0.745004
I0113 12:56:40.827307 25204 solver.cpp:245]     Train net output #0: loss = 0.745002 (* 1 = 0.745002 loss)
I0113 12:56:40.827312 25204 sgd_solver.cpp:106] Iteration 33000, lr = 0.001
I0113 12:57:01.698860 25204 solver.cpp:229] Iteration 33200, loss = 0.64079
I0113 12:57:01.698901 25204 solver.cpp:245]     Train net output #0: loss = 0.640787 (* 1 = 0.640787 loss)
I0113 12:57:01.698907 25204 sgd_solver.cpp:106] Iteration 33200, lr = 0.001
I0113 12:57:22.570268 25204 solver.cpp:229] Iteration 33400, loss = 0.911815
I0113 12:57:22.570320 25204 solver.cpp:245]     Train net output #0: loss = 0.911812 (* 1 = 0.911812 loss)
I0113 12:57:22.570325 25204 sgd_solver.cpp:106] Iteration 33400, lr = 0.001
I0113 12:57:43.430675 25204 solver.cpp:229] Iteration 33600, loss = 0.213438
I0113 12:57:43.430714 25204 solver.cpp:245]     Train net output #0: loss = 0.213435 (* 1 = 0.213435 loss)
I0113 12:57:43.430721 25204 sgd_solver.cpp:106] Iteration 33600, lr = 0.001
I0113 12:58:04.299406 25204 solver.cpp:229] Iteration 33800, loss = 1.14294
I0113 12:58:04.299476 25204 solver.cpp:245]     Train net output #0: loss = 1.14294 (* 1 = 1.14294 loss)
I0113 12:58:04.299484 25204 sgd_solver.cpp:106] Iteration 33800, lr = 0.001
I0113 12:58:25.064553 25204 solver.cpp:338] Iteration 34000, Testing net (#0)
I0113 12:59:11.333755 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673405
I0113 12:59:11.333806 25204 solver.cpp:406]     Test net output #1: loss = 0.85963 (* 1 = 0.85963 loss)
I0113 12:59:11.389508 25204 solver.cpp:229] Iteration 34000, loss = 1.61815
I0113 12:59:11.389546 25204 solver.cpp:245]     Train net output #0: loss = 1.61814 (* 1 = 1.61814 loss)
I0113 12:59:11.389551 25204 sgd_solver.cpp:106] Iteration 34000, lr = 0.001
I0113 12:59:32.258157 25204 solver.cpp:229] Iteration 34200, loss = 0.165811
I0113 12:59:32.258198 25204 solver.cpp:245]     Train net output #0: loss = 0.165809 (* 1 = 0.165809 loss)
I0113 12:59:32.258203 25204 sgd_solver.cpp:106] Iteration 34200, lr = 0.001
I0113 12:59:53.131891 25204 solver.cpp:229] Iteration 34400, loss = 1.24372
I0113 12:59:53.131961 25204 solver.cpp:245]     Train net output #0: loss = 1.24372 (* 1 = 1.24372 loss)
I0113 12:59:53.131968 25204 sgd_solver.cpp:106] Iteration 34400, lr = 0.001
I0113 13:00:14.008002 25204 solver.cpp:229] Iteration 34600, loss = 1.35795
I0113 13:00:14.008043 25204 solver.cpp:245]     Train net output #0: loss = 1.35794 (* 1 = 1.35794 loss)
I0113 13:00:14.008047 25204 sgd_solver.cpp:106] Iteration 34600, lr = 0.001
I0113 13:00:34.877014 25204 solver.cpp:229] Iteration 34800, loss = 0.63701
I0113 13:00:34.877066 25204 solver.cpp:245]     Train net output #0: loss = 0.637007 (* 1 = 0.637007 loss)
I0113 13:00:34.877073 25204 sgd_solver.cpp:106] Iteration 34800, lr = 0.001
I0113 13:00:55.652292 25204 solver.cpp:338] Iteration 35000, Testing net (#0)
I0113 13:01:41.918715 25204 solver.cpp:406]     Test net output #0: accuracy = 0.678936
I0113 13:01:41.918789 25204 solver.cpp:406]     Test net output #1: loss = 0.850137 (* 1 = 0.850137 loss)
I0113 13:01:41.974455 25204 solver.cpp:229] Iteration 35000, loss = 1.28084
I0113 13:01:41.974493 25204 solver.cpp:245]     Train net output #0: loss = 1.28084 (* 1 = 1.28084 loss)
I0113 13:01:41.974498 25204 sgd_solver.cpp:106] Iteration 35000, lr = 0.001
I0113 13:02:02.852509 25204 solver.cpp:229] Iteration 35200, loss = 0.721376
I0113 13:02:02.852547 25204 solver.cpp:245]     Train net output #0: loss = 0.721373 (* 1 = 0.721373 loss)
I0113 13:02:02.852552 25204 sgd_solver.cpp:106] Iteration 35200, lr = 0.001
I0113 13:02:23.717052 25204 solver.cpp:229] Iteration 35400, loss = 0.270193
I0113 13:02:23.717100 25204 solver.cpp:245]     Train net output #0: loss = 0.270189 (* 1 = 0.270189 loss)
I0113 13:02:23.717106 25204 sgd_solver.cpp:106] Iteration 35400, lr = 0.001
I0113 13:02:44.585757 25204 solver.cpp:229] Iteration 35600, loss = 0.738984
I0113 13:02:44.585796 25204 solver.cpp:245]     Train net output #0: loss = 0.738981 (* 1 = 0.738981 loss)
I0113 13:02:44.585808 25204 sgd_solver.cpp:106] Iteration 35600, lr = 0.001
I0113 13:03:05.452208 25204 solver.cpp:229] Iteration 35800, loss = 0.235855
I0113 13:03:05.452314 25204 solver.cpp:245]     Train net output #0: loss = 0.235852 (* 1 = 0.235852 loss)
I0113 13:03:05.452322 25204 sgd_solver.cpp:106] Iteration 35800, lr = 0.001
I0113 13:03:26.210822 25204 solver.cpp:338] Iteration 36000, Testing net (#0)
I0113 13:04:12.452567 25204 solver.cpp:406]     Test net output #0: accuracy = 0.676867
I0113 13:04:12.452642 25204 solver.cpp:406]     Test net output #1: loss = 0.877751 (* 1 = 0.877751 loss)
I0113 13:04:12.508380 25204 solver.cpp:229] Iteration 36000, loss = 1.19792
I0113 13:04:12.508419 25204 solver.cpp:245]     Train net output #0: loss = 1.19791 (* 1 = 1.19791 loss)
I0113 13:04:12.508425 25204 sgd_solver.cpp:106] Iteration 36000, lr = 0.001
I0113 13:04:33.382657 25204 solver.cpp:229] Iteration 36200, loss = 1.12479
I0113 13:04:33.382695 25204 solver.cpp:245]     Train net output #0: loss = 1.12479 (* 1 = 1.12479 loss)
I0113 13:04:33.382701 25204 sgd_solver.cpp:106] Iteration 36200, lr = 0.001
I0113 13:04:54.244212 25204 solver.cpp:229] Iteration 36400, loss = 0.192553
I0113 13:04:54.244264 25204 solver.cpp:245]     Train net output #0: loss = 0.19255 (* 1 = 0.19255 loss)
I0113 13:04:54.244271 25204 sgd_solver.cpp:106] Iteration 36400, lr = 0.001
I0113 13:05:15.115698 25204 solver.cpp:229] Iteration 36600, loss = 1.01037
I0113 13:05:15.115736 25204 solver.cpp:245]     Train net output #0: loss = 1.01036 (* 1 = 1.01036 loss)
I0113 13:05:15.115742 25204 sgd_solver.cpp:106] Iteration 36600, lr = 0.001
I0113 13:05:35.980808 25204 solver.cpp:229] Iteration 36800, loss = 0.515601
I0113 13:05:35.980862 25204 solver.cpp:245]     Train net output #0: loss = 0.515598 (* 1 = 0.515598 loss)
I0113 13:05:35.980868 25204 sgd_solver.cpp:106] Iteration 36800, lr = 0.001
I0113 13:05:56.733696 25204 solver.cpp:338] Iteration 37000, Testing net (#0)
I0113 13:06:43.015326 25204 solver.cpp:406]     Test net output #0: accuracy = 0.672139
I0113 13:06:43.015379 25204 solver.cpp:406]     Test net output #1: loss = 0.967056 (* 1 = 0.967056 loss)
I0113 13:06:43.071094 25204 solver.cpp:229] Iteration 37000, loss = 1.58351
I0113 13:06:43.071135 25204 solver.cpp:245]     Train net output #0: loss = 1.5835 (* 1 = 1.5835 loss)
I0113 13:06:43.071141 25204 sgd_solver.cpp:106] Iteration 37000, lr = 0.001
I0113 13:07:03.944113 25204 solver.cpp:229] Iteration 37200, loss = 1.56842
I0113 13:07:03.944154 25204 solver.cpp:245]     Train net output #0: loss = 1.56842 (* 1 = 1.56842 loss)
I0113 13:07:03.944159 25204 sgd_solver.cpp:106] Iteration 37200, lr = 0.001
I0113 13:07:24.807724 25204 solver.cpp:229] Iteration 37400, loss = 0.883837
I0113 13:07:24.807775 25204 solver.cpp:245]     Train net output #0: loss = 0.883833 (* 1 = 0.883833 loss)
I0113 13:07:24.807781 25204 sgd_solver.cpp:106] Iteration 37400, lr = 0.001
I0113 13:07:45.663410 25204 solver.cpp:229] Iteration 37600, loss = 0.298595
I0113 13:07:45.663450 25204 solver.cpp:245]     Train net output #0: loss = 0.298592 (* 1 = 0.298592 loss)
I0113 13:07:45.663456 25204 sgd_solver.cpp:106] Iteration 37600, lr = 0.001
I0113 13:08:06.534415 25204 solver.cpp:229] Iteration 37800, loss = 0.857071
I0113 13:08:06.534463 25204 solver.cpp:245]     Train net output #0: loss = 0.857068 (* 1 = 0.857068 loss)
I0113 13:08:06.534468 25204 sgd_solver.cpp:106] Iteration 37800, lr = 0.001
I0113 13:08:27.291375 25204 solver.cpp:338] Iteration 38000, Testing net (#0)
I0113 13:09:13.565338 25204 solver.cpp:406]     Test net output #0: accuracy = 0.675132
I0113 13:09:13.565407 25204 solver.cpp:406]     Test net output #1: loss = 0.878778 (* 1 = 0.878778 loss)
I0113 13:09:13.621095 25204 solver.cpp:229] Iteration 38000, loss = 0.261876
I0113 13:09:13.621132 25204 solver.cpp:245]     Train net output #0: loss = 0.261873 (* 1 = 0.261873 loss)
I0113 13:09:13.621137 25204 sgd_solver.cpp:106] Iteration 38000, lr = 0.001
I0113 13:09:34.485692 25204 solver.cpp:229] Iteration 38200, loss = 0.737789
I0113 13:09:34.485733 25204 solver.cpp:245]     Train net output #0: loss = 0.737785 (* 1 = 0.737785 loss)
I0113 13:09:34.485744 25204 sgd_solver.cpp:106] Iteration 38200, lr = 0.001
I0113 13:09:55.352666 25204 solver.cpp:229] Iteration 38400, loss = 0.67239
I0113 13:09:55.352761 25204 solver.cpp:245]     Train net output #0: loss = 0.672386 (* 1 = 0.672386 loss)
I0113 13:09:55.352767 25204 sgd_solver.cpp:106] Iteration 38400, lr = 0.001
I0113 13:10:16.210726 25204 solver.cpp:229] Iteration 38600, loss = 0.851607
I0113 13:10:16.210765 25204 solver.cpp:245]     Train net output #0: loss = 0.851603 (* 1 = 0.851603 loss)
I0113 13:10:16.210770 25204 sgd_solver.cpp:106] Iteration 38600, lr = 0.001
I0113 13:10:37.072790 25204 solver.cpp:229] Iteration 38800, loss = 1.32729
I0113 13:10:37.072846 25204 solver.cpp:245]     Train net output #0: loss = 1.32729 (* 1 = 1.32729 loss)
I0113 13:10:37.072852 25204 sgd_solver.cpp:106] Iteration 38800, lr = 0.001
I0113 13:10:57.838073 25204 solver.cpp:338] Iteration 39000, Testing net (#0)
I0113 13:11:44.092661 25204 solver.cpp:406]     Test net output #0: accuracy = 0.681272
I0113 13:11:44.092713 25204 solver.cpp:406]     Test net output #1: loss = 0.846395 (* 1 = 0.846395 loss)
I0113 13:11:44.148386 25204 solver.cpp:229] Iteration 39000, loss = 1.13877
I0113 13:11:44.148425 25204 solver.cpp:245]     Train net output #0: loss = 1.13877 (* 1 = 1.13877 loss)
I0113 13:11:44.148432 25204 sgd_solver.cpp:106] Iteration 39000, lr = 0.001
I0113 13:12:05.012466 25204 solver.cpp:229] Iteration 39200, loss = 0.35559
I0113 13:12:05.012503 25204 solver.cpp:245]     Train net output #0: loss = 0.355586 (* 1 = 0.355586 loss)
I0113 13:12:05.012509 25204 sgd_solver.cpp:106] Iteration 39200, lr = 0.001
I0113 13:12:25.877290 25204 solver.cpp:229] Iteration 39400, loss = 0.419171
I0113 13:12:25.877337 25204 solver.cpp:245]     Train net output #0: loss = 0.419168 (* 1 = 0.419168 loss)
I0113 13:12:25.877343 25204 sgd_solver.cpp:106] Iteration 39400, lr = 0.001
I0113 13:12:46.738422 25204 solver.cpp:229] Iteration 39600, loss = 1.20616
I0113 13:12:46.738463 25204 solver.cpp:245]     Train net output #0: loss = 1.20616 (* 1 = 1.20616 loss)
I0113 13:12:46.738469 25204 sgd_solver.cpp:106] Iteration 39600, lr = 0.001
I0113 13:13:07.595281 25204 solver.cpp:229] Iteration 39800, loss = 0.31701
I0113 13:13:07.595332 25204 solver.cpp:245]     Train net output #0: loss = 0.317007 (* 1 = 0.317007 loss)
I0113 13:13:07.595338 25204 sgd_solver.cpp:106] Iteration 39800, lr = 0.001
I0113 13:13:28.355600 25204 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_40000.caffemodel.h5
I0113 13:13:28.404888 25204 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_40000.solverstate.h5
I0113 13:13:28.405508 25204 solver.cpp:338] Iteration 40000, Testing net (#0)
I0113 13:14:14.620319 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673426
I0113 13:14:14.620378 25204 solver.cpp:406]     Test net output #1: loss = 0.915196 (* 1 = 0.915196 loss)
I0113 13:14:14.676151 25204 solver.cpp:229] Iteration 40000, loss = 1.07038
I0113 13:14:14.676188 25204 solver.cpp:245]     Train net output #0: loss = 1.07038 (* 1 = 1.07038 loss)
I0113 13:14:14.676194 25204 sgd_solver.cpp:106] Iteration 40000, lr = 0.001
I0113 13:14:35.541141 25204 solver.cpp:229] Iteration 40200, loss = 0.28769
I0113 13:14:35.541180 25204 solver.cpp:245]     Train net output #0: loss = 0.287686 (* 1 = 0.287686 loss)
I0113 13:14:35.541187 25204 sgd_solver.cpp:106] Iteration 40200, lr = 0.001
I0113 13:14:56.398840 25204 solver.cpp:229] Iteration 40400, loss = 0.792223
I0113 13:14:56.398923 25204 solver.cpp:245]     Train net output #0: loss = 0.792219 (* 1 = 0.792219 loss)
I0113 13:14:56.398929 25204 sgd_solver.cpp:106] Iteration 40400, lr = 0.001
I0113 13:15:17.270306 25204 solver.cpp:229] Iteration 40600, loss = 0.930813
I0113 13:15:17.270349 25204 solver.cpp:245]     Train net output #0: loss = 0.930809 (* 1 = 0.930809 loss)
I0113 13:15:17.270354 25204 sgd_solver.cpp:106] Iteration 40600, lr = 0.001
I0113 13:15:38.127861 25204 solver.cpp:229] Iteration 40800, loss = 0.519718
I0113 13:15:38.127974 25204 solver.cpp:245]     Train net output #0: loss = 0.519714 (* 1 = 0.519714 loss)
I0113 13:15:38.127979 25204 sgd_solver.cpp:106] Iteration 40800, lr = 0.001
I0113 13:15:58.888491 25204 solver.cpp:338] Iteration 41000, Testing net (#0)
I0113 13:16:45.127118 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673601
I0113 13:16:45.127174 25204 solver.cpp:406]     Test net output #1: loss = 0.875062 (* 1 = 0.875062 loss)
I0113 13:16:45.182792 25204 solver.cpp:229] Iteration 41000, loss = 0.63727
I0113 13:16:45.182829 25204 solver.cpp:245]     Train net output #0: loss = 0.637266 (* 1 = 0.637266 loss)
I0113 13:16:45.182835 25204 sgd_solver.cpp:106] Iteration 41000, lr = 0.001
I0113 13:17:06.061033 25204 solver.cpp:229] Iteration 41200, loss = 1.48251
I0113 13:17:06.061074 25204 solver.cpp:245]     Train net output #0: loss = 1.48251 (* 1 = 1.48251 loss)
I0113 13:17:06.061079 25204 sgd_solver.cpp:106] Iteration 41200, lr = 0.001
I0113 13:17:26.920433 25204 solver.cpp:229] Iteration 41400, loss = 0.166632
I0113 13:17:26.920492 25204 solver.cpp:245]     Train net output #0: loss = 0.166629 (* 1 = 0.166629 loss)
I0113 13:17:26.920498 25204 sgd_solver.cpp:106] Iteration 41400, lr = 0.001
I0113 13:17:47.784581 25204 solver.cpp:229] Iteration 41600, loss = 0.780867
I0113 13:17:47.784621 25204 solver.cpp:245]     Train net output #0: loss = 0.780864 (* 1 = 0.780864 loss)
I0113 13:17:47.784626 25204 sgd_solver.cpp:106] Iteration 41600, lr = 0.001
I0113 13:18:08.654155 25204 solver.cpp:229] Iteration 41800, loss = 0.895468
I0113 13:18:08.654229 25204 solver.cpp:245]     Train net output #0: loss = 0.895464 (* 1 = 0.895464 loss)
I0113 13:18:08.654237 25204 sgd_solver.cpp:106] Iteration 41800, lr = 0.001
I0113 13:18:29.406406 25204 solver.cpp:338] Iteration 42000, Testing net (#0)
I0113 13:19:15.659059 25204 solver.cpp:406]     Test net output #0: accuracy = 0.67821
I0113 13:19:15.659111 25204 solver.cpp:406]     Test net output #1: loss = 0.860773 (* 1 = 0.860773 loss)
I0113 13:19:15.714844 25204 solver.cpp:229] Iteration 42000, loss = 0.368621
I0113 13:19:15.714881 25204 solver.cpp:245]     Train net output #0: loss = 0.368617 (* 1 = 0.368617 loss)
I0113 13:19:15.714887 25204 sgd_solver.cpp:106] Iteration 42000, lr = 0.001
I0113 13:19:36.587157 25204 solver.cpp:229] Iteration 42200, loss = 0.91041
I0113 13:19:36.587195 25204 solver.cpp:245]     Train net output #0: loss = 0.910406 (* 1 = 0.910406 loss)
I0113 13:19:36.587201 25204 sgd_solver.cpp:106] Iteration 42200, lr = 0.001
I0113 13:19:57.456502 25204 solver.cpp:229] Iteration 42400, loss = 0.412865
I0113 13:19:57.456552 25204 solver.cpp:245]     Train net output #0: loss = 0.412862 (* 1 = 0.412862 loss)
I0113 13:19:57.456558 25204 sgd_solver.cpp:106] Iteration 42400, lr = 0.001
I0113 13:20:18.312726 25204 solver.cpp:229] Iteration 42600, loss = 0.912039
I0113 13:20:18.312767 25204 solver.cpp:245]     Train net output #0: loss = 0.912037 (* 1 = 0.912037 loss)
I0113 13:20:18.312772 25204 sgd_solver.cpp:106] Iteration 42600, lr = 0.001
I0113 13:20:39.179399 25204 solver.cpp:229] Iteration 42800, loss = 0.880796
I0113 13:20:39.179451 25204 solver.cpp:245]     Train net output #0: loss = 0.880793 (* 1 = 0.880793 loss)
I0113 13:20:39.179457 25204 sgd_solver.cpp:106] Iteration 42800, lr = 0.001
I0113 13:20:59.935648 25204 solver.cpp:338] Iteration 43000, Testing net (#0)
I0113 13:21:46.199546 25204 solver.cpp:406]     Test net output #0: accuracy = 0.677979
I0113 13:21:46.199622 25204 solver.cpp:406]     Test net output #1: loss = 0.930657 (* 1 = 0.930657 loss)
I0113 13:21:46.255340 25204 solver.cpp:229] Iteration 43000, loss = 0.177008
I0113 13:21:46.255379 25204 solver.cpp:245]     Train net output #0: loss = 0.177006 (* 1 = 0.177006 loss)
I0113 13:21:46.255385 25204 sgd_solver.cpp:106] Iteration 43000, lr = 0.001
I0113 13:22:07.122277 25204 solver.cpp:229] Iteration 43200, loss = 0.535826
I0113 13:22:07.122318 25204 solver.cpp:245]     Train net output #0: loss = 0.535823 (* 1 = 0.535823 loss)
I0113 13:22:07.122323 25204 sgd_solver.cpp:106] Iteration 43200, lr = 0.001
I0113 13:22:27.988571 25204 solver.cpp:229] Iteration 43400, loss = 0.8715
I0113 13:22:27.988674 25204 solver.cpp:245]     Train net output #0: loss = 0.871497 (* 1 = 0.871497 loss)
I0113 13:22:27.988680 25204 sgd_solver.cpp:106] Iteration 43400, lr = 0.001
I0113 13:22:48.845499 25204 solver.cpp:229] Iteration 43600, loss = 0.168246
I0113 13:22:48.845538 25204 solver.cpp:245]     Train net output #0: loss = 0.168244 (* 1 = 0.168244 loss)
I0113 13:22:48.845544 25204 sgd_solver.cpp:106] Iteration 43600, lr = 0.001
I0113 13:23:09.714221 25204 solver.cpp:229] Iteration 43800, loss = 0.788846
I0113 13:23:09.714283 25204 solver.cpp:245]     Train net output #0: loss = 0.788843 (* 1 = 0.788843 loss)
I0113 13:23:09.714289 25204 sgd_solver.cpp:106] Iteration 43800, lr = 0.001
I0113 13:23:30.474977 25204 solver.cpp:338] Iteration 44000, Testing net (#0)
I0113 13:24:16.735834 25204 solver.cpp:406]     Test net output #0: accuracy = 0.672419
I0113 13:24:16.735908 25204 solver.cpp:406]     Test net output #1: loss = 0.860927 (* 1 = 0.860927 loss)
I0113 13:24:16.791620 25204 solver.cpp:229] Iteration 44000, loss = 1.10428
I0113 13:24:16.791658 25204 solver.cpp:245]     Train net output #0: loss = 1.10427 (* 1 = 1.10427 loss)
I0113 13:24:16.791663 25204 sgd_solver.cpp:106] Iteration 44000, lr = 0.001
I0113 13:24:37.650180 25204 solver.cpp:229] Iteration 44200, loss = 1.08883
I0113 13:24:37.650223 25204 solver.cpp:245]     Train net output #0: loss = 1.08883 (* 1 = 1.08883 loss)
I0113 13:24:37.650230 25204 sgd_solver.cpp:106] Iteration 44200, lr = 0.001
I0113 13:24:58.514482 25204 solver.cpp:229] Iteration 44400, loss = 1.05706
I0113 13:24:58.514531 25204 solver.cpp:245]     Train net output #0: loss = 1.05706 (* 1 = 1.05706 loss)
I0113 13:24:58.514538 25204 sgd_solver.cpp:106] Iteration 44400, lr = 0.001
I0113 13:25:19.376116 25204 solver.cpp:229] Iteration 44600, loss = 0.337567
I0113 13:25:19.376154 25204 solver.cpp:245]     Train net output #0: loss = 0.337565 (* 1 = 0.337565 loss)
I0113 13:25:19.376160 25204 sgd_solver.cpp:106] Iteration 44600, lr = 0.001
I0113 13:25:40.233460 25204 solver.cpp:229] Iteration 44800, loss = 1.0664
I0113 13:25:40.233547 25204 solver.cpp:245]     Train net output #0: loss = 1.0664 (* 1 = 1.0664 loss)
I0113 13:25:40.233553 25204 sgd_solver.cpp:106] Iteration 44800, lr = 0.001
I0113 13:26:01.002776 25204 solver.cpp:338] Iteration 45000, Testing net (#0)
I0113 13:26:47.272169 25204 solver.cpp:406]     Test net output #0: accuracy = 0.674391
I0113 13:26:47.272224 25204 solver.cpp:406]     Test net output #1: loss = 0.880636 (* 1 = 0.880636 loss)
I0113 13:26:47.328008 25204 solver.cpp:229] Iteration 45000, loss = 0.844281
I0113 13:26:47.328047 25204 solver.cpp:245]     Train net output #0: loss = 0.844279 (* 1 = 0.844279 loss)
I0113 13:26:47.328052 25204 sgd_solver.cpp:106] Iteration 45000, lr = 0.001
I0113 13:27:08.197561 25204 solver.cpp:229] Iteration 45200, loss = 0.187398
I0113 13:27:08.197600 25204 solver.cpp:245]     Train net output #0: loss = 0.187396 (* 1 = 0.187396 loss)
I0113 13:27:08.197605 25204 sgd_solver.cpp:106] Iteration 45200, lr = 0.001
I0113 13:27:29.058946 25204 solver.cpp:229] Iteration 45400, loss = 0.889688
I0113 13:27:29.059044 25204 solver.cpp:245]     Train net output #0: loss = 0.889685 (* 1 = 0.889685 loss)
I0113 13:27:29.059051 25204 sgd_solver.cpp:106] Iteration 45400, lr = 0.001
I0113 13:27:49.923120 25204 solver.cpp:229] Iteration 45600, loss = 0.717272
I0113 13:27:49.923161 25204 solver.cpp:245]     Train net output #0: loss = 0.717269 (* 1 = 0.717269 loss)
I0113 13:27:49.923166 25204 sgd_solver.cpp:106] Iteration 45600, lr = 0.001
I0113 13:28:10.780339 25204 solver.cpp:229] Iteration 45800, loss = 0.158652
I0113 13:28:10.780400 25204 solver.cpp:245]     Train net output #0: loss = 0.158649 (* 1 = 0.158649 loss)
I0113 13:28:10.780405 25204 sgd_solver.cpp:106] Iteration 45800, lr = 0.001
I0113 13:28:31.539927 25204 solver.cpp:338] Iteration 46000, Testing net (#0)
I0113 13:29:17.821076 25204 solver.cpp:406]     Test net output #0: accuracy = 0.681503
I0113 13:29:17.821208 25204 solver.cpp:406]     Test net output #1: loss = 0.855506 (* 1 = 0.855506 loss)
I0113 13:29:17.876965 25204 solver.cpp:229] Iteration 46000, loss = 1.30167
I0113 13:29:17.877002 25204 solver.cpp:245]     Train net output #0: loss = 1.30166 (* 1 = 1.30166 loss)
I0113 13:29:17.877007 25204 sgd_solver.cpp:106] Iteration 46000, lr = 0.001
I0113 13:29:38.735232 25204 solver.cpp:229] Iteration 46200, loss = 0.66423
I0113 13:29:38.735273 25204 solver.cpp:245]     Train net output #0: loss = 0.664228 (* 1 = 0.664228 loss)
I0113 13:29:38.735280 25204 sgd_solver.cpp:106] Iteration 46200, lr = 0.001
I0113 13:29:59.587450 25204 solver.cpp:229] Iteration 46400, loss = 1.51436
I0113 13:29:59.587523 25204 solver.cpp:245]     Train net output #0: loss = 1.51436 (* 1 = 1.51436 loss)
I0113 13:29:59.587528 25204 sgd_solver.cpp:106] Iteration 46400, lr = 0.001
I0113 13:30:20.458065 25204 solver.cpp:229] Iteration 46600, loss = 1.2018
I0113 13:30:20.458102 25204 solver.cpp:245]     Train net output #0: loss = 1.2018 (* 1 = 1.2018 loss)
I0113 13:30:20.458108 25204 sgd_solver.cpp:106] Iteration 46600, lr = 0.001
I0113 13:30:41.320969 25204 solver.cpp:229] Iteration 46800, loss = 0.367107
I0113 13:30:41.321022 25204 solver.cpp:245]     Train net output #0: loss = 0.367105 (* 1 = 0.367105 loss)
I0113 13:30:41.321027 25204 sgd_solver.cpp:106] Iteration 46800, lr = 0.001
I0113 13:31:02.072700 25204 solver.cpp:338] Iteration 47000, Testing net (#0)
I0113 13:31:48.354270 25204 solver.cpp:406]     Test net output #0: accuracy = 0.674146
I0113 13:31:48.354342 25204 solver.cpp:406]     Test net output #1: loss = 0.859046 (* 1 = 0.859046 loss)
I0113 13:31:48.410099 25204 solver.cpp:229] Iteration 47000, loss = 0.900365
I0113 13:31:48.410138 25204 solver.cpp:245]     Train net output #0: loss = 0.900363 (* 1 = 0.900363 loss)
I0113 13:31:48.410145 25204 sgd_solver.cpp:106] Iteration 47000, lr = 0.001
I0113 13:32:09.287696 25204 solver.cpp:229] Iteration 47200, loss = 1.38117
I0113 13:32:09.287735 25204 solver.cpp:245]     Train net output #0: loss = 1.38117 (* 1 = 1.38117 loss)
I0113 13:32:09.287740 25204 sgd_solver.cpp:106] Iteration 47200, lr = 0.001
I0113 13:32:30.148950 25204 solver.cpp:229] Iteration 47400, loss = 1.30284
I0113 13:32:30.149032 25204 solver.cpp:245]     Train net output #0: loss = 1.30284 (* 1 = 1.30284 loss)
I0113 13:32:30.149039 25204 sgd_solver.cpp:106] Iteration 47400, lr = 0.001
I0113 13:32:51.009317 25204 solver.cpp:229] Iteration 47600, loss = 1.40277
I0113 13:32:51.009358 25204 solver.cpp:245]     Train net output #0: loss = 1.40277 (* 1 = 1.40277 loss)
I0113 13:32:51.009363 25204 sgd_solver.cpp:106] Iteration 47600, lr = 0.001
I0113 13:33:11.883857 25204 solver.cpp:229] Iteration 47800, loss = 1.12217
I0113 13:33:11.883906 25204 solver.cpp:245]     Train net output #0: loss = 1.12217 (* 1 = 1.12217 loss)
I0113 13:33:11.883913 25204 sgd_solver.cpp:106] Iteration 47800, lr = 0.001
I0113 13:33:32.644300 25204 solver.cpp:338] Iteration 48000, Testing net (#0)
I0113 13:34:18.933508 25204 solver.cpp:406]     Test net output #0: accuracy = 0.672867
I0113 13:34:18.933583 25204 solver.cpp:406]     Test net output #1: loss = 0.993887 (* 1 = 0.993887 loss)
I0113 13:34:18.989291 25204 solver.cpp:229] Iteration 48000, loss = 0.686524
I0113 13:34:18.989331 25204 solver.cpp:245]     Train net output #0: loss = 0.686523 (* 1 = 0.686523 loss)
I0113 13:34:18.989336 25204 sgd_solver.cpp:106] Iteration 48000, lr = 0.001
I0113 13:34:39.863698 25204 solver.cpp:229] Iteration 48200, loss = 0.605386
I0113 13:34:39.863736 25204 solver.cpp:245]     Train net output #0: loss = 0.605384 (* 1 = 0.605384 loss)
I0113 13:34:39.863742 25204 sgd_solver.cpp:106] Iteration 48200, lr = 0.001
I0113 13:35:00.744091 25204 solver.cpp:229] Iteration 48400, loss = 1.2621
I0113 13:35:00.744140 25204 solver.cpp:245]     Train net output #0: loss = 1.26209 (* 1 = 1.26209 loss)
I0113 13:35:00.744146 25204 sgd_solver.cpp:106] Iteration 48400, lr = 0.001
I0113 13:35:21.607420 25204 solver.cpp:229] Iteration 48600, loss = 0.385115
I0113 13:35:21.607465 25204 solver.cpp:245]     Train net output #0: loss = 0.385114 (* 1 = 0.385114 loss)
I0113 13:35:21.607470 25204 sgd_solver.cpp:106] Iteration 48600, lr = 0.001
I0113 13:35:42.477205 25204 solver.cpp:229] Iteration 48800, loss = 1.47681
I0113 13:35:42.477301 25204 solver.cpp:245]     Train net output #0: loss = 1.47681 (* 1 = 1.47681 loss)
I0113 13:35:42.477308 25204 sgd_solver.cpp:106] Iteration 48800, lr = 0.001
I0113 13:36:03.249974 25204 solver.cpp:338] Iteration 49000, Testing net (#0)
I0113 13:36:49.529417 25204 solver.cpp:406]     Test net output #0: accuracy = 0.677545
I0113 13:36:49.529471 25204 solver.cpp:406]     Test net output #1: loss = 0.851895 (* 1 = 0.851895 loss)
I0113 13:36:49.585268 25204 solver.cpp:229] Iteration 49000, loss = 0.394416
I0113 13:36:49.585305 25204 solver.cpp:245]     Train net output #0: loss = 0.394414 (* 1 = 0.394414 loss)
I0113 13:36:49.585310 25204 sgd_solver.cpp:106] Iteration 49000, lr = 0.001
I0113 13:37:10.454959 25204 solver.cpp:229] Iteration 49200, loss = 1.77109
I0113 13:37:10.454998 25204 solver.cpp:245]     Train net output #0: loss = 1.77109 (* 1 = 1.77109 loss)
I0113 13:37:10.455003 25204 sgd_solver.cpp:106] Iteration 49200, lr = 0.001
I0113 13:37:31.333524 25204 solver.cpp:229] Iteration 49400, loss = 0.584841
I0113 13:37:31.333575 25204 solver.cpp:245]     Train net output #0: loss = 0.58484 (* 1 = 0.58484 loss)
I0113 13:37:31.333580 25204 sgd_solver.cpp:106] Iteration 49400, lr = 0.001
I0113 13:37:52.203338 25204 solver.cpp:229] Iteration 49600, loss = 0.191637
I0113 13:37:52.203377 25204 solver.cpp:245]     Train net output #0: loss = 0.191635 (* 1 = 0.191635 loss)
I0113 13:37:52.203383 25204 sgd_solver.cpp:106] Iteration 49600, lr = 0.001
I0113 13:38:13.073210 25204 solver.cpp:229] Iteration 49800, loss = 1.17145
I0113 13:38:13.073268 25204 solver.cpp:245]     Train net output #0: loss = 1.17145 (* 1 = 1.17145 loss)
I0113 13:38:13.073274 25204 sgd_solver.cpp:106] Iteration 49800, lr = 0.001
I0113 13:38:33.841583 25204 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_50000.caffemodel.h5
I0113 13:38:33.890894 25204 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_50000.solverstate.h5
I0113 13:38:33.891517 25204 solver.cpp:338] Iteration 50000, Testing net (#0)
I0113 13:39:20.116699 25204 solver.cpp:406]     Test net output #0: accuracy = 0.680112
I0113 13:39:20.116755 25204 solver.cpp:406]     Test net output #1: loss = 0.848018 (* 1 = 0.848018 loss)
I0113 13:39:20.172528 25204 solver.cpp:229] Iteration 50000, loss = 0.982888
I0113 13:39:20.172566 25204 solver.cpp:245]     Train net output #0: loss = 0.982886 (* 1 = 0.982886 loss)
I0113 13:39:20.172572 25204 sgd_solver.cpp:106] Iteration 50000, lr = 0.001
I0113 13:39:41.044014 25204 solver.cpp:229] Iteration 50200, loss = 0.131044
I0113 13:39:41.044052 25204 solver.cpp:245]     Train net output #0: loss = 0.131043 (* 1 = 0.131043 loss)
I0113 13:39:41.044059 25204 sgd_solver.cpp:106] Iteration 50200, lr = 0.001
I0113 13:40:01.911974 25204 solver.cpp:229] Iteration 50400, loss = 0.634397
I0113 13:40:01.912024 25204 solver.cpp:245]     Train net output #0: loss = 0.634396 (* 1 = 0.634396 loss)
I0113 13:40:01.912030 25204 sgd_solver.cpp:106] Iteration 50400, lr = 0.001
I0113 13:40:22.784385 25204 solver.cpp:229] Iteration 50600, loss = 1.19563
I0113 13:40:22.784425 25204 solver.cpp:245]     Train net output #0: loss = 1.19563 (* 1 = 1.19563 loss)
I0113 13:40:22.784430 25204 sgd_solver.cpp:106] Iteration 50600, lr = 0.001
I0113 13:40:43.646005 25204 solver.cpp:229] Iteration 50800, loss = 0.266699
I0113 13:40:43.646075 25204 solver.cpp:245]     Train net output #0: loss = 0.266698 (* 1 = 0.266698 loss)
I0113 13:40:43.646081 25204 sgd_solver.cpp:106] Iteration 50800, lr = 0.001
I0113 13:41:04.417029 25204 solver.cpp:338] Iteration 51000, Testing net (#0)
I0113 13:41:50.657722 25204 solver.cpp:406]     Test net output #0: accuracy = 0.671874
I0113 13:41:50.657826 25204 solver.cpp:406]     Test net output #1: loss = 0.881311 (* 1 = 0.881311 loss)
I0113 13:41:50.713603 25204 solver.cpp:229] Iteration 51000, loss = 0.804306
I0113 13:41:50.713646 25204 solver.cpp:245]     Train net output #0: loss = 0.804305 (* 1 = 0.804305 loss)
I0113 13:41:50.713652 25204 sgd_solver.cpp:106] Iteration 51000, lr = 0.001
I0113 13:42:11.593634 25204 solver.cpp:229] Iteration 51200, loss = 1.42728
I0113 13:42:11.593675 25204 solver.cpp:245]     Train net output #0: loss = 1.42728 (* 1 = 1.42728 loss)
I0113 13:42:11.593680 25204 sgd_solver.cpp:106] Iteration 51200, lr = 0.001
I0113 13:42:32.453330 25204 solver.cpp:229] Iteration 51400, loss = 1.94896
I0113 13:42:32.453382 25204 solver.cpp:245]     Train net output #0: loss = 1.94896 (* 1 = 1.94896 loss)
I0113 13:42:32.453388 25204 sgd_solver.cpp:106] Iteration 51400, lr = 0.001
I0113 13:42:53.322763 25204 solver.cpp:229] Iteration 51600, loss = 0.832711
I0113 13:42:53.322803 25204 solver.cpp:245]     Train net output #0: loss = 0.832711 (* 1 = 0.832711 loss)
I0113 13:42:53.322809 25204 sgd_solver.cpp:106] Iteration 51600, lr = 0.001
I0113 13:43:14.190394 25204 solver.cpp:229] Iteration 51800, loss = 0.580491
I0113 13:43:14.190479 25204 solver.cpp:245]     Train net output #0: loss = 0.58049 (* 1 = 0.58049 loss)
I0113 13:43:14.190485 25204 sgd_solver.cpp:106] Iteration 51800, lr = 0.001
I0113 13:43:34.946930 25204 solver.cpp:338] Iteration 52000, Testing net (#0)
I0113 13:44:21.190726 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673958
I0113 13:44:21.190789 25204 solver.cpp:406]     Test net output #1: loss = 0.872083 (* 1 = 0.872083 loss)
I0113 13:44:21.246572 25204 solver.cpp:229] Iteration 52000, loss = 1.0899
I0113 13:44:21.246609 25204 solver.cpp:245]     Train net output #0: loss = 1.0899 (* 1 = 1.0899 loss)
I0113 13:44:21.246614 25204 sgd_solver.cpp:106] Iteration 52000, lr = 0.001
I0113 13:44:42.120065 25204 solver.cpp:229] Iteration 52200, loss = 0.390378
I0113 13:44:42.120105 25204 solver.cpp:245]     Train net output #0: loss = 0.390377 (* 1 = 0.390377 loss)
I0113 13:44:42.120111 25204 sgd_solver.cpp:106] Iteration 52200, lr = 0.001
I0113 13:45:02.988910 25204 solver.cpp:229] Iteration 52400, loss = 0.139892
I0113 13:45:02.988993 25204 solver.cpp:245]     Train net output #0: loss = 0.139891 (* 1 = 0.139891 loss)
I0113 13:45:02.988999 25204 sgd_solver.cpp:106] Iteration 52400, lr = 0.001
I0113 13:45:23.850375 25204 solver.cpp:229] Iteration 52600, loss = 0.9168
I0113 13:45:23.850415 25204 solver.cpp:245]     Train net output #0: loss = 0.916798 (* 1 = 0.916798 loss)
I0113 13:45:23.850420 25204 sgd_solver.cpp:106] Iteration 52600, lr = 0.001
I0113 13:45:44.718729 25204 solver.cpp:229] Iteration 52800, loss = 1.10771
I0113 13:45:44.718780 25204 solver.cpp:245]     Train net output #0: loss = 1.10771 (* 1 = 1.10771 loss)
I0113 13:45:44.718785 25204 sgd_solver.cpp:106] Iteration 52800, lr = 0.001
I0113 13:46:05.474562 25204 solver.cpp:338] Iteration 53000, Testing net (#0)
I0113 13:46:51.712721 25204 solver.cpp:406]     Test net output #0: accuracy = 0.680209
I0113 13:46:51.712774 25204 solver.cpp:406]     Test net output #1: loss = 0.890242 (* 1 = 0.890242 loss)
I0113 13:46:51.768539 25204 solver.cpp:229] Iteration 53000, loss = 1.274
I0113 13:46:51.768577 25204 solver.cpp:245]     Train net output #0: loss = 1.274 (* 1 = 1.274 loss)
I0113 13:46:51.768582 25204 sgd_solver.cpp:106] Iteration 53000, lr = 0.001
I0113 13:47:12.643800 25204 solver.cpp:229] Iteration 53200, loss = 1.02089
I0113 13:47:12.643838 25204 solver.cpp:245]     Train net output #0: loss = 1.02089 (* 1 = 1.02089 loss)
I0113 13:47:12.643844 25204 sgd_solver.cpp:106] Iteration 53200, lr = 0.001
I0113 13:47:33.512562 25204 solver.cpp:229] Iteration 53400, loss = 0.444429
I0113 13:47:33.512624 25204 solver.cpp:245]     Train net output #0: loss = 0.444427 (* 1 = 0.444427 loss)
I0113 13:47:33.512629 25204 sgd_solver.cpp:106] Iteration 53400, lr = 0.001
I0113 13:47:54.367743 25204 solver.cpp:229] Iteration 53600, loss = 1.97202
I0113 13:47:54.367782 25204 solver.cpp:245]     Train net output #0: loss = 1.97202 (* 1 = 1.97202 loss)
I0113 13:47:54.367789 25204 sgd_solver.cpp:106] Iteration 53600, lr = 0.001
I0113 13:48:15.242812 25204 solver.cpp:229] Iteration 53800, loss = 1.03439
I0113 13:48:15.242933 25204 solver.cpp:245]     Train net output #0: loss = 1.03439 (* 1 = 1.03439 loss)
I0113 13:48:15.242939 25204 sgd_solver.cpp:106] Iteration 53800, lr = 0.001
I0113 13:48:36.003901 25204 solver.cpp:338] Iteration 54000, Testing net (#0)
I0113 13:49:22.245136 25204 solver.cpp:406]     Test net output #0: accuracy = 0.675999
I0113 13:49:22.245193 25204 solver.cpp:406]     Test net output #1: loss = 0.934961 (* 1 = 0.934961 loss)
I0113 13:49:22.300863 25204 solver.cpp:229] Iteration 54000, loss = 0.292664
I0113 13:49:22.300899 25204 solver.cpp:245]     Train net output #0: loss = 0.292662 (* 1 = 0.292662 loss)
I0113 13:49:22.300904 25204 sgd_solver.cpp:106] Iteration 54000, lr = 0.001
I0113 13:49:43.157160 25204 solver.cpp:229] Iteration 54200, loss = 0.682393
I0113 13:49:43.157202 25204 solver.cpp:245]     Train net output #0: loss = 0.682391 (* 1 = 0.682391 loss)
I0113 13:49:43.157207 25204 sgd_solver.cpp:106] Iteration 54200, lr = 0.001
I0113 13:50:04.030009 25204 solver.cpp:229] Iteration 54400, loss = 0.624434
I0113 13:50:04.030061 25204 solver.cpp:245]     Train net output #0: loss = 0.624432 (* 1 = 0.624432 loss)
I0113 13:50:04.030066 25204 sgd_solver.cpp:106] Iteration 54400, lr = 0.001
I0113 13:50:24.888411 25204 solver.cpp:229] Iteration 54600, loss = 1.05488
I0113 13:50:24.888449 25204 solver.cpp:245]     Train net output #0: loss = 1.05488 (* 1 = 1.05488 loss)
I0113 13:50:24.888454 25204 sgd_solver.cpp:106] Iteration 54600, lr = 0.001
I0113 13:50:45.748021 25204 solver.cpp:229] Iteration 54800, loss = 0.791668
I0113 13:50:45.748073 25204 solver.cpp:245]     Train net output #0: loss = 0.791666 (* 1 = 0.791666 loss)
I0113 13:50:45.748080 25204 sgd_solver.cpp:106] Iteration 54800, lr = 0.001
I0113 13:51:06.516809 25204 solver.cpp:338] Iteration 55000, Testing net (#0)
I0113 13:51:52.755676 25204 solver.cpp:406]     Test net output #0: accuracy = 0.671748
I0113 13:51:52.755753 25204 solver.cpp:406]     Test net output #1: loss = 0.859602 (* 1 = 0.859602 loss)
I0113 13:51:52.811460 25204 solver.cpp:229] Iteration 55000, loss = 1.67418
I0113 13:51:52.811497 25204 solver.cpp:245]     Train net output #0: loss = 1.67418 (* 1 = 1.67418 loss)
I0113 13:51:52.811502 25204 sgd_solver.cpp:106] Iteration 55000, lr = 0.001
I0113 13:52:13.675509 25204 solver.cpp:229] Iteration 55200, loss = 1.03715
I0113 13:52:13.675549 25204 solver.cpp:245]     Train net output #0: loss = 1.03715 (* 1 = 1.03715 loss)
I0113 13:52:13.675554 25204 sgd_solver.cpp:106] Iteration 55200, lr = 0.001
I0113 13:52:34.537935 25204 solver.cpp:229] Iteration 55400, loss = 1.34613
I0113 13:52:34.538030 25204 solver.cpp:245]     Train net output #0: loss = 1.34613 (* 1 = 1.34613 loss)
I0113 13:52:34.538035 25204 sgd_solver.cpp:106] Iteration 55400, lr = 0.001
I0113 13:52:55.401890 25204 solver.cpp:229] Iteration 55600, loss = 0.461663
I0113 13:52:55.401929 25204 solver.cpp:245]     Train net output #0: loss = 0.461662 (* 1 = 0.461662 loss)
I0113 13:52:55.401934 25204 sgd_solver.cpp:106] Iteration 55600, lr = 0.001
I0113 13:53:16.265393 25204 solver.cpp:229] Iteration 55800, loss = 1.58225
I0113 13:53:16.265460 25204 solver.cpp:245]     Train net output #0: loss = 1.58225 (* 1 = 1.58225 loss)
I0113 13:53:16.265466 25204 sgd_solver.cpp:106] Iteration 55800, lr = 0.001
I0113 13:53:37.028173 25204 solver.cpp:338] Iteration 56000, Testing net (#0)
I0113 13:54:23.285114 25204 solver.cpp:406]     Test net output #0: accuracy = 0.676447
I0113 13:54:23.285176 25204 solver.cpp:406]     Test net output #1: loss = 0.890141 (* 1 = 0.890141 loss)
I0113 13:54:23.340967 25204 solver.cpp:229] Iteration 56000, loss = 0.934614
I0113 13:54:23.341003 25204 solver.cpp:245]     Train net output #0: loss = 0.934613 (* 1 = 0.934613 loss)
I0113 13:54:23.341009 25204 sgd_solver.cpp:106] Iteration 56000, lr = 0.001
I0113 13:54:44.204592 25204 solver.cpp:229] Iteration 56200, loss = 0.898014
I0113 13:54:44.204632 25204 solver.cpp:245]     Train net output #0: loss = 0.898013 (* 1 = 0.898013 loss)
I0113 13:54:44.204666 25204 sgd_solver.cpp:106] Iteration 56200, lr = 0.001
I0113 13:55:05.066038 25204 solver.cpp:229] Iteration 56400, loss = 1.1825
I0113 13:55:05.066143 25204 solver.cpp:245]     Train net output #0: loss = 1.1825 (* 1 = 1.1825 loss)
I0113 13:55:05.066149 25204 sgd_solver.cpp:106] Iteration 56400, lr = 0.001
I0113 13:55:25.932608 25204 solver.cpp:229] Iteration 56600, loss = 1.51481
I0113 13:55:25.932649 25204 solver.cpp:245]     Train net output #0: loss = 1.51481 (* 1 = 1.51481 loss)
I0113 13:55:25.932656 25204 sgd_solver.cpp:106] Iteration 56600, lr = 0.001
I0113 13:55:46.784886 25204 solver.cpp:229] Iteration 56800, loss = 0.156682
I0113 13:55:46.785001 25204 solver.cpp:245]     Train net output #0: loss = 0.15668 (* 1 = 0.15668 loss)
I0113 13:55:46.785007 25204 sgd_solver.cpp:106] Iteration 56800, lr = 0.001
I0113 13:56:07.546169 25204 solver.cpp:338] Iteration 57000, Testing net (#0)
I0113 13:56:53.818135 25204 solver.cpp:406]     Test net output #0: accuracy = 0.68072
I0113 13:56:53.818212 25204 solver.cpp:406]     Test net output #1: loss = 0.892683 (* 1 = 0.892683 loss)
I0113 13:56:53.873921 25204 solver.cpp:229] Iteration 57000, loss = 1.19938
I0113 13:56:53.873960 25204 solver.cpp:245]     Train net output #0: loss = 1.19938 (* 1 = 1.19938 loss)
I0113 13:56:53.873965 25204 sgd_solver.cpp:106] Iteration 57000, lr = 0.001
I0113 13:57:14.737889 25204 solver.cpp:229] Iteration 57200, loss = 0.834688
I0113 13:57:14.737927 25204 solver.cpp:245]     Train net output #0: loss = 0.834686 (* 1 = 0.834686 loss)
I0113 13:57:14.737933 25204 sgd_solver.cpp:106] Iteration 57200, lr = 0.001
I0113 13:57:35.587756 25204 solver.cpp:229] Iteration 57400, loss = 0.296252
I0113 13:57:35.587828 25204 solver.cpp:245]     Train net output #0: loss = 0.29625 (* 1 = 0.29625 loss)
I0113 13:57:35.587834 25204 sgd_solver.cpp:106] Iteration 57400, lr = 0.001
I0113 13:57:56.448807 25204 solver.cpp:229] Iteration 57600, loss = 1.30145
I0113 13:57:56.448846 25204 solver.cpp:245]     Train net output #0: loss = 1.30145 (* 1 = 1.30145 loss)
I0113 13:57:56.448853 25204 sgd_solver.cpp:106] Iteration 57600, lr = 0.001
I0113 13:58:17.306337 25204 solver.cpp:229] Iteration 57800, loss = 1.12612
I0113 13:58:17.306401 25204 solver.cpp:245]     Train net output #0: loss = 1.12611 (* 1 = 1.12611 loss)
I0113 13:58:17.306406 25204 sgd_solver.cpp:106] Iteration 57800, lr = 0.001
I0113 13:58:38.052973 25204 solver.cpp:338] Iteration 58000, Testing net (#0)
I0113 13:59:58.800498 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673307
I0113 13:59:58.800575 25204 solver.cpp:406]     Test net output #1: loss = 0.892054 (* 1 = 0.892054 loss)
I0113 13:59:58.899494 25204 solver.cpp:229] Iteration 58000, loss = 0.240692
I0113 13:59:58.899538 25204 solver.cpp:245]     Train net output #0: loss = 0.240691 (* 1 = 0.240691 loss)
I0113 13:59:58.899545 25204 sgd_solver.cpp:106] Iteration 58000, lr = 0.001
I0113 14:00:36.581595 25204 solver.cpp:229] Iteration 58200, loss = 0.949482
I0113 14:00:36.581645 25204 solver.cpp:245]     Train net output #0: loss = 0.94948 (* 1 = 0.94948 loss)
I0113 14:00:36.581650 25204 sgd_solver.cpp:106] Iteration 58200, lr = 0.001
I0113 14:01:20.091681 25204 solver.cpp:229] Iteration 58400, loss = 0.20526
I0113 14:01:20.091729 25204 solver.cpp:245]     Train net output #0: loss = 0.205259 (* 1 = 0.205259 loss)
I0113 14:01:20.091735 25204 sgd_solver.cpp:106] Iteration 58400, lr = 0.001
I0113 14:02:03.616878 25204 solver.cpp:229] Iteration 58600, loss = 0.76937
I0113 14:02:03.616926 25204 solver.cpp:245]     Train net output #0: loss = 0.769369 (* 1 = 0.769369 loss)
I0113 14:02:03.616932 25204 sgd_solver.cpp:106] Iteration 58600, lr = 0.001
I0113 14:02:47.156780 25204 solver.cpp:229] Iteration 58800, loss = 0.902314
I0113 14:02:47.156827 25204 solver.cpp:245]     Train net output #0: loss = 0.902312 (* 1 = 0.902312 loss)
I0113 14:02:47.156833 25204 sgd_solver.cpp:106] Iteration 58800, lr = 0.001
I0113 14:03:30.492457 25204 solver.cpp:338] Iteration 59000, Testing net (#0)
I0113 14:05:12.726501 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673531
I0113 14:05:12.726578 25204 solver.cpp:406]     Test net output #1: loss = 0.954983 (* 1 = 0.954983 loss)
I0113 14:05:12.825013 25204 solver.cpp:229] Iteration 59000, loss = 0.16403
I0113 14:05:12.825058 25204 solver.cpp:245]     Train net output #0: loss = 0.164028 (* 1 = 0.164028 loss)
I0113 14:05:12.825065 25204 sgd_solver.cpp:106] Iteration 59000, lr = 0.001
I0113 14:05:50.508340 25204 solver.cpp:229] Iteration 59200, loss = 0.874611
I0113 14:05:50.508389 25204 solver.cpp:245]     Train net output #0: loss = 0.87461 (* 1 = 0.87461 loss)
I0113 14:05:50.508394 25204 sgd_solver.cpp:106] Iteration 59200, lr = 0.001
I0113 14:06:34.061980 25204 solver.cpp:229] Iteration 59400, loss = 1.32399
I0113 14:06:34.062026 25204 solver.cpp:245]     Train net output #0: loss = 1.32399 (* 1 = 1.32399 loss)
I0113 14:06:34.062032 25204 sgd_solver.cpp:106] Iteration 59400, lr = 0.001
I0113 14:07:17.625808 25204 solver.cpp:229] Iteration 59600, loss = 0.46394
I0113 14:07:17.625854 25204 solver.cpp:245]     Train net output #0: loss = 0.463938 (* 1 = 0.463938 loss)
I0113 14:07:17.625860 25204 sgd_solver.cpp:106] Iteration 59600, lr = 0.001
I0113 14:08:01.193241 25204 solver.cpp:229] Iteration 59800, loss = 0.68584
I0113 14:08:01.193289 25204 solver.cpp:245]     Train net output #0: loss = 0.685838 (* 1 = 0.685838 loss)
I0113 14:08:01.193296 25204 sgd_solver.cpp:106] Iteration 59800, lr = 0.001
I0113 14:08:44.551570 25204 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_60000.caffemodel.h5
I0113 14:08:44.653730 25204 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_60000.solverstate.h5
I0113 14:08:44.654348 25204 solver.cpp:338] Iteration 60000, Testing net (#0)
I0113 14:10:26.809679 25204 solver.cpp:406]     Test net output #0: accuracy = 0.678839
I0113 14:10:26.809744 25204 solver.cpp:406]     Test net output #1: loss = 0.856978 (* 1 = 0.856978 loss)
I0113 14:10:26.922205 25204 solver.cpp:229] Iteration 60000, loss = 0.886853
I0113 14:10:26.922246 25204 solver.cpp:245]     Train net output #0: loss = 0.886852 (* 1 = 0.886852 loss)
I0113 14:10:26.922252 25204 sgd_solver.cpp:106] Iteration 60000, lr = 0.001
I0113 14:11:04.598232 25204 solver.cpp:229] Iteration 60200, loss = 0.29903
I0113 14:11:04.598309 25204 solver.cpp:245]     Train net output #0: loss = 0.299029 (* 1 = 0.299029 loss)
I0113 14:11:04.598315 25204 sgd_solver.cpp:106] Iteration 60200, lr = 0.001
I0113 14:11:48.169075 25204 solver.cpp:229] Iteration 60400, loss = 1.04057
I0113 14:11:48.169123 25204 solver.cpp:245]     Train net output #0: loss = 1.04057 (* 1 = 1.04057 loss)
I0113 14:11:48.169129 25204 sgd_solver.cpp:106] Iteration 60400, lr = 0.001
I0113 14:12:31.748524 25204 solver.cpp:229] Iteration 60600, loss = 0.474579
I0113 14:12:31.748571 25204 solver.cpp:245]     Train net output #0: loss = 0.474578 (* 1 = 0.474578 loss)
I0113 14:12:31.748577 25204 sgd_solver.cpp:106] Iteration 60600, lr = 0.001
I0113 14:13:15.334312 25204 solver.cpp:229] Iteration 60800, loss = 1.18744
I0113 14:13:15.334368 25204 solver.cpp:245]     Train net output #0: loss = 1.18744 (* 1 = 1.18744 loss)
I0113 14:13:15.334374 25204 sgd_solver.cpp:106] Iteration 60800, lr = 0.001
I0113 14:13:58.687101 25204 solver.cpp:338] Iteration 61000, Testing net (#0)
I0113 14:15:40.945793 25204 solver.cpp:406]     Test net output #0: accuracy = 0.677174
I0113 14:15:40.945858 25204 solver.cpp:406]     Test net output #1: loss = 0.854384 (* 1 = 0.854384 loss)
I0113 14:15:41.060577 25204 solver.cpp:229] Iteration 61000, loss = 0.981417
I0113 14:15:41.060618 25204 solver.cpp:245]     Train net output #0: loss = 0.981416 (* 1 = 0.981416 loss)
I0113 14:15:41.060624 25204 sgd_solver.cpp:106] Iteration 61000, lr = 0.001
I0113 14:16:18.690354 25204 solver.cpp:229] Iteration 61200, loss = 1.19897
I0113 14:16:18.690403 25204 solver.cpp:245]     Train net output #0: loss = 1.19897 (* 1 = 1.19897 loss)
I0113 14:16:18.690409 25204 sgd_solver.cpp:106] Iteration 61200, lr = 0.001
I0113 14:17:02.294780 25204 solver.cpp:229] Iteration 61400, loss = 1.19525
I0113 14:17:02.294867 25204 solver.cpp:245]     Train net output #0: loss = 1.19525 (* 1 = 1.19525 loss)
I0113 14:17:02.294872 25204 sgd_solver.cpp:106] Iteration 61400, lr = 0.001
I0113 14:17:45.874979 25204 solver.cpp:229] Iteration 61600, loss = 0.668294
I0113 14:17:45.875033 25204 solver.cpp:245]     Train net output #0: loss = 0.668294 (* 1 = 0.668294 loss)
I0113 14:17:45.875038 25204 sgd_solver.cpp:106] Iteration 61600, lr = 0.001
I0113 14:18:29.457600 25204 solver.cpp:229] Iteration 61800, loss = 0.159173
I0113 14:18:29.457650 25204 solver.cpp:245]     Train net output #0: loss = 0.159172 (* 1 = 0.159172 loss)
I0113 14:18:29.457656 25204 sgd_solver.cpp:106] Iteration 61800, lr = 0.001
I0113 14:19:12.805533 25204 solver.cpp:338] Iteration 62000, Testing net (#0)
I0113 14:20:55.044829 25204 solver.cpp:406]     Test net output #0: accuracy = 0.672083
I0113 14:20:55.044891 25204 solver.cpp:406]     Test net output #1: loss = 0.859444 (* 1 = 0.859444 loss)
I0113 14:20:55.143757 25204 solver.cpp:229] Iteration 62000, loss = 1.65045
I0113 14:20:55.143801 25204 solver.cpp:245]     Train net output #0: loss = 1.65044 (* 1 = 1.65044 loss)
I0113 14:20:55.143808 25204 sgd_solver.cpp:106] Iteration 62000, lr = 0.001
I0113 14:21:32.787534 25204 solver.cpp:229] Iteration 62200, loss = 0.827164
I0113 14:21:32.787587 25204 solver.cpp:245]     Train net output #0: loss = 0.827164 (* 1 = 0.827164 loss)
I0113 14:21:32.787593 25204 sgd_solver.cpp:106] Iteration 62200, lr = 0.001
I0113 14:22:16.327426 25204 solver.cpp:229] Iteration 62400, loss = 0.289604
I0113 14:22:16.327481 25204 solver.cpp:245]     Train net output #0: loss = 0.289604 (* 1 = 0.289604 loss)
I0113 14:22:16.327488 25204 sgd_solver.cpp:106] Iteration 62400, lr = 0.001
I0113 14:22:59.877840 25204 solver.cpp:229] Iteration 62600, loss = 1.24023
I0113 14:22:59.877893 25204 solver.cpp:245]     Train net output #0: loss = 1.24023 (* 1 = 1.24023 loss)
I0113 14:22:59.877899 25204 sgd_solver.cpp:106] Iteration 62600, lr = 0.001
I0113 14:23:43.405751 25204 solver.cpp:229] Iteration 62800, loss = 0.254189
I0113 14:23:43.405815 25204 solver.cpp:245]     Train net output #0: loss = 0.254189 (* 1 = 0.254189 loss)
I0113 14:23:43.405822 25204 sgd_solver.cpp:106] Iteration 62800, lr = 0.001
I0113 14:24:26.735141 25204 solver.cpp:338] Iteration 63000, Testing net (#0)
I0113 14:26:08.969858 25204 solver.cpp:406]     Test net output #0: accuracy = 0.674867
I0113 14:26:08.969928 25204 solver.cpp:406]     Test net output #1: loss = 0.883262 (* 1 = 0.883262 loss)
I0113 14:26:09.070919 25204 solver.cpp:229] Iteration 63000, loss = 0.838054
I0113 14:26:09.070961 25204 solver.cpp:245]     Train net output #0: loss = 0.838055 (* 1 = 0.838055 loss)
I0113 14:26:09.070969 25204 sgd_solver.cpp:106] Iteration 63000, lr = 0.001
I0113 14:26:46.698627 25204 solver.cpp:229] Iteration 63200, loss = 0.429209
I0113 14:26:46.698674 25204 solver.cpp:245]     Train net output #0: loss = 0.429209 (* 1 = 0.429209 loss)
I0113 14:26:46.698680 25204 sgd_solver.cpp:106] Iteration 63200, lr = 0.001
I0113 14:27:30.235498 25204 solver.cpp:229] Iteration 63400, loss = 0.170373
I0113 14:27:30.235555 25204 solver.cpp:245]     Train net output #0: loss = 0.170374 (* 1 = 0.170374 loss)
I0113 14:27:30.235561 25204 sgd_solver.cpp:106] Iteration 63400, lr = 0.001
I0113 14:28:13.780261 25204 solver.cpp:229] Iteration 63600, loss = 1.25127
I0113 14:28:13.780310 25204 solver.cpp:245]     Train net output #0: loss = 1.25127 (* 1 = 1.25127 loss)
I0113 14:28:13.780316 25204 sgd_solver.cpp:106] Iteration 63600, lr = 0.001
I0113 14:28:57.363714 25204 solver.cpp:229] Iteration 63800, loss = 0.396784
I0113 14:28:57.363762 25204 solver.cpp:245]     Train net output #0: loss = 0.396785 (* 1 = 0.396785 loss)
I0113 14:28:57.363768 25204 sgd_solver.cpp:106] Iteration 63800, lr = 0.001
I0113 14:29:40.699097 25204 solver.cpp:338] Iteration 64000, Testing net (#0)
I0113 14:31:22.946079 25204 solver.cpp:406]     Test net output #0: accuracy = 0.681468
I0113 14:31:22.946135 25204 solver.cpp:406]     Test net output #1: loss = 0.942817 (* 1 = 0.942817 loss)
I0113 14:31:23.044548 25204 solver.cpp:229] Iteration 64000, loss = 0.279813
I0113 14:31:23.044591 25204 solver.cpp:245]     Train net output #0: loss = 0.279814 (* 1 = 0.279814 loss)
I0113 14:31:23.044597 25204 sgd_solver.cpp:106] Iteration 64000, lr = 0.001
I0113 14:32:00.669704 25204 solver.cpp:229] Iteration 64200, loss = 1.17585
I0113 14:32:00.669755 25204 solver.cpp:245]     Train net output #0: loss = 1.17586 (* 1 = 1.17586 loss)
I0113 14:32:00.669761 25204 sgd_solver.cpp:106] Iteration 64200, lr = 0.001
I0113 14:32:44.247923 25204 solver.cpp:229] Iteration 64400, loss = 1.20805
I0113 14:32:44.247975 25204 solver.cpp:245]     Train net output #0: loss = 1.20805 (* 1 = 1.20805 loss)
I0113 14:32:44.247982 25204 sgd_solver.cpp:106] Iteration 64400, lr = 0.001
I0113 14:33:27.842253 25204 solver.cpp:229] Iteration 64600, loss = 0.312844
I0113 14:33:27.842303 25204 solver.cpp:245]     Train net output #0: loss = 0.312845 (* 1 = 0.312845 loss)
I0113 14:33:27.842309 25204 sgd_solver.cpp:106] Iteration 64600, lr = 0.001
I0113 14:34:11.425478 25204 solver.cpp:229] Iteration 64800, loss = 0.911846
I0113 14:34:11.425529 25204 solver.cpp:245]     Train net output #0: loss = 0.911847 (* 1 = 0.911847 loss)
I0113 14:34:11.425535 25204 sgd_solver.cpp:106] Iteration 64800, lr = 0.001
I0113 14:34:54.760035 25204 solver.cpp:338] Iteration 65000, Testing net (#0)
I0113 14:36:37.006453 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673482
I0113 14:36:37.006515 25204 solver.cpp:406]     Test net output #1: loss = 0.875524 (* 1 = 0.875524 loss)
I0113 14:36:37.105067 25204 solver.cpp:229] Iteration 65000, loss = 0.278637
I0113 14:36:37.105111 25204 solver.cpp:245]     Train net output #0: loss = 0.278638 (* 1 = 0.278638 loss)
I0113 14:36:37.105118 25204 sgd_solver.cpp:106] Iteration 65000, lr = 0.001
I0113 14:37:14.742132 25204 solver.cpp:229] Iteration 65200, loss = 1.11077
I0113 14:37:14.742183 25204 solver.cpp:245]     Train net output #0: loss = 1.11077 (* 1 = 1.11077 loss)
I0113 14:37:14.742190 25204 sgd_solver.cpp:106] Iteration 65200, lr = 0.001
I0113 14:37:58.317193 25204 solver.cpp:229] Iteration 65400, loss = 0.465538
I0113 14:37:58.317263 25204 solver.cpp:245]     Train net output #0: loss = 0.465539 (* 1 = 0.465539 loss)
I0113 14:37:58.317270 25204 sgd_solver.cpp:106] Iteration 65400, lr = 0.001
I0113 14:38:41.846463 25204 solver.cpp:229] Iteration 65600, loss = 0.170626
I0113 14:38:41.846513 25204 solver.cpp:245]     Train net output #0: loss = 0.170627 (* 1 = 0.170627 loss)
I0113 14:38:41.846519 25204 sgd_solver.cpp:106] Iteration 65600, lr = 0.001
I0113 14:39:25.410884 25204 solver.cpp:229] Iteration 65800, loss = 1.52965
I0113 14:39:25.410933 25204 solver.cpp:245]     Train net output #0: loss = 1.52965 (* 1 = 1.52965 loss)
I0113 14:39:25.410939 25204 sgd_solver.cpp:106] Iteration 65800, lr = 0.001
I0113 14:40:08.764422 25204 solver.cpp:338] Iteration 66000, Testing net (#0)
I0113 14:41:50.991788 25204 solver.cpp:406]     Test net output #0: accuracy = 0.673657
I0113 14:41:50.991842 25204 solver.cpp:406]     Test net output #1: loss = 0.857013 (* 1 = 0.857013 loss)
I0113 14:41:51.088469 25204 solver.cpp:229] Iteration 66000, loss = 0.400222
I0113 14:41:51.088520 25204 solver.cpp:245]     Train net output #0: loss = 0.400223 (* 1 = 0.400223 loss)
I0113 14:41:51.088526 25204 sgd_solver.cpp:106] Iteration 66000, lr = 0.001
I0113 14:42:28.697563 25204 solver.cpp:229] Iteration 66200, loss = 0.528122
I0113 14:42:28.697614 25204 solver.cpp:245]     Train net output #0: loss = 0.528123 (* 1 = 0.528123 loss)
I0113 14:42:28.697621 25204 sgd_solver.cpp:106] Iteration 66200, lr = 0.001
I0113 14:43:12.240154 25204 solver.cpp:229] Iteration 66400, loss = 0.432603
I0113 14:43:12.240207 25204 solver.cpp:245]     Train net output #0: loss = 0.432604 (* 1 = 0.432604 loss)
I0113 14:43:12.240213 25204 sgd_solver.cpp:106] Iteration 66400, lr = 0.001
I0113 14:43:55.796771 25204 solver.cpp:229] Iteration 66600, loss = 0.71294
I0113 14:43:55.796860 25204 solver.cpp:245]     Train net output #0: loss = 0.712941 (* 1 = 0.712941 loss)
I0113 14:43:55.796871 25204 sgd_solver.cpp:106] Iteration 66600, lr = 0.001
I0113 14:44:39.316299 25204 solver.cpp:229] Iteration 66800, loss = 0.48349
I0113 14:44:39.316356 25204 solver.cpp:245]     Train net output #0: loss = 0.483491 (* 1 = 0.483491 loss)
I0113 14:44:39.316364 25204 sgd_solver.cpp:106] Iteration 66800, lr = 0.001
I0113 14:45:22.648071 25204 solver.cpp:338] Iteration 67000, Testing net (#0)
I0113 14:47:04.906020 25204 solver.cpp:406]     Test net output #0: accuracy = 0.677874
I0113 14:47:04.906072 25204 solver.cpp:406]     Test net output #1: loss = 0.910108 (* 1 = 0.910108 loss)
I0113 14:47:05.004823 25204 solver.cpp:229] Iteration 67000, loss = 0.741547
I0113 14:47:05.004870 25204 solver.cpp:245]     Train net output #0: loss = 0.741547 (* 1 = 0.741547 loss)
I0113 14:47:05.004876 25204 sgd_solver.cpp:106] Iteration 67000, lr = 0.001
I0113 14:47:42.582538 25204 solver.cpp:229] Iteration 67200, loss = 0.304047
I0113 14:47:42.582587 25204 solver.cpp:245]     Train net output #0: loss = 0.304048 (* 1 = 0.304048 loss)
I0113 14:47:42.582592 25204 sgd_solver.cpp:106] Iteration 67200, lr = 0.001
I0113 14:48:26.087167 25204 solver.cpp:229] Iteration 67400, loss = 0.786386
I0113 14:48:26.087220 25204 solver.cpp:245]     Train net output #0: loss = 0.786386 (* 1 = 0.786386 loss)
I0113 14:48:26.087227 25204 sgd_solver.cpp:106] Iteration 67400, lr = 0.001
I0113 14:49:09.644237 25204 solver.cpp:229] Iteration 67600, loss = 0.516759
I0113 14:49:09.644287 25204 solver.cpp:245]     Train net output #0: loss = 0.516759 (* 1 = 0.516759 loss)
I0113 14:49:09.644294 25204 sgd_solver.cpp:106] Iteration 67600, lr = 0.001
I0113 14:49:53.171573 25204 solver.cpp:229] Iteration 67800, loss = 0.173567
I0113 14:49:53.171622 25204 solver.cpp:245]     Train net output #0: loss = 0.173568 (* 1 = 0.173568 loss)
I0113 14:49:53.171627 25204 sgd_solver.cpp:106] Iteration 67800, lr = 0.001
I0113 14:50:36.518479 25204 solver.cpp:338] Iteration 68000, Testing net (#0)
I0113 14:52:18.804745 25204 solver.cpp:406]     Test net output #0: accuracy = 0.678524
I0113 14:52:18.804811 25204 solver.cpp:406]     Test net output #1: loss = 0.865895 (* 1 = 0.865895 loss)
I0113 14:52:18.903820 25204 solver.cpp:229] Iteration 68000, loss = 1.05451
I0113 14:52:18.903863 25204 solver.cpp:245]     Train net output #0: loss = 1.05451 (* 1 = 1.05451 loss)
I0113 14:52:18.903869 25204 sgd_solver.cpp:106] Iteration 68000, lr = 0.001
I0113 14:52:56.497164 25204 solver.cpp:229] Iteration 68200, loss = 0.399663
I0113 14:52:56.497210 25204 solver.cpp:245]     Train net output #0: loss = 0.399664 (* 1 = 0.399664 loss)
I0113 14:52:56.497218 25204 sgd_solver.cpp:106] Iteration 68200, lr = 0.001
I0113 14:53:40.040035 25204 solver.cpp:229] Iteration 68400, loss = 0.263496
I0113 14:53:40.040082 25204 solver.cpp:245]     Train net output #0: loss = 0.263497 (* 1 = 0.263497 loss)
I0113 14:53:40.040088 25204 sgd_solver.cpp:106] Iteration 68400, lr = 0.001
I0113 14:54:23.611752 25204 solver.cpp:229] Iteration 68600, loss = 0.553382
I0113 14:54:23.611801 25204 solver.cpp:245]     Train net output #0: loss = 0.553382 (* 1 = 0.553382 loss)
I0113 14:54:23.611807 25204 sgd_solver.cpp:106] Iteration 68600, lr = 0.001
I0113 14:55:07.153054 25204 solver.cpp:229] Iteration 68800, loss = 1.1729
I0113 14:55:07.153105 25204 solver.cpp:245]     Train net output #0: loss = 1.1729 (* 1 = 1.1729 loss)
I0113 14:55:07.153110 25204 sgd_solver.cpp:106] Iteration 68800, lr = 0.001
I0113 14:55:50.475095 25204 solver.cpp:338] Iteration 69000, Testing net (#0)
I0113 14:57:32.783625 25204 solver.cpp:406]     Test net output #0: accuracy = 0.672461
I0113 14:57:32.783675 25204 solver.cpp:406]     Test net output #1: loss = 0.874549 (* 1 = 0.874549 loss)
I0113 14:57:32.882393 25204 solver.cpp:229] Iteration 69000, loss = 1.56448
I0113 14:57:32.882443 25204 solver.cpp:245]     Train net output #0: loss = 1.56448 (* 1 = 1.56448 loss)
I0113 14:57:32.882450 25204 sgd_solver.cpp:106] Iteration 69000, lr = 0.001
I0113 14:58:10.493314 25204 solver.cpp:229] Iteration 69200, loss = 1.35068
I0113 14:58:10.493403 25204 solver.cpp:245]     Train net output #0: loss = 1.35068 (* 1 = 1.35068 loss)
I0113 14:58:10.493410 25204 sgd_solver.cpp:106] Iteration 69200, lr = 0.001
I0113 14:58:54.009160 25204 solver.cpp:229] Iteration 69400, loss = 0.329281
I0113 14:58:54.009214 25204 solver.cpp:245]     Train net output #0: loss = 0.329281 (* 1 = 0.329281 loss)
I0113 14:58:54.009223 25204 sgd_solver.cpp:106] Iteration 69400, lr = 0.001
I0113 14:59:37.574141 25204 solver.cpp:229] Iteration 69600, loss = 0.973668
I0113 14:59:37.574189 25204 solver.cpp:245]     Train net output #0: loss = 0.973668 (* 1 = 0.973668 loss)
I0113 14:59:37.574195 25204 sgd_solver.cpp:106] Iteration 69600, lr = 0.001
I0113 15:00:21.158741 25204 solver.cpp:229] Iteration 69800, loss = 0.557374
I0113 15:00:21.158792 25204 solver.cpp:245]     Train net output #0: loss = 0.557374 (* 1 = 0.557374 loss)
I0113 15:00:21.158798 25204 sgd_solver.cpp:106] Iteration 69800, lr = 0.001
I0113 15:01:04.476784 25204 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_70000.caffemodel.h5
I0113 15:01:04.578886 25204 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_70000.solverstate.h5
I0113 15:01:04.649300 25204 solver.cpp:318] Iteration 70000, loss = 0.184582
I0113 15:01:04.649334 25204 solver.cpp:338] Iteration 70000, Testing net (#0)
I0113 15:02:46.850107 25204 solver.cpp:406]     Test net output #0: accuracy = 0.6743
I0113 15:02:46.850179 25204 solver.cpp:406]     Test net output #1: loss = 0.931913 (* 1 = 0.931913 loss)
I0113 15:02:46.850184 25204 solver.cpp:323] Optimization Done.
I0113 15:02:46.850188 25204 caffe.cpp:216] Optimization Done.
I0113 15:02:46.967834 29875 caffe.cpp:185] Using GPUs 0
I0113 15:02:47.395041 29875 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 0.0001
display: 200
max_iter: 85000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "krnet_full"
solver_mode: GPU
device_id: 0
net: "krnet_full_train_test.prototxt"
snapshot_format: HDF5
I0113 15:02:47.395166 29875 solver.cpp:91] Creating training net from net file: krnet_full_train_test.prototxt
I0113 15:02:47.395665 29875 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0113 15:02:47.395686 29875 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0113 15:02:47.395892 29875 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "train_lmdb"
    batch_size: 100
    backend: LMDB
    prefetch: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0113 15:02:47.396000 29875 layer_factory.hpp:77] Creating layer cifar
I0113 15:02:47.397019 29875 net.cpp:106] Creating Layer cifar
I0113 15:02:47.397033 29875 net.cpp:411] cifar -> data
I0113 15:02:47.397049 29875 net.cpp:411] cifar -> label
I0113 15:02:47.397059 29875 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I0113 15:02:47.397563 29879 db_lmdb.cpp:38] Opened lmdb train_lmdb
I0113 15:02:47.470487 29875 data_layer.cpp:41] output data size: 100,3,100,100
I0113 15:02:47.494539 29875 net.cpp:150] Setting up cifar
I0113 15:02:47.494572 29875 net.cpp:157] Top shape: 100 3 100 100 (3000000)
I0113 15:02:47.494590 29875 net.cpp:157] Top shape: 100 (100)
I0113 15:02:47.494592 29875 net.cpp:165] Memory required for data: 12000400
I0113 15:02:47.494603 29875 layer_factory.hpp:77] Creating layer conv1
I0113 15:02:47.494632 29875 net.cpp:106] Creating Layer conv1
I0113 15:02:47.494637 29875 net.cpp:454] conv1 <- data
I0113 15:02:47.494654 29875 net.cpp:411] conv1 -> conv1
I0113 15:02:47.497781 29875 net.cpp:150] Setting up conv1
I0113 15:02:47.497795 29875 net.cpp:157] Top shape: 100 32 100 100 (32000000)
I0113 15:02:47.497798 29875 net.cpp:165] Memory required for data: 140000400
I0113 15:02:47.497815 29875 layer_factory.hpp:77] Creating layer pool1
I0113 15:02:47.497828 29875 net.cpp:106] Creating Layer pool1
I0113 15:02:47.497831 29875 net.cpp:454] pool1 <- conv1
I0113 15:02:47.497838 29875 net.cpp:411] pool1 -> pool1
I0113 15:02:47.497875 29875 net.cpp:150] Setting up pool1
I0113 15:02:47.497882 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.497885 29875 net.cpp:165] Memory required for data: 172000400
I0113 15:02:47.497889 29875 layer_factory.hpp:77] Creating layer relu1
I0113 15:02:47.497895 29875 net.cpp:106] Creating Layer relu1
I0113 15:02:47.497898 29875 net.cpp:454] relu1 <- pool1
I0113 15:02:47.497905 29875 net.cpp:397] relu1 -> pool1 (in-place)
I0113 15:02:47.497911 29875 net.cpp:150] Setting up relu1
I0113 15:02:47.497915 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.497917 29875 net.cpp:165] Memory required for data: 204000400
I0113 15:02:47.497920 29875 layer_factory.hpp:77] Creating layer norm1
I0113 15:02:47.497928 29875 net.cpp:106] Creating Layer norm1
I0113 15:02:47.497931 29875 net.cpp:454] norm1 <- pool1
I0113 15:02:47.497937 29875 net.cpp:411] norm1 -> norm1
I0113 15:02:47.498025 29875 net.cpp:150] Setting up norm1
I0113 15:02:47.498034 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.498035 29875 net.cpp:165] Memory required for data: 236000400
I0113 15:02:47.498039 29875 layer_factory.hpp:77] Creating layer conv2
I0113 15:02:47.498049 29875 net.cpp:106] Creating Layer conv2
I0113 15:02:47.498052 29875 net.cpp:454] conv2 <- norm1
I0113 15:02:47.498060 29875 net.cpp:411] conv2 -> conv2
I0113 15:02:47.501202 29875 net.cpp:150] Setting up conv2
I0113 15:02:47.501212 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.501222 29875 net.cpp:165] Memory required for data: 268000400
I0113 15:02:47.501235 29875 layer_factory.hpp:77] Creating layer relu2
I0113 15:02:47.501241 29875 net.cpp:106] Creating Layer relu2
I0113 15:02:47.501245 29875 net.cpp:454] relu2 <- conv2
I0113 15:02:47.501250 29875 net.cpp:397] relu2 -> conv2 (in-place)
I0113 15:02:47.501257 29875 net.cpp:150] Setting up relu2
I0113 15:02:47.501260 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.501262 29875 net.cpp:165] Memory required for data: 300000400
I0113 15:02:47.501266 29875 layer_factory.hpp:77] Creating layer pool2
I0113 15:02:47.501271 29875 net.cpp:106] Creating Layer pool2
I0113 15:02:47.501274 29875 net.cpp:454] pool2 <- conv2
I0113 15:02:47.501279 29875 net.cpp:411] pool2 -> pool2
I0113 15:02:47.501299 29875 net.cpp:150] Setting up pool2
I0113 15:02:47.501305 29875 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 15:02:47.501307 29875 net.cpp:165] Memory required for data: 308000400
I0113 15:02:47.501310 29875 layer_factory.hpp:77] Creating layer norm2
I0113 15:02:47.501318 29875 net.cpp:106] Creating Layer norm2
I0113 15:02:47.501322 29875 net.cpp:454] norm2 <- pool2
I0113 15:02:47.501328 29875 net.cpp:411] norm2 -> norm2
I0113 15:02:47.501394 29875 net.cpp:150] Setting up norm2
I0113 15:02:47.501401 29875 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 15:02:47.501404 29875 net.cpp:165] Memory required for data: 316000400
I0113 15:02:47.501406 29875 layer_factory.hpp:77] Creating layer conv3
I0113 15:02:47.501417 29875 net.cpp:106] Creating Layer conv3
I0113 15:02:47.501420 29875 net.cpp:454] conv3 <- norm2
I0113 15:02:47.501427 29875 net.cpp:411] conv3 -> conv3
I0113 15:02:47.508260 29875 net.cpp:150] Setting up conv3
I0113 15:02:47.508301 29875 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 15:02:47.508306 29875 net.cpp:165] Memory required for data: 332000400
I0113 15:02:47.508324 29875 layer_factory.hpp:77] Creating layer relu3
I0113 15:02:47.508337 29875 net.cpp:106] Creating Layer relu3
I0113 15:02:47.508342 29875 net.cpp:454] relu3 <- conv3
I0113 15:02:47.508349 29875 net.cpp:397] relu3 -> conv3 (in-place)
I0113 15:02:47.508358 29875 net.cpp:150] Setting up relu3
I0113 15:02:47.508361 29875 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 15:02:47.508364 29875 net.cpp:165] Memory required for data: 348000400
I0113 15:02:47.508366 29875 layer_factory.hpp:77] Creating layer pool3
I0113 15:02:47.508374 29875 net.cpp:106] Creating Layer pool3
I0113 15:02:47.508378 29875 net.cpp:454] pool3 <- conv3
I0113 15:02:47.508383 29875 net.cpp:411] pool3 -> pool3
I0113 15:02:47.508409 29875 net.cpp:150] Setting up pool3
I0113 15:02:47.508415 29875 net.cpp:157] Top shape: 100 64 12 12 (921600)
I0113 15:02:47.508417 29875 net.cpp:165] Memory required for data: 351686800
I0113 15:02:47.508420 29875 layer_factory.hpp:77] Creating layer ip1
I0113 15:02:47.508433 29875 net.cpp:106] Creating Layer ip1
I0113 15:02:47.508436 29875 net.cpp:454] ip1 <- pool3
I0113 15:02:47.508443 29875 net.cpp:411] ip1 -> ip1
I0113 15:02:47.511785 29875 net.cpp:150] Setting up ip1
I0113 15:02:47.511798 29875 net.cpp:157] Top shape: 100 3 (300)
I0113 15:02:47.511801 29875 net.cpp:165] Memory required for data: 351688000
I0113 15:02:47.511808 29875 layer_factory.hpp:77] Creating layer loss
I0113 15:02:47.511817 29875 net.cpp:106] Creating Layer loss
I0113 15:02:47.511821 29875 net.cpp:454] loss <- ip1
I0113 15:02:47.511826 29875 net.cpp:454] loss <- label
I0113 15:02:47.511831 29875 net.cpp:411] loss -> loss
I0113 15:02:47.511842 29875 layer_factory.hpp:77] Creating layer loss
I0113 15:02:47.511911 29875 net.cpp:150] Setting up loss
I0113 15:02:47.511919 29875 net.cpp:157] Top shape: (1)
I0113 15:02:47.511922 29875 net.cpp:160]     with loss weight 1
I0113 15:02:47.511951 29875 net.cpp:165] Memory required for data: 351688004
I0113 15:02:47.511955 29875 net.cpp:226] loss needs backward computation.
I0113 15:02:47.511960 29875 net.cpp:226] ip1 needs backward computation.
I0113 15:02:47.511961 29875 net.cpp:226] pool3 needs backward computation.
I0113 15:02:47.511968 29875 net.cpp:226] relu3 needs backward computation.
I0113 15:02:47.511971 29875 net.cpp:226] conv3 needs backward computation.
I0113 15:02:47.511975 29875 net.cpp:226] norm2 needs backward computation.
I0113 15:02:47.511977 29875 net.cpp:226] pool2 needs backward computation.
I0113 15:02:47.511981 29875 net.cpp:226] relu2 needs backward computation.
I0113 15:02:47.511983 29875 net.cpp:226] conv2 needs backward computation.
I0113 15:02:47.511986 29875 net.cpp:226] norm1 needs backward computation.
I0113 15:02:47.511989 29875 net.cpp:226] relu1 needs backward computation.
I0113 15:02:47.511992 29875 net.cpp:226] pool1 needs backward computation.
I0113 15:02:47.511994 29875 net.cpp:226] conv1 needs backward computation.
I0113 15:02:47.511998 29875 net.cpp:228] cifar does not need backward computation.
I0113 15:02:47.512009 29875 net.cpp:270] This network produces output loss
I0113 15:02:47.512024 29875 net.cpp:283] Network initialization done.
I0113 15:02:47.512466 29875 solver.cpp:181] Creating test net (#0) specified by net file: krnet_full_train_test.prototxt
I0113 15:02:47.512508 29875 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0113 15:02:47.512744 29875 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "test_lmdb"
    batch_size: 100
    backend: LMDB
    prefetch: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0113 15:02:47.512868 29875 layer_factory.hpp:77] Creating layer cifar
I0113 15:02:47.513569 29875 net.cpp:106] Creating Layer cifar
I0113 15:02:47.513584 29875 net.cpp:411] cifar -> data
I0113 15:02:47.513600 29875 net.cpp:411] cifar -> label
I0113 15:02:47.513614 29875 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I0113 15:02:47.514354 29881 db_lmdb.cpp:38] Opened lmdb test_lmdb
I0113 15:02:47.515012 29875 data_layer.cpp:41] output data size: 100,3,100,100
I0113 15:02:47.533378 29875 net.cpp:150] Setting up cifar
I0113 15:02:47.533424 29875 net.cpp:157] Top shape: 100 3 100 100 (3000000)
I0113 15:02:47.533434 29875 net.cpp:157] Top shape: 100 (100)
I0113 15:02:47.533439 29875 net.cpp:165] Memory required for data: 12000400
I0113 15:02:47.533449 29875 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0113 15:02:47.533474 29875 net.cpp:106] Creating Layer label_cifar_1_split
I0113 15:02:47.533483 29875 net.cpp:454] label_cifar_1_split <- label
I0113 15:02:47.533499 29875 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0113 15:02:47.533519 29875 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0113 15:02:47.533581 29875 net.cpp:150] Setting up label_cifar_1_split
I0113 15:02:47.533591 29875 net.cpp:157] Top shape: 100 (100)
I0113 15:02:47.533596 29875 net.cpp:157] Top shape: 100 (100)
I0113 15:02:47.533601 29875 net.cpp:165] Memory required for data: 12001200
I0113 15:02:47.533605 29875 layer_factory.hpp:77] Creating layer conv1
I0113 15:02:47.533628 29875 net.cpp:106] Creating Layer conv1
I0113 15:02:47.533634 29875 net.cpp:454] conv1 <- data
I0113 15:02:47.533648 29875 net.cpp:411] conv1 -> conv1
I0113 15:02:47.536592 29875 net.cpp:150] Setting up conv1
I0113 15:02:47.536653 29875 net.cpp:157] Top shape: 100 32 100 100 (32000000)
I0113 15:02:47.536659 29875 net.cpp:165] Memory required for data: 140001200
I0113 15:02:47.536689 29875 layer_factory.hpp:77] Creating layer pool1
I0113 15:02:47.536713 29875 net.cpp:106] Creating Layer pool1
I0113 15:02:47.536721 29875 net.cpp:454] pool1 <- conv1
I0113 15:02:47.536736 29875 net.cpp:411] pool1 -> pool1
I0113 15:02:47.536787 29875 net.cpp:150] Setting up pool1
I0113 15:02:47.536798 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.536801 29875 net.cpp:165] Memory required for data: 172001200
I0113 15:02:47.536806 29875 layer_factory.hpp:77] Creating layer relu1
I0113 15:02:47.536819 29875 net.cpp:106] Creating Layer relu1
I0113 15:02:47.536825 29875 net.cpp:454] relu1 <- pool1
I0113 15:02:47.536835 29875 net.cpp:397] relu1 -> pool1 (in-place)
I0113 15:02:47.536846 29875 net.cpp:150] Setting up relu1
I0113 15:02:47.536854 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.536857 29875 net.cpp:165] Memory required for data: 204001200
I0113 15:02:47.536862 29875 layer_factory.hpp:77] Creating layer norm1
I0113 15:02:47.536875 29875 net.cpp:106] Creating Layer norm1
I0113 15:02:47.536880 29875 net.cpp:454] norm1 <- pool1
I0113 15:02:47.536890 29875 net.cpp:411] norm1 -> norm1
I0113 15:02:47.537004 29875 net.cpp:150] Setting up norm1
I0113 15:02:47.537024 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.537029 29875 net.cpp:165] Memory required for data: 236001200
I0113 15:02:47.537034 29875 layer_factory.hpp:77] Creating layer conv2
I0113 15:02:47.537055 29875 net.cpp:106] Creating Layer conv2
I0113 15:02:47.537061 29875 net.cpp:454] conv2 <- norm1
I0113 15:02:47.537075 29875 net.cpp:411] conv2 -> conv2
I0113 15:02:47.541651 29875 net.cpp:150] Setting up conv2
I0113 15:02:47.541697 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.541703 29875 net.cpp:165] Memory required for data: 268001200
I0113 15:02:47.541730 29875 layer_factory.hpp:77] Creating layer relu2
I0113 15:02:47.541750 29875 net.cpp:106] Creating Layer relu2
I0113 15:02:47.541761 29875 net.cpp:454] relu2 <- conv2
I0113 15:02:47.541774 29875 net.cpp:397] relu2 -> conv2 (in-place)
I0113 15:02:47.541790 29875 net.cpp:150] Setting up relu2
I0113 15:02:47.541805 29875 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:02:47.541810 29875 net.cpp:165] Memory required for data: 300001200
I0113 15:02:47.541815 29875 layer_factory.hpp:77] Creating layer pool2
I0113 15:02:47.541831 29875 net.cpp:106] Creating Layer pool2
I0113 15:02:47.541836 29875 net.cpp:454] pool2 <- conv2
I0113 15:02:47.541848 29875 net.cpp:411] pool2 -> pool2
I0113 15:02:47.541884 29875 net.cpp:150] Setting up pool2
I0113 15:02:47.541895 29875 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 15:02:47.541900 29875 net.cpp:165] Memory required for data: 308001200
I0113 15:02:47.541905 29875 layer_factory.hpp:77] Creating layer norm2
I0113 15:02:47.541918 29875 net.cpp:106] Creating Layer norm2
I0113 15:02:47.541924 29875 net.cpp:454] norm2 <- pool2
I0113 15:02:47.541935 29875 net.cpp:411] norm2 -> norm2
I0113 15:02:47.542052 29875 net.cpp:150] Setting up norm2
I0113 15:02:47.542064 29875 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 15:02:47.542068 29875 net.cpp:165] Memory required for data: 316001200
I0113 15:02:47.542074 29875 layer_factory.hpp:77] Creating layer conv3
I0113 15:02:47.542093 29875 net.cpp:106] Creating Layer conv3
I0113 15:02:47.542098 29875 net.cpp:454] conv3 <- norm2
I0113 15:02:47.542112 29875 net.cpp:411] conv3 -> conv3
I0113 15:02:47.550835 29875 net.cpp:150] Setting up conv3
I0113 15:02:47.550884 29875 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 15:02:47.550889 29875 net.cpp:165] Memory required for data: 332001200
I0113 15:02:47.550917 29875 layer_factory.hpp:77] Creating layer relu3
I0113 15:02:47.550937 29875 net.cpp:106] Creating Layer relu3
I0113 15:02:47.550947 29875 net.cpp:454] relu3 <- conv3
I0113 15:02:47.550961 29875 net.cpp:397] relu3 -> conv3 (in-place)
I0113 15:02:47.551002 29875 net.cpp:150] Setting up relu3
I0113 15:02:47.551009 29875 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 15:02:47.551014 29875 net.cpp:165] Memory required for data: 348001200
I0113 15:02:47.551019 29875 layer_factory.hpp:77] Creating layer pool3
I0113 15:02:47.551031 29875 net.cpp:106] Creating Layer pool3
I0113 15:02:47.551038 29875 net.cpp:454] pool3 <- conv3
I0113 15:02:47.551048 29875 net.cpp:411] pool3 -> pool3
I0113 15:02:47.551082 29875 net.cpp:150] Setting up pool3
I0113 15:02:47.551092 29875 net.cpp:157] Top shape: 100 64 12 12 (921600)
I0113 15:02:47.551096 29875 net.cpp:165] Memory required for data: 351687600
I0113 15:02:47.551101 29875 layer_factory.hpp:77] Creating layer ip1
I0113 15:02:47.551117 29875 net.cpp:106] Creating Layer ip1
I0113 15:02:47.551123 29875 net.cpp:454] ip1 <- pool3
I0113 15:02:47.551136 29875 net.cpp:411] ip1 -> ip1
I0113 15:02:47.555753 29875 net.cpp:150] Setting up ip1
I0113 15:02:47.555799 29875 net.cpp:157] Top shape: 100 3 (300)
I0113 15:02:47.555804 29875 net.cpp:165] Memory required for data: 351688800
I0113 15:02:47.555821 29875 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I0113 15:02:47.555840 29875 net.cpp:106] Creating Layer ip1_ip1_0_split
I0113 15:02:47.555850 29875 net.cpp:454] ip1_ip1_0_split <- ip1
I0113 15:02:47.555865 29875 net.cpp:411] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0113 15:02:47.555881 29875 net.cpp:411] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0113 15:02:47.555925 29875 net.cpp:150] Setting up ip1_ip1_0_split
I0113 15:02:47.555935 29875 net.cpp:157] Top shape: 100 3 (300)
I0113 15:02:47.555941 29875 net.cpp:157] Top shape: 100 3 (300)
I0113 15:02:47.555945 29875 net.cpp:165] Memory required for data: 351691200
I0113 15:02:47.555950 29875 layer_factory.hpp:77] Creating layer accuracy
I0113 15:02:47.555966 29875 net.cpp:106] Creating Layer accuracy
I0113 15:02:47.555972 29875 net.cpp:454] accuracy <- ip1_ip1_0_split_0
I0113 15:02:47.555981 29875 net.cpp:454] accuracy <- label_cifar_1_split_0
I0113 15:02:47.555992 29875 net.cpp:411] accuracy -> accuracy
I0113 15:02:47.556006 29875 net.cpp:150] Setting up accuracy
I0113 15:02:47.556015 29875 net.cpp:157] Top shape: (1)
I0113 15:02:47.556020 29875 net.cpp:165] Memory required for data: 351691204
I0113 15:02:47.556023 29875 layer_factory.hpp:77] Creating layer loss
I0113 15:02:47.556049 29875 net.cpp:106] Creating Layer loss
I0113 15:02:47.556056 29875 net.cpp:454] loss <- ip1_ip1_0_split_1
I0113 15:02:47.556064 29875 net.cpp:454] loss <- label_cifar_1_split_1
I0113 15:02:47.556076 29875 net.cpp:411] loss -> loss
I0113 15:02:47.556093 29875 layer_factory.hpp:77] Creating layer loss
I0113 15:02:47.556195 29875 net.cpp:150] Setting up loss
I0113 15:02:47.556205 29875 net.cpp:157] Top shape: (1)
I0113 15:02:47.556210 29875 net.cpp:160]     with loss weight 1
I0113 15:02:47.556231 29875 net.cpp:165] Memory required for data: 351691208
I0113 15:02:47.556236 29875 net.cpp:226] loss needs backward computation.
I0113 15:02:47.556243 29875 net.cpp:228] accuracy does not need backward computation.
I0113 15:02:47.556249 29875 net.cpp:226] ip1_ip1_0_split needs backward computation.
I0113 15:02:47.556254 29875 net.cpp:226] ip1 needs backward computation.
I0113 15:02:47.556259 29875 net.cpp:226] pool3 needs backward computation.
I0113 15:02:47.556265 29875 net.cpp:226] relu3 needs backward computation.
I0113 15:02:47.556270 29875 net.cpp:226] conv3 needs backward computation.
I0113 15:02:47.556275 29875 net.cpp:226] norm2 needs backward computation.
I0113 15:02:47.556280 29875 net.cpp:226] pool2 needs backward computation.
I0113 15:02:47.556285 29875 net.cpp:226] relu2 needs backward computation.
I0113 15:02:47.556290 29875 net.cpp:226] conv2 needs backward computation.
I0113 15:02:47.556295 29875 net.cpp:226] norm1 needs backward computation.
I0113 15:02:47.556299 29875 net.cpp:226] relu1 needs backward computation.
I0113 15:02:47.556304 29875 net.cpp:226] pool1 needs backward computation.
I0113 15:02:47.556309 29875 net.cpp:226] conv1 needs backward computation.
I0113 15:02:47.556330 29875 net.cpp:228] label_cifar_1_split does not need backward computation.
I0113 15:02:47.556337 29875 net.cpp:228] cifar does not need backward computation.
I0113 15:02:47.556341 29875 net.cpp:270] This network produces output accuracy
I0113 15:02:47.556347 29875 net.cpp:270] This network produces output loss
I0113 15:02:47.556370 29875 net.cpp:283] Network initialization done.
I0113 15:02:47.556459 29875 solver.cpp:60] Solver scaffolding done.
I0113 15:02:47.556746 29875 caffe.cpp:203] Resuming from krnet_full_iter_70000.solverstate.h5
I0113 15:02:47.558336 29875 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0113 15:02:47.559923 29875 caffe.cpp:213] Starting Optimization
I0113 15:02:47.559947 29875 solver.cpp:280] Solving CIFAR10_full
I0113 15:02:47.559953 29875 solver.cpp:281] Learning Rate Policy: fixed
I0113 15:02:47.560793 29875 solver.cpp:338] Iteration 70000, Testing net (#0)
I0113 15:02:47.562261 29875 blocking_queue.cpp:50] Data layer prefetch queue empty
I0113 15:02:54.330338 29875 solver.cpp:406]     Test net output #0: accuracy = 0.6526
I0113 15:02:54.330376 29875 solver.cpp:406]     Test net output #1: loss = 0.982309 (* 1 = 0.982309 loss)
I0113 15:02:54.449746 29875 solver.cpp:229] Iteration 70000, loss = 0.18464
I0113 15:02:54.449784 29875 solver.cpp:245]     Train net output #0: loss = 0.18464 (* 1 = 0.18464 loss)
I0113 15:02:54.449792 29875 sgd_solver.cpp:106] Iteration 70000, lr = 0.0001
I0113 15:03:33.723467 29875 solver.cpp:229] Iteration 70200, loss = 0.658696
I0113 15:03:33.723538 29875 solver.cpp:245]     Train net output #0: loss = 0.658696 (* 1 = 0.658696 loss)
I0113 15:03:33.723544 29875 sgd_solver.cpp:106] Iteration 70200, lr = 0.0001
I0113 15:04:17.269799 29875 solver.cpp:229] Iteration 70400, loss = 0.506659
I0113 15:04:17.269877 29875 solver.cpp:245]     Train net output #0: loss = 0.506659 (* 1 = 0.506659 loss)
I0113 15:04:17.269883 29875 sgd_solver.cpp:106] Iteration 70400, lr = 0.0001
I0113 15:05:00.833741 29875 solver.cpp:229] Iteration 70600, loss = 0.783292
I0113 15:05:00.833817 29875 solver.cpp:245]     Train net output #0: loss = 0.783292 (* 1 = 0.783292 loss)
I0113 15:05:00.833822 29875 sgd_solver.cpp:106] Iteration 70600, lr = 0.0001
I0113 15:05:44.400789 29875 solver.cpp:229] Iteration 70800, loss = 1.19549
I0113 15:05:44.400842 29875 solver.cpp:245]     Train net output #0: loss = 1.19549 (* 1 = 1.19549 loss)
I0113 15:05:44.400857 29875 sgd_solver.cpp:106] Iteration 70800, lr = 0.0001
I0113 15:06:27.758028 29875 solver.cpp:338] Iteration 71000, Testing net (#0)
I0113 15:06:35.896956 29875 solver.cpp:406]     Test net output #0: accuracy = 0.7144
I0113 15:06:35.896997 29875 solver.cpp:406]     Test net output #1: loss = 0.802958 (* 1 = 0.802958 loss)
I0113 15:06:36.015985 29875 solver.cpp:229] Iteration 71000, loss = 0.42101
I0113 15:06:36.016026 29875 solver.cpp:245]     Train net output #0: loss = 0.421011 (* 1 = 0.421011 loss)
I0113 15:06:36.016031 29875 sgd_solver.cpp:106] Iteration 71000, lr = 0.0001
I0113 15:07:16.434829 29875 solver.cpp:229] Iteration 71200, loss = 0.99971
I0113 15:07:16.434903 29875 solver.cpp:245]     Train net output #0: loss = 0.99971 (* 1 = 0.99971 loss)
I0113 15:07:16.434909 29875 sgd_solver.cpp:106] Iteration 71200, lr = 0.0001
I0113 15:07:53.843453 29875 solver.cpp:229] Iteration 71400, loss = 1.39487
I0113 15:07:53.843513 29875 solver.cpp:245]     Train net output #0: loss = 1.39487 (* 1 = 1.39487 loss)
I0113 15:07:53.843519 29875 sgd_solver.cpp:106] Iteration 71400, lr = 0.0001
I0113 15:08:31.269170 29875 solver.cpp:229] Iteration 71600, loss = 0.427936
I0113 15:08:31.269228 29875 solver.cpp:245]     Train net output #0: loss = 0.427935 (* 1 = 0.427935 loss)
I0113 15:08:31.269235 29875 sgd_solver.cpp:106] Iteration 71600, lr = 0.0001
I0113 15:09:11.346091 29875 solver.cpp:229] Iteration 71800, loss = 0.780628
I0113 15:09:11.346164 29875 solver.cpp:245]     Train net output #0: loss = 0.780629 (* 1 = 0.780629 loss)
I0113 15:09:11.346169 29875 sgd_solver.cpp:106] Iteration 71800, lr = 0.0001
I0113 15:09:54.711356 29875 solver.cpp:338] Iteration 72000, Testing net (#0)
I0113 15:10:02.856931 29875 solver.cpp:406]     Test net output #0: accuracy = 0.7077
I0113 15:10:02.856974 29875 solver.cpp:406]     Test net output #1: loss = 0.814854 (* 1 = 0.814854 loss)
I0113 15:10:02.982745 29875 solver.cpp:229] Iteration 72000, loss = 1.37629
I0113 15:10:02.982784 29875 solver.cpp:245]     Train net output #0: loss = 1.37629 (* 1 = 1.37629 loss)
I0113 15:10:02.982789 29875 sgd_solver.cpp:106] Iteration 72000, lr = 0.0001
I0113 15:10:46.559872 29875 solver.cpp:229] Iteration 72200, loss = 0.694748
I0113 15:10:46.559926 29875 solver.cpp:245]     Train net output #0: loss = 0.694748 (* 1 = 0.694748 loss)
I0113 15:10:46.559931 29875 sgd_solver.cpp:106] Iteration 72200, lr = 0.0001
I0113 15:11:30.144935 29875 solver.cpp:229] Iteration 72400, loss = 0.948109
I0113 15:11:30.144986 29875 solver.cpp:245]     Train net output #0: loss = 0.94811 (* 1 = 0.94811 loss)
I0113 15:11:30.144992 29875 sgd_solver.cpp:106] Iteration 72400, lr = 0.0001
I0113 15:12:13.730799 29875 solver.cpp:229] Iteration 72600, loss = 1.61685
I0113 15:12:13.730849 29875 solver.cpp:245]     Train net output #0: loss = 1.61685 (* 1 = 1.61685 loss)
I0113 15:12:13.730855 29875 sgd_solver.cpp:106] Iteration 72600, lr = 0.0001
I0113 15:12:53.312048 29875 solver.cpp:229] Iteration 72800, loss = 0.346473
I0113 15:12:53.312103 29875 solver.cpp:245]     Train net output #0: loss = 0.346473 (* 1 = 0.346473 loss)
I0113 15:12:53.312109 29875 sgd_solver.cpp:106] Iteration 72800, lr = 0.0001
I0113 15:13:30.548334 29875 solver.cpp:338] Iteration 73000, Testing net (#0)
I0113 15:13:37.352088 29875 solver.cpp:406]     Test net output #0: accuracy = 0.5951
I0113 15:13:37.352130 29875 solver.cpp:406]     Test net output #1: loss = 0.9703 (* 1 = 0.9703 loss)
I0113 15:13:37.450711 29875 solver.cpp:229] Iteration 73000, loss = 1.50952
I0113 15:13:37.450755 29875 solver.cpp:245]     Train net output #0: loss = 1.50952 (* 1 = 1.50952 loss)
I0113 15:13:37.450762 29875 sgd_solver.cpp:106] Iteration 73000, lr = 0.0001
I0113 15:14:14.878265 29875 solver.cpp:229] Iteration 73200, loss = 1.29547
I0113 15:14:14.878320 29875 solver.cpp:245]     Train net output #0: loss = 1.29547 (* 1 = 1.29547 loss)
I0113 15:14:14.878326 29875 sgd_solver.cpp:106] Iteration 73200, lr = 0.0001
I0113 15:14:57.062593 29875 solver.cpp:229] Iteration 73400, loss = 1.71107
I0113 15:14:57.062659 29875 solver.cpp:245]     Train net output #0: loss = 1.71107 (* 1 = 1.71107 loss)
I0113 15:14:57.062665 29875 sgd_solver.cpp:106] Iteration 73400, lr = 0.0001
I0113 15:15:40.601480 29875 solver.cpp:229] Iteration 73600, loss = 1.21963
I0113 15:15:40.601531 29875 solver.cpp:245]     Train net output #0: loss = 1.21963 (* 1 = 1.21963 loss)
I0113 15:15:40.601536 29875 sgd_solver.cpp:106] Iteration 73600, lr = 0.0001
I0113 15:16:24.141609 29875 solver.cpp:229] Iteration 73800, loss = 0.533292
I0113 15:16:24.141700 29875 solver.cpp:245]     Train net output #0: loss = 0.533292 (* 1 = 0.533292 loss)
I0113 15:16:24.141706 29875 sgd_solver.cpp:106] Iteration 73800, lr = 0.0001
I0113 15:17:07.472249 29875 solver.cpp:338] Iteration 74000, Testing net (#0)
I0113 15:17:15.612758 29875 solver.cpp:406]     Test net output #0: accuracy = 0.6975
I0113 15:17:15.612798 29875 solver.cpp:406]     Test net output #1: loss = 0.823724 (* 1 = 0.823724 loss)
I0113 15:17:15.743039 29875 solver.cpp:229] Iteration 74000, loss = 1.66179
I0113 15:17:15.743115 29875 solver.cpp:245]     Train net output #0: loss = 1.66179 (* 1 = 1.66179 loss)
I0113 15:17:15.743122 29875 sgd_solver.cpp:106] Iteration 74000, lr = 0.0001
I0113 15:17:59.292389 29875 solver.cpp:229] Iteration 74200, loss = 1.29937
I0113 15:17:59.292486 29875 solver.cpp:245]     Train net output #0: loss = 1.29937 (* 1 = 1.29937 loss)
I0113 15:17:59.292495 29875 sgd_solver.cpp:106] Iteration 74200, lr = 0.0001
I0113 15:18:36.761409 29875 solver.cpp:229] Iteration 74400, loss = 0.414528
I0113 15:18:36.761514 29875 solver.cpp:245]     Train net output #0: loss = 0.414529 (* 1 = 0.414529 loss)
I0113 15:18:36.761521 29875 sgd_solver.cpp:106] Iteration 74400, lr = 0.0001
I0113 15:19:14.177639 29875 solver.cpp:229] Iteration 74600, loss = 0.731245
I0113 15:19:14.177708 29875 solver.cpp:245]     Train net output #0: loss = 0.731245 (* 1 = 0.731245 loss)
I0113 15:19:14.177714 29875 sgd_solver.cpp:106] Iteration 74600, lr = 0.0001
I0113 15:19:51.584305 29875 solver.cpp:229] Iteration 74800, loss = 0.428422
I0113 15:19:51.584358 29875 solver.cpp:245]     Train net output #0: loss = 0.428422 (* 1 = 0.428422 loss)
I0113 15:19:51.584364 29875 sgd_solver.cpp:106] Iteration 74800, lr = 0.0001
I0113 15:20:34.413163 29875 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_75000.caffemodel.h5
I0113 15:20:34.515413 29875 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_75000.solverstate.h5
I0113 15:20:34.516072 29875 solver.cpp:338] Iteration 75000, Testing net (#0)
I0113 15:20:42.576758 29875 solver.cpp:406]     Test net output #0: accuracy = 0.7298
I0113 15:20:42.576802 29875 solver.cpp:406]     Test net output #1: loss = 0.772927 (* 1 = 0.772927 loss)
I0113 15:20:42.702128 29875 solver.cpp:229] Iteration 75000, loss = 0.355707
I0113 15:20:42.702164 29875 solver.cpp:245]     Train net output #0: loss = 0.355707 (* 1 = 0.355707 loss)
I0113 15:20:42.702169 29875 sgd_solver.cpp:106] Iteration 75000, lr = 0.0001
I0113 15:21:26.264672 29875 solver.cpp:229] Iteration 75200, loss = 1.64696
I0113 15:21:26.264721 29875 solver.cpp:245]     Train net output #0: loss = 1.64696 (* 1 = 1.64696 loss)
I0113 15:21:26.264727 29875 sgd_solver.cpp:106] Iteration 75200, lr = 0.0001
I0113 15:22:09.822453 29875 solver.cpp:229] Iteration 75400, loss = 1.3425
I0113 15:22:09.822502 29875 solver.cpp:245]     Train net output #0: loss = 1.3425 (* 1 = 1.3425 loss)
I0113 15:22:09.822507 29875 sgd_solver.cpp:106] Iteration 75400, lr = 0.0001
I0113 15:22:53.393703 29875 solver.cpp:229] Iteration 75600, loss = 1.32863
I0113 15:22:53.393754 29875 solver.cpp:245]     Train net output #0: loss = 1.32863 (* 1 = 1.32863 loss)
I0113 15:22:53.393759 29875 sgd_solver.cpp:106] Iteration 75600, lr = 0.0001
I0113 15:23:36.178341 29875 solver.cpp:229] Iteration 75800, loss = 1.56326
I0113 15:23:36.178393 29875 solver.cpp:245]     Train net output #0: loss = 1.56326 (* 1 = 1.56326 loss)
I0113 15:23:36.178400 29875 sgd_solver.cpp:106] Iteration 75800, lr = 0.0001
I0113 15:24:13.411378 29875 solver.cpp:338] Iteration 76000, Testing net (#0)
I0113 15:24:20.223914 29875 solver.cpp:406]     Test net output #0: accuracy = 0.6482
I0113 15:24:20.223959 29875 solver.cpp:406]     Test net output #1: loss = 0.891715 (* 1 = 0.891715 loss)
I0113 15:24:20.337870 29875 solver.cpp:229] Iteration 76000, loss = 1.35855
I0113 15:24:20.337910 29875 solver.cpp:245]     Train net output #0: loss = 1.35855 (* 1 = 1.35855 loss)
I0113 15:24:20.337916 29875 sgd_solver.cpp:106] Iteration 76000, lr = 0.0001
I0113 15:24:57.745762 29875 solver.cpp:229] Iteration 76200, loss = 1.1742
I0113 15:24:57.745822 29875 solver.cpp:245]     Train net output #0: loss = 1.1742 (* 1 = 1.1742 loss)
I0113 15:24:57.745828 29875 sgd_solver.cpp:106] Iteration 76200, lr = 0.0001
I0113 15:25:36.753711 29875 solver.cpp:229] Iteration 76400, loss = 0.892364
I0113 15:25:36.753762 29875 solver.cpp:245]     Train net output #0: loss = 0.892364 (* 1 = 0.892364 loss)
I0113 15:25:36.753767 29875 sgd_solver.cpp:106] Iteration 76400, lr = 0.0001
I0113 15:26:20.304474 29875 solver.cpp:229] Iteration 76600, loss = 0.391422
I0113 15:26:20.304528 29875 solver.cpp:245]     Train net output #0: loss = 0.391422 (* 1 = 0.391422 loss)
I0113 15:26:20.304534 29875 sgd_solver.cpp:106] Iteration 76600, lr = 0.0001
I0113 15:27:03.870247 29875 solver.cpp:229] Iteration 76800, loss = 1.56861
I0113 15:27:03.870307 29875 solver.cpp:245]     Train net output #0: loss = 1.56861 (* 1 = 1.56861 loss)
I0113 15:27:03.870313 29875 sgd_solver.cpp:106] Iteration 76800, lr = 0.0001
I0113 15:27:47.208948 29875 solver.cpp:338] Iteration 77000, Testing net (#0)
I0113 15:27:55.347499 29875 solver.cpp:406]     Test net output #0: accuracy = 0.6384
I0113 15:27:55.347543 29875 solver.cpp:406]     Test net output #1: loss = 0.904009 (* 1 = 0.904009 loss)
I0113 15:27:55.475252 29875 solver.cpp:229] Iteration 77000, loss = 0.687131
I0113 15:27:55.475296 29875 solver.cpp:245]     Train net output #0: loss = 0.687131 (* 1 = 0.687131 loss)
I0113 15:27:55.475301 29875 sgd_solver.cpp:106] Iteration 77000, lr = 0.0001
I0113 15:28:39.074501 29875 solver.cpp:229] Iteration 77200, loss = 0.705803
I0113 15:28:39.074560 29875 solver.cpp:245]     Train net output #0: loss = 0.705803 (* 1 = 0.705803 loss)
I0113 15:28:39.074566 29875 sgd_solver.cpp:106] Iteration 77200, lr = 0.0001
I0113 15:29:19.749598 29875 solver.cpp:229] Iteration 77400, loss = 1.39172
I0113 15:29:19.749671 29875 solver.cpp:245]     Train net output #0: loss = 1.39172 (* 1 = 1.39172 loss)
I0113 15:29:19.749677 29875 sgd_solver.cpp:106] Iteration 77400, lr = 0.0001
I0113 15:29:57.182828 29875 solver.cpp:229] Iteration 77600, loss = 1.57113
I0113 15:29:57.182881 29875 solver.cpp:245]     Train net output #0: loss = 1.57113 (* 1 = 1.57113 loss)
I0113 15:29:57.182888 29875 sgd_solver.cpp:106] Iteration 77600, lr = 0.0001
I0113 15:30:34.621733 29875 solver.cpp:229] Iteration 77800, loss = 1.34415
I0113 15:30:34.621832 29875 solver.cpp:245]     Train net output #0: loss = 1.34415 (* 1 = 1.34415 loss)
I0113 15:30:34.621842 29875 sgd_solver.cpp:106] Iteration 77800, lr = 0.0001
I0113 15:31:14.233021 29875 solver.cpp:338] Iteration 78000, Testing net (#0)
I0113 15:31:22.370756 29875 solver.cpp:406]     Test net output #0: accuracy = 0.7131
I0113 15:31:22.370798 29875 solver.cpp:406]     Test net output #1: loss = 0.800602 (* 1 = 0.800602 loss)
I0113 15:31:22.500635 29875 solver.cpp:229] Iteration 78000, loss = 0.942339
I0113 15:31:22.500677 29875 solver.cpp:245]     Train net output #0: loss = 0.94234 (* 1 = 0.94234 loss)
I0113 15:31:22.500684 29875 sgd_solver.cpp:106] Iteration 78000, lr = 0.0001
I0113 15:32:06.058516 29875 solver.cpp:229] Iteration 78200, loss = 0.809316
I0113 15:32:06.058588 29875 solver.cpp:245]     Train net output #0: loss = 0.809317 (* 1 = 0.809317 loss)
I0113 15:32:06.058593 29875 sgd_solver.cpp:106] Iteration 78200, lr = 0.0001
I0113 15:32:49.607872 29875 solver.cpp:229] Iteration 78400, loss = 1.456
I0113 15:32:49.607928 29875 solver.cpp:245]     Train net output #0: loss = 1.456 (* 1 = 1.456 loss)
I0113 15:32:49.607939 29875 sgd_solver.cpp:106] Iteration 78400, lr = 0.0001
I0113 15:33:33.164755 29875 solver.cpp:229] Iteration 78600, loss = 1.05328
I0113 15:33:33.164809 29875 solver.cpp:245]     Train net output #0: loss = 1.05328 (* 1 = 1.05328 loss)
I0113 15:33:33.164815 29875 sgd_solver.cpp:106] Iteration 78600, lr = 0.0001
I0113 15:34:16.741307 29875 solver.cpp:229] Iteration 78800, loss = 0.396724
I0113 15:34:16.741369 29875 solver.cpp:245]     Train net output #0: loss = 0.396725 (* 1 = 0.396725 loss)
I0113 15:34:16.741374 29875 sgd_solver.cpp:106] Iteration 78800, lr = 0.0001
I0113 15:34:56.415446 29875 solver.cpp:338] Iteration 79000, Testing net (#0)
I0113 15:35:03.214329 29875 solver.cpp:406]     Test net output #0: accuracy = 0.7112
I0113 15:35:03.214371 29875 solver.cpp:406]     Test net output #1: loss = 0.802043 (* 1 = 0.802043 loss)
I0113 15:35:03.321132 29875 solver.cpp:229] Iteration 79000, loss = 0.500926
I0113 15:35:03.321171 29875 solver.cpp:245]     Train net output #0: loss = 0.500927 (* 1 = 0.500927 loss)
I0113 15:35:03.321177 29875 sgd_solver.cpp:106] Iteration 79000, lr = 0.0001
I0113 15:35:40.740779 29875 solver.cpp:229] Iteration 79200, loss = 0.779
I0113 15:35:40.740834 29875 solver.cpp:245]     Train net output #0: loss = 0.779 (* 1 = 0.779 loss)
I0113 15:35:40.740840 29875 sgd_solver.cpp:106] Iteration 79200, lr = 0.0001
I0113 15:36:18.160926 29875 solver.cpp:229] Iteration 79400, loss = 0.398977
I0113 15:36:18.160979 29875 solver.cpp:245]     Train net output #0: loss = 0.398978 (* 1 = 0.398978 loss)
I0113 15:36:18.160985 29875 sgd_solver.cpp:106] Iteration 79400, lr = 0.0001
I0113 15:37:00.074811 29875 solver.cpp:229] Iteration 79600, loss = 0.425398
I0113 15:37:00.074903 29875 solver.cpp:245]     Train net output #0: loss = 0.425399 (* 1 = 0.425399 loss)
I0113 15:37:00.074909 29875 sgd_solver.cpp:106] Iteration 79600, lr = 0.0001
I0113 15:37:43.621747 29875 solver.cpp:229] Iteration 79800, loss = 0.508853
I0113 15:37:43.621806 29875 solver.cpp:245]     Train net output #0: loss = 0.508853 (* 1 = 0.508853 loss)
I0113 15:37:43.621812 29875 sgd_solver.cpp:106] Iteration 79800, lr = 0.0001
I0113 15:38:26.966281 29875 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_80000.caffemodel.h5
I0113 15:38:27.056653 29875 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_80000.solverstate.h5
I0113 15:38:27.057265 29875 solver.cpp:338] Iteration 80000, Testing net (#0)
I0113 15:38:35.113950 29875 solver.cpp:406]     Test net output #0: accuracy = 0.599
I0113 15:38:35.113992 29875 solver.cpp:406]     Test net output #1: loss = 0.976194 (* 1 = 0.976194 loss)
I0113 15:38:35.232866 29875 solver.cpp:229] Iteration 80000, loss = 0.492402
I0113 15:38:35.232903 29875 solver.cpp:245]     Train net output #0: loss = 0.492403 (* 1 = 0.492403 loss)
I0113 15:38:35.232909 29875 sgd_solver.cpp:106] Iteration 80000, lr = 0.0001
I0113 15:39:18.826264 29875 solver.cpp:229] Iteration 80200, loss = 1.49279
I0113 15:39:18.826328 29875 solver.cpp:245]     Train net output #0: loss = 1.49279 (* 1 = 1.49279 loss)
I0113 15:39:18.826334 29875 sgd_solver.cpp:106] Iteration 80200, lr = 0.0001
I0113 15:40:02.428130 29875 solver.cpp:229] Iteration 80400, loss = 0.424187
I0113 15:40:02.428181 29875 solver.cpp:245]     Train net output #0: loss = 0.424188 (* 1 = 0.424188 loss)
I0113 15:40:02.428186 29875 sgd_solver.cpp:106] Iteration 80400, lr = 0.0001
I0113 15:40:40.141789 29875 solver.cpp:229] Iteration 80600, loss = 0.343289
I0113 15:40:40.141865 29875 solver.cpp:245]     Train net output #0: loss = 0.343289 (* 1 = 0.343289 loss)
I0113 15:40:40.141870 29875 sgd_solver.cpp:106] Iteration 80600, lr = 0.0001
I0113 15:41:17.570674 29875 solver.cpp:229] Iteration 80800, loss = 1.4918
I0113 15:41:17.570737 29875 solver.cpp:245]     Train net output #0: loss = 1.4918 (* 1 = 1.4918 loss)
I0113 15:41:17.570744 29875 sgd_solver.cpp:106] Iteration 80800, lr = 0.0001
I0113 15:41:54.793264 29875 solver.cpp:338] Iteration 81000, Testing net (#0)
I0113 15:42:02.098804 29875 solver.cpp:406]     Test net output #0: accuracy = 0.6868
I0113 15:42:02.098851 29875 solver.cpp:406]     Test net output #1: loss = 0.83825 (* 1 = 0.83825 loss)
I0113 15:42:02.224755 29875 solver.cpp:229] Iteration 81000, loss = 0.400901
I0113 15:42:02.224791 29875 solver.cpp:245]     Train net output #0: loss = 0.400901 (* 1 = 0.400901 loss)
I0113 15:42:02.224795 29875 sgd_solver.cpp:106] Iteration 81000, lr = 0.0001
I0113 15:42:45.783020 29875 solver.cpp:229] Iteration 81200, loss = 0.912577
I0113 15:42:45.783069 29875 solver.cpp:245]     Train net output #0: loss = 0.912578 (* 1 = 0.912578 loss)
I0113 15:42:45.783074 29875 sgd_solver.cpp:106] Iteration 81200, lr = 0.0001
I0113 15:43:29.326633 29875 solver.cpp:229] Iteration 81400, loss = 1.27527
I0113 15:43:29.326688 29875 solver.cpp:245]     Train net output #0: loss = 1.27527 (* 1 = 1.27527 loss)
I0113 15:43:29.326694 29875 sgd_solver.cpp:106] Iteration 81400, lr = 0.0001
I0113 15:44:12.897138 29875 solver.cpp:229] Iteration 81600, loss = 0.420498
I0113 15:44:12.897194 29875 solver.cpp:245]     Train net output #0: loss = 0.420498 (* 1 = 0.420498 loss)
I0113 15:44:12.897200 29875 sgd_solver.cpp:106] Iteration 81600, lr = 0.0001
I0113 15:44:56.451856 29875 solver.cpp:229] Iteration 81800, loss = 0.960098
I0113 15:44:56.451954 29875 solver.cpp:245]     Train net output #0: loss = 0.960098 (* 1 = 0.960098 loss)
I0113 15:44:56.451963 29875 sgd_solver.cpp:106] Iteration 81800, lr = 0.0001
I0113 15:45:39.298250 29875 solver.cpp:338] Iteration 82000, Testing net (#0)
I0113 15:45:46.112006 29875 solver.cpp:406]     Test net output #0: accuracy = 0.7296
I0113 15:45:46.112053 29875 solver.cpp:406]     Test net output #1: loss = 0.782338 (* 1 = 0.782338 loss)
I0113 15:45:46.226222 29875 solver.cpp:229] Iteration 82000, loss = 0.712197
I0113 15:45:46.226263 29875 solver.cpp:245]     Train net output #0: loss = 0.712198 (* 1 = 0.712198 loss)
I0113 15:45:46.226267 29875 sgd_solver.cpp:106] Iteration 82000, lr = 0.0001
I0113 15:46:23.647274 29875 solver.cpp:229] Iteration 82200, loss = 1.30919
I0113 15:46:23.647370 29875 solver.cpp:245]     Train net output #0: loss = 1.30919 (* 1 = 1.30919 loss)
I0113 15:46:23.647377 29875 sgd_solver.cpp:106] Iteration 82200, lr = 0.0001
I0113 15:47:01.072577 29875 solver.cpp:229] Iteration 82400, loss = 1.15222
I0113 15:47:01.072641 29875 solver.cpp:245]     Train net output #0: loss = 1.15222 (* 1 = 1.15222 loss)
I0113 15:47:01.072648 29875 sgd_solver.cpp:106] Iteration 82400, lr = 0.0001
I0113 15:47:39.820034 29875 solver.cpp:229] Iteration 82600, loss = 0.83429
I0113 15:47:39.820085 29875 solver.cpp:245]     Train net output #0: loss = 0.834291 (* 1 = 0.834291 loss)
I0113 15:47:39.820091 29875 sgd_solver.cpp:106] Iteration 82600, lr = 0.0001
I0113 15:48:23.381541 29875 solver.cpp:229] Iteration 82800, loss = 0.349186
I0113 15:48:23.381603 29875 solver.cpp:245]     Train net output #0: loss = 0.349187 (* 1 = 0.349187 loss)
I0113 15:48:23.381608 29875 sgd_solver.cpp:106] Iteration 82800, lr = 0.0001
I0113 15:49:06.707764 29875 solver.cpp:338] Iteration 83000, Testing net (#0)
I0113 15:49:14.847257 29875 solver.cpp:406]     Test net output #0: accuracy = 0.6626
I0113 15:49:14.847297 29875 solver.cpp:406]     Test net output #1: loss = 0.872828 (* 1 = 0.872828 loss)
I0113 15:49:14.978544 29875 solver.cpp:229] Iteration 83000, loss = 0.630232
I0113 15:49:14.978590 29875 solver.cpp:245]     Train net output #0: loss = 0.630232 (* 1 = 0.630232 loss)
I0113 15:49:14.978596 29875 sgd_solver.cpp:106] Iteration 83000, lr = 0.0001
I0113 15:49:58.543789 29875 solver.cpp:229] Iteration 83200, loss = 0.404033
I0113 15:49:58.543848 29875 solver.cpp:245]     Train net output #0: loss = 0.404033 (* 1 = 0.404033 loss)
I0113 15:49:58.543853 29875 sgd_solver.cpp:106] Iteration 83200, lr = 0.0001
I0113 15:50:42.091114 29875 solver.cpp:229] Iteration 83400, loss = 0.820894
I0113 15:50:42.091171 29875 solver.cpp:245]     Train net output #0: loss = 0.820895 (* 1 = 0.820895 loss)
I0113 15:50:42.091177 29875 sgd_solver.cpp:106] Iteration 83400, lr = 0.0001
I0113 15:51:23.027710 29875 solver.cpp:229] Iteration 83600, loss = 1.45474
I0113 15:51:23.027781 29875 solver.cpp:245]     Train net output #0: loss = 1.45474 (* 1 = 1.45474 loss)
I0113 15:51:23.027787 29875 sgd_solver.cpp:106] Iteration 83600, lr = 0.0001
I0113 15:52:00.466158 29875 solver.cpp:229] Iteration 83800, loss = 0.378829
I0113 15:52:00.466219 29875 solver.cpp:245]     Train net output #0: loss = 0.37883 (* 1 = 0.37883 loss)
I0113 15:52:00.466228 29875 sgd_solver.cpp:106] Iteration 83800, lr = 0.0001
I0113 15:52:37.713147 29875 solver.cpp:338] Iteration 84000, Testing net (#0)
I0113 15:52:44.520901 29875 solver.cpp:406]     Test net output #0: accuracy = 0.6329
I0113 15:52:44.520942 29875 solver.cpp:406]     Test net output #1: loss = 0.916773 (* 1 = 0.916773 loss)
I0113 15:52:44.619909 29875 solver.cpp:229] Iteration 84000, loss = 1.79909
I0113 15:52:44.619953 29875 solver.cpp:245]     Train net output #0: loss = 1.79909 (* 1 = 1.79909 loss)
I0113 15:52:44.619959 29875 sgd_solver.cpp:106] Iteration 84000, lr = 0.0001
I0113 15:53:25.488595 29875 solver.cpp:229] Iteration 84200, loss = 0.415808
I0113 15:53:25.488651 29875 solver.cpp:245]     Train net output #0: loss = 0.415809 (* 1 = 0.415809 loss)
I0113 15:53:25.488658 29875 sgd_solver.cpp:106] Iteration 84200, lr = 0.0001
I0113 15:54:09.051235 29875 solver.cpp:229] Iteration 84400, loss = 0.358822
I0113 15:54:09.051313 29875 solver.cpp:245]     Train net output #0: loss = 0.358823 (* 1 = 0.358823 loss)
I0113 15:54:09.051321 29875 sgd_solver.cpp:106] Iteration 84400, lr = 0.0001
I0113 15:54:52.597652 29875 solver.cpp:229] Iteration 84600, loss = 0.389275
I0113 15:54:52.597749 29875 solver.cpp:245]     Train net output #0: loss = 0.389276 (* 1 = 0.389276 loss)
I0113 15:54:52.597757 29875 sgd_solver.cpp:106] Iteration 84600, lr = 0.0001
I0113 15:55:36.168866 29875 solver.cpp:229] Iteration 84800, loss = 1.42164
I0113 15:55:36.168927 29875 solver.cpp:245]     Train net output #0: loss = 1.42164 (* 1 = 1.42164 loss)
I0113 15:55:36.168933 29875 sgd_solver.cpp:106] Iteration 84800, lr = 0.0001
I0113 15:56:19.506901 29875 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_85000.caffemodel.h5
I0113 15:56:19.596269 29875 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_85000.solverstate.h5
I0113 15:56:19.682895 29875 solver.cpp:318] Iteration 85000, loss = 0.353646
I0113 15:56:19.682929 29875 solver.cpp:338] Iteration 85000, Testing net (#0)
I0113 15:56:27.737625 29875 solver.cpp:406]     Test net output #0: accuracy = 0.7106
I0113 15:56:27.737671 29875 solver.cpp:406]     Test net output #1: loss = 0.801627 (* 1 = 0.801627 loss)
I0113 15:56:27.737678 29875 solver.cpp:323] Optimization Done.
I0113 15:56:27.737679 29875 caffe.cpp:216] Optimization Done.
I0113 15:56:27.869760 31052 caffe.cpp:185] Using GPUs 0
I0113 15:56:28.333377 31052 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 1000
base_lr: 1e-05
display: 200
max_iter: 95000
lr_policy: "fixed"
momentum: 0.9
weight_decay: 0.004
snapshot: 5000
snapshot_prefix: "krnet_full"
solver_mode: GPU
device_id: 0
net: "krnet_full_train_test.prototxt"
snapshot_format: HDF5
I0113 15:56:28.333503 31052 solver.cpp:91] Creating training net from net file: krnet_full_train_test.prototxt
I0113 15:56:28.333997 31052 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer cifar
I0113 15:56:28.334014 31052 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0113 15:56:28.334209 31052 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TRAIN
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "train_lmdb"
    batch_size: 100
    backend: LMDB
    prefetch: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0113 15:56:28.334317 31052 layer_factory.hpp:77] Creating layer cifar
I0113 15:56:28.335415 31052 net.cpp:106] Creating Layer cifar
I0113 15:56:28.335428 31052 net.cpp:411] cifar -> data
I0113 15:56:28.335453 31052 net.cpp:411] cifar -> label
I0113 15:56:28.335464 31052 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I0113 15:56:28.336015 31056 db_lmdb.cpp:38] Opened lmdb train_lmdb
I0113 15:56:28.444751 31052 data_layer.cpp:41] output data size: 100,3,100,100
I0113 15:56:28.467855 31052 net.cpp:150] Setting up cifar
I0113 15:56:28.467890 31052 net.cpp:157] Top shape: 100 3 100 100 (3000000)
I0113 15:56:28.467905 31052 net.cpp:157] Top shape: 100 (100)
I0113 15:56:28.467907 31052 net.cpp:165] Memory required for data: 12000400
I0113 15:56:28.467918 31052 layer_factory.hpp:77] Creating layer conv1
I0113 15:56:28.467945 31052 net.cpp:106] Creating Layer conv1
I0113 15:56:28.467952 31052 net.cpp:454] conv1 <- data
I0113 15:56:28.467967 31052 net.cpp:411] conv1 -> conv1
I0113 15:56:28.471734 31052 net.cpp:150] Setting up conv1
I0113 15:56:28.471756 31052 net.cpp:157] Top shape: 100 32 100 100 (32000000)
I0113 15:56:28.471760 31052 net.cpp:165] Memory required for data: 140000400
I0113 15:56:28.471776 31052 layer_factory.hpp:77] Creating layer pool1
I0113 15:56:28.471797 31052 net.cpp:106] Creating Layer pool1
I0113 15:56:28.471801 31052 net.cpp:454] pool1 <- conv1
I0113 15:56:28.471808 31052 net.cpp:411] pool1 -> pool1
I0113 15:56:28.471843 31052 net.cpp:150] Setting up pool1
I0113 15:56:28.471849 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.471851 31052 net.cpp:165] Memory required for data: 172000400
I0113 15:56:28.471854 31052 layer_factory.hpp:77] Creating layer relu1
I0113 15:56:28.471861 31052 net.cpp:106] Creating Layer relu1
I0113 15:56:28.471864 31052 net.cpp:454] relu1 <- pool1
I0113 15:56:28.471870 31052 net.cpp:397] relu1 -> pool1 (in-place)
I0113 15:56:28.471880 31052 net.cpp:150] Setting up relu1
I0113 15:56:28.471884 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.471886 31052 net.cpp:165] Memory required for data: 204000400
I0113 15:56:28.471889 31052 layer_factory.hpp:77] Creating layer norm1
I0113 15:56:28.471897 31052 net.cpp:106] Creating Layer norm1
I0113 15:56:28.471900 31052 net.cpp:454] norm1 <- pool1
I0113 15:56:28.471906 31052 net.cpp:411] norm1 -> norm1
I0113 15:56:28.472003 31052 net.cpp:150] Setting up norm1
I0113 15:56:28.472010 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.472013 31052 net.cpp:165] Memory required for data: 236000400
I0113 15:56:28.472017 31052 layer_factory.hpp:77] Creating layer conv2
I0113 15:56:28.472026 31052 net.cpp:106] Creating Layer conv2
I0113 15:56:28.472030 31052 net.cpp:454] conv2 <- norm1
I0113 15:56:28.472038 31052 net.cpp:411] conv2 -> conv2
I0113 15:56:28.475383 31052 net.cpp:150] Setting up conv2
I0113 15:56:28.475392 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.475395 31052 net.cpp:165] Memory required for data: 268000400
I0113 15:56:28.475405 31052 layer_factory.hpp:77] Creating layer relu2
I0113 15:56:28.475411 31052 net.cpp:106] Creating Layer relu2
I0113 15:56:28.475415 31052 net.cpp:454] relu2 <- conv2
I0113 15:56:28.475420 31052 net.cpp:397] relu2 -> conv2 (in-place)
I0113 15:56:28.475426 31052 net.cpp:150] Setting up relu2
I0113 15:56:28.475430 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.475432 31052 net.cpp:165] Memory required for data: 300000400
I0113 15:56:28.475435 31052 layer_factory.hpp:77] Creating layer pool2
I0113 15:56:28.475440 31052 net.cpp:106] Creating Layer pool2
I0113 15:56:28.475442 31052 net.cpp:454] pool2 <- conv2
I0113 15:56:28.475448 31052 net.cpp:411] pool2 -> pool2
I0113 15:56:28.475466 31052 net.cpp:150] Setting up pool2
I0113 15:56:28.475471 31052 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 15:56:28.475474 31052 net.cpp:165] Memory required for data: 308000400
I0113 15:56:28.475476 31052 layer_factory.hpp:77] Creating layer norm2
I0113 15:56:28.475484 31052 net.cpp:106] Creating Layer norm2
I0113 15:56:28.475487 31052 net.cpp:454] norm2 <- pool2
I0113 15:56:28.475493 31052 net.cpp:411] norm2 -> norm2
I0113 15:56:28.475579 31052 net.cpp:150] Setting up norm2
I0113 15:56:28.475585 31052 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 15:56:28.475589 31052 net.cpp:165] Memory required for data: 316000400
I0113 15:56:28.475590 31052 layer_factory.hpp:77] Creating layer conv3
I0113 15:56:28.475600 31052 net.cpp:106] Creating Layer conv3
I0113 15:56:28.475603 31052 net.cpp:454] conv3 <- norm2
I0113 15:56:28.475610 31052 net.cpp:411] conv3 -> conv3
I0113 15:56:28.482429 31052 net.cpp:150] Setting up conv3
I0113 15:56:28.482467 31052 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 15:56:28.482470 31052 net.cpp:165] Memory required for data: 332000400
I0113 15:56:28.482489 31052 layer_factory.hpp:77] Creating layer relu3
I0113 15:56:28.482501 31052 net.cpp:106] Creating Layer relu3
I0113 15:56:28.482507 31052 net.cpp:454] relu3 <- conv3
I0113 15:56:28.482514 31052 net.cpp:397] relu3 -> conv3 (in-place)
I0113 15:56:28.482522 31052 net.cpp:150] Setting up relu3
I0113 15:56:28.482527 31052 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 15:56:28.482528 31052 net.cpp:165] Memory required for data: 348000400
I0113 15:56:28.482532 31052 layer_factory.hpp:77] Creating layer pool3
I0113 15:56:28.482538 31052 net.cpp:106] Creating Layer pool3
I0113 15:56:28.482542 31052 net.cpp:454] pool3 <- conv3
I0113 15:56:28.482547 31052 net.cpp:411] pool3 -> pool3
I0113 15:56:28.482573 31052 net.cpp:150] Setting up pool3
I0113 15:56:28.482578 31052 net.cpp:157] Top shape: 100 64 12 12 (921600)
I0113 15:56:28.482580 31052 net.cpp:165] Memory required for data: 351686800
I0113 15:56:28.482583 31052 layer_factory.hpp:77] Creating layer ip1
I0113 15:56:28.482595 31052 net.cpp:106] Creating Layer ip1
I0113 15:56:28.482599 31052 net.cpp:454] ip1 <- pool3
I0113 15:56:28.482609 31052 net.cpp:411] ip1 -> ip1
I0113 15:56:28.485909 31052 net.cpp:150] Setting up ip1
I0113 15:56:28.485919 31052 net.cpp:157] Top shape: 100 3 (300)
I0113 15:56:28.485923 31052 net.cpp:165] Memory required for data: 351688000
I0113 15:56:28.485929 31052 layer_factory.hpp:77] Creating layer loss
I0113 15:56:28.485936 31052 net.cpp:106] Creating Layer loss
I0113 15:56:28.485939 31052 net.cpp:454] loss <- ip1
I0113 15:56:28.485944 31052 net.cpp:454] loss <- label
I0113 15:56:28.485949 31052 net.cpp:411] loss -> loss
I0113 15:56:28.485960 31052 layer_factory.hpp:77] Creating layer loss
I0113 15:56:28.486023 31052 net.cpp:150] Setting up loss
I0113 15:56:28.486029 31052 net.cpp:157] Top shape: (1)
I0113 15:56:28.486032 31052 net.cpp:160]     with loss weight 1
I0113 15:56:28.486058 31052 net.cpp:165] Memory required for data: 351688004
I0113 15:56:28.486063 31052 net.cpp:226] loss needs backward computation.
I0113 15:56:28.486066 31052 net.cpp:226] ip1 needs backward computation.
I0113 15:56:28.486068 31052 net.cpp:226] pool3 needs backward computation.
I0113 15:56:28.486071 31052 net.cpp:226] relu3 needs backward computation.
I0113 15:56:28.486074 31052 net.cpp:226] conv3 needs backward computation.
I0113 15:56:28.486076 31052 net.cpp:226] norm2 needs backward computation.
I0113 15:56:28.486079 31052 net.cpp:226] pool2 needs backward computation.
I0113 15:56:28.486083 31052 net.cpp:226] relu2 needs backward computation.
I0113 15:56:28.486085 31052 net.cpp:226] conv2 needs backward computation.
I0113 15:56:28.486088 31052 net.cpp:226] norm1 needs backward computation.
I0113 15:56:28.486091 31052 net.cpp:226] relu1 needs backward computation.
I0113 15:56:28.486093 31052 net.cpp:226] pool1 needs backward computation.
I0113 15:56:28.486096 31052 net.cpp:226] conv1 needs backward computation.
I0113 15:56:28.486100 31052 net.cpp:228] cifar does not need backward computation.
I0113 15:56:28.486102 31052 net.cpp:270] This network produces output loss
I0113 15:56:28.486116 31052 net.cpp:283] Network initialization done.
I0113 15:56:28.486547 31052 solver.cpp:181] Creating test net (#0) specified by net file: krnet_full_train_test.prototxt
I0113 15:56:28.486588 31052 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer cifar
I0113 15:56:28.486819 31052 net.cpp:49] Initializing net from parameters: 
name: "CIFAR10_full"
state {
  phase: TEST
}
layer {
  name: "cifar"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mean_file: "mean.binaryproto"
  }
  data_param {
    source: "test_lmdb"
    batch_size: 100
    backend: LMDB
    prefetch: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 250
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0113 15:56:28.486937 31052 layer_factory.hpp:77] Creating layer cifar
I0113 15:56:28.487623 31052 net.cpp:106] Creating Layer cifar
I0113 15:56:28.487638 31052 net.cpp:411] cifar -> data
I0113 15:56:28.487655 31052 net.cpp:411] cifar -> label
I0113 15:56:28.487668 31052 data_transformer.cpp:25] Loading mean file from: mean.binaryproto
I0113 15:56:28.488373 31058 db_lmdb.cpp:38] Opened lmdb test_lmdb
I0113 15:56:28.489007 31052 data_layer.cpp:41] output data size: 100,3,100,100
I0113 15:56:28.506369 31052 net.cpp:150] Setting up cifar
I0113 15:56:28.506417 31052 net.cpp:157] Top shape: 100 3 100 100 (3000000)
I0113 15:56:28.506428 31052 net.cpp:157] Top shape: 100 (100)
I0113 15:56:28.506433 31052 net.cpp:165] Memory required for data: 12000400
I0113 15:56:28.506443 31052 layer_factory.hpp:77] Creating layer label_cifar_1_split
I0113 15:56:28.506469 31052 net.cpp:106] Creating Layer label_cifar_1_split
I0113 15:56:28.506479 31052 net.cpp:454] label_cifar_1_split <- label
I0113 15:56:28.506495 31052 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_0
I0113 15:56:28.506515 31052 net.cpp:411] label_cifar_1_split -> label_cifar_1_split_1
I0113 15:56:28.506573 31052 net.cpp:150] Setting up label_cifar_1_split
I0113 15:56:28.506584 31052 net.cpp:157] Top shape: 100 (100)
I0113 15:56:28.506590 31052 net.cpp:157] Top shape: 100 (100)
I0113 15:56:28.506595 31052 net.cpp:165] Memory required for data: 12001200
I0113 15:56:28.506600 31052 layer_factory.hpp:77] Creating layer conv1
I0113 15:56:28.506624 31052 net.cpp:106] Creating Layer conv1
I0113 15:56:28.506631 31052 net.cpp:454] conv1 <- data
I0113 15:56:28.506645 31052 net.cpp:411] conv1 -> conv1
I0113 15:56:28.509868 31052 net.cpp:150] Setting up conv1
I0113 15:56:28.509935 31052 net.cpp:157] Top shape: 100 32 100 100 (32000000)
I0113 15:56:28.509963 31052 net.cpp:165] Memory required for data: 140001200
I0113 15:56:28.509995 31052 layer_factory.hpp:77] Creating layer pool1
I0113 15:56:28.510020 31052 net.cpp:106] Creating Layer pool1
I0113 15:56:28.510028 31052 net.cpp:454] pool1 <- conv1
I0113 15:56:28.510043 31052 net.cpp:411] pool1 -> pool1
I0113 15:56:28.510097 31052 net.cpp:150] Setting up pool1
I0113 15:56:28.510107 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.510133 31052 net.cpp:165] Memory required for data: 172001200
I0113 15:56:28.510139 31052 layer_factory.hpp:77] Creating layer relu1
I0113 15:56:28.510152 31052 net.cpp:106] Creating Layer relu1
I0113 15:56:28.510159 31052 net.cpp:454] relu1 <- pool1
I0113 15:56:28.510169 31052 net.cpp:397] relu1 -> pool1 (in-place)
I0113 15:56:28.510180 31052 net.cpp:150] Setting up relu1
I0113 15:56:28.510187 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.510191 31052 net.cpp:165] Memory required for data: 204001200
I0113 15:56:28.510196 31052 layer_factory.hpp:77] Creating layer norm1
I0113 15:56:28.510208 31052 net.cpp:106] Creating Layer norm1
I0113 15:56:28.510215 31052 net.cpp:454] norm1 <- pool1
I0113 15:56:28.510237 31052 net.cpp:411] norm1 -> norm1
I0113 15:56:28.510382 31052 net.cpp:150] Setting up norm1
I0113 15:56:28.510395 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.510398 31052 net.cpp:165] Memory required for data: 236001200
I0113 15:56:28.510403 31052 layer_factory.hpp:77] Creating layer conv2
I0113 15:56:28.510424 31052 net.cpp:106] Creating Layer conv2
I0113 15:56:28.510432 31052 net.cpp:454] conv2 <- norm1
I0113 15:56:28.510444 31052 net.cpp:411] conv2 -> conv2
I0113 15:56:28.515326 31052 net.cpp:150] Setting up conv2
I0113 15:56:28.515370 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.515377 31052 net.cpp:165] Memory required for data: 268001200
I0113 15:56:28.515404 31052 layer_factory.hpp:77] Creating layer relu2
I0113 15:56:28.515424 31052 net.cpp:106] Creating Layer relu2
I0113 15:56:28.515434 31052 net.cpp:454] relu2 <- conv2
I0113 15:56:28.515447 31052 net.cpp:397] relu2 -> conv2 (in-place)
I0113 15:56:28.515462 31052 net.cpp:150] Setting up relu2
I0113 15:56:28.515470 31052 net.cpp:157] Top shape: 100 32 50 50 (8000000)
I0113 15:56:28.515475 31052 net.cpp:165] Memory required for data: 300001200
I0113 15:56:28.515480 31052 layer_factory.hpp:77] Creating layer pool2
I0113 15:56:28.515497 31052 net.cpp:106] Creating Layer pool2
I0113 15:56:28.515503 31052 net.cpp:454] pool2 <- conv2
I0113 15:56:28.515516 31052 net.cpp:411] pool2 -> pool2
I0113 15:56:28.515552 31052 net.cpp:150] Setting up pool2
I0113 15:56:28.515563 31052 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 15:56:28.515568 31052 net.cpp:165] Memory required for data: 308001200
I0113 15:56:28.515573 31052 layer_factory.hpp:77] Creating layer norm2
I0113 15:56:28.515586 31052 net.cpp:106] Creating Layer norm2
I0113 15:56:28.515593 31052 net.cpp:454] norm2 <- pool2
I0113 15:56:28.515604 31052 net.cpp:411] norm2 -> norm2
I0113 15:56:28.515727 31052 net.cpp:150] Setting up norm2
I0113 15:56:28.515739 31052 net.cpp:157] Top shape: 100 32 25 25 (2000000)
I0113 15:56:28.515744 31052 net.cpp:165] Memory required for data: 316001200
I0113 15:56:28.515749 31052 layer_factory.hpp:77] Creating layer conv3
I0113 15:56:28.515768 31052 net.cpp:106] Creating Layer conv3
I0113 15:56:28.515774 31052 net.cpp:454] conv3 <- norm2
I0113 15:56:28.515787 31052 net.cpp:411] conv3 -> conv3
I0113 15:56:28.524889 31052 net.cpp:150] Setting up conv3
I0113 15:56:28.524936 31052 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 15:56:28.524943 31052 net.cpp:165] Memory required for data: 332001200
I0113 15:56:28.524972 31052 layer_factory.hpp:77] Creating layer relu3
I0113 15:56:28.524992 31052 net.cpp:106] Creating Layer relu3
I0113 15:56:28.525002 31052 net.cpp:454] relu3 <- conv3
I0113 15:56:28.525015 31052 net.cpp:397] relu3 -> conv3 (in-place)
I0113 15:56:28.525050 31052 net.cpp:150] Setting up relu3
I0113 15:56:28.525059 31052 net.cpp:157] Top shape: 100 64 25 25 (4000000)
I0113 15:56:28.525063 31052 net.cpp:165] Memory required for data: 348001200
I0113 15:56:28.525069 31052 layer_factory.hpp:77] Creating layer pool3
I0113 15:56:28.525082 31052 net.cpp:106] Creating Layer pool3
I0113 15:56:28.525089 31052 net.cpp:454] pool3 <- conv3
I0113 15:56:28.525100 31052 net.cpp:411] pool3 -> pool3
I0113 15:56:28.525135 31052 net.cpp:150] Setting up pool3
I0113 15:56:28.525146 31052 net.cpp:157] Top shape: 100 64 12 12 (921600)
I0113 15:56:28.525151 31052 net.cpp:165] Memory required for data: 351687600
I0113 15:56:28.525156 31052 layer_factory.hpp:77] Creating layer ip1
I0113 15:56:28.525171 31052 net.cpp:106] Creating Layer ip1
I0113 15:56:28.525177 31052 net.cpp:454] ip1 <- pool3
I0113 15:56:28.525190 31052 net.cpp:411] ip1 -> ip1
I0113 15:56:28.530138 31052 net.cpp:150] Setting up ip1
I0113 15:56:28.530182 31052 net.cpp:157] Top shape: 100 3 (300)
I0113 15:56:28.530189 31052 net.cpp:165] Memory required for data: 351688800
I0113 15:56:28.530206 31052 layer_factory.hpp:77] Creating layer ip1_ip1_0_split
I0113 15:56:28.530228 31052 net.cpp:106] Creating Layer ip1_ip1_0_split
I0113 15:56:28.530241 31052 net.cpp:454] ip1_ip1_0_split <- ip1
I0113 15:56:28.530266 31052 net.cpp:411] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0113 15:56:28.530283 31052 net.cpp:411] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0113 15:56:28.530333 31052 net.cpp:150] Setting up ip1_ip1_0_split
I0113 15:56:28.530344 31052 net.cpp:157] Top shape: 100 3 (300)
I0113 15:56:28.530350 31052 net.cpp:157] Top shape: 100 3 (300)
I0113 15:56:28.530354 31052 net.cpp:165] Memory required for data: 351691200
I0113 15:56:28.530360 31052 layer_factory.hpp:77] Creating layer accuracy
I0113 15:56:28.530376 31052 net.cpp:106] Creating Layer accuracy
I0113 15:56:28.530383 31052 net.cpp:454] accuracy <- ip1_ip1_0_split_0
I0113 15:56:28.530393 31052 net.cpp:454] accuracy <- label_cifar_1_split_0
I0113 15:56:28.530405 31052 net.cpp:411] accuracy -> accuracy
I0113 15:56:28.530419 31052 net.cpp:150] Setting up accuracy
I0113 15:56:28.530428 31052 net.cpp:157] Top shape: (1)
I0113 15:56:28.530432 31052 net.cpp:165] Memory required for data: 351691204
I0113 15:56:28.530438 31052 layer_factory.hpp:77] Creating layer loss
I0113 15:56:28.530457 31052 net.cpp:106] Creating Layer loss
I0113 15:56:28.530462 31052 net.cpp:454] loss <- ip1_ip1_0_split_1
I0113 15:56:28.530472 31052 net.cpp:454] loss <- label_cifar_1_split_1
I0113 15:56:28.530481 31052 net.cpp:411] loss -> loss
I0113 15:56:28.530500 31052 layer_factory.hpp:77] Creating layer loss
I0113 15:56:28.530607 31052 net.cpp:150] Setting up loss
I0113 15:56:28.530618 31052 net.cpp:157] Top shape: (1)
I0113 15:56:28.530623 31052 net.cpp:160]     with loss weight 1
I0113 15:56:28.530637 31052 net.cpp:165] Memory required for data: 351691208
I0113 15:56:28.530644 31052 net.cpp:226] loss needs backward computation.
I0113 15:56:28.530652 31052 net.cpp:228] accuracy does not need backward computation.
I0113 15:56:28.530658 31052 net.cpp:226] ip1_ip1_0_split needs backward computation.
I0113 15:56:28.530663 31052 net.cpp:226] ip1 needs backward computation.
I0113 15:56:28.530668 31052 net.cpp:226] pool3 needs backward computation.
I0113 15:56:28.530673 31052 net.cpp:226] relu3 needs backward computation.
I0113 15:56:28.530678 31052 net.cpp:226] conv3 needs backward computation.
I0113 15:56:28.530683 31052 net.cpp:226] norm2 needs backward computation.
I0113 15:56:28.530689 31052 net.cpp:226] pool2 needs backward computation.
I0113 15:56:28.530694 31052 net.cpp:226] relu2 needs backward computation.
I0113 15:56:28.530699 31052 net.cpp:226] conv2 needs backward computation.
I0113 15:56:28.530704 31052 net.cpp:226] norm1 needs backward computation.
I0113 15:56:28.530709 31052 net.cpp:226] relu1 needs backward computation.
I0113 15:56:28.530714 31052 net.cpp:226] pool1 needs backward computation.
I0113 15:56:28.530719 31052 net.cpp:226] conv1 needs backward computation.
I0113 15:56:28.530740 31052 net.cpp:228] label_cifar_1_split does not need backward computation.
I0113 15:56:28.530747 31052 net.cpp:228] cifar does not need backward computation.
I0113 15:56:28.530751 31052 net.cpp:270] This network produces output accuracy
I0113 15:56:28.530758 31052 net.cpp:270] This network produces output loss
I0113 15:56:28.530782 31052 net.cpp:283] Network initialization done.
I0113 15:56:28.530874 31052 solver.cpp:60] Solver scaffolding done.
I0113 15:56:28.531179 31052 caffe.cpp:203] Resuming from krnet_full_iter_85000.solverstate.h5
I0113 15:56:28.532914 31052 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0113 15:56:28.534598 31052 caffe.cpp:213] Starting Optimization
I0113 15:56:28.534627 31052 solver.cpp:280] Solving CIFAR10_full
I0113 15:56:28.534633 31052 solver.cpp:281] Learning Rate Policy: fixed
I0113 15:56:28.535502 31052 solver.cpp:338] Iteration 85000, Testing net (#0)
I0113 15:56:28.536104 31052 blocking_queue.cpp:50] Data layer prefetch queue empty
I0113 15:56:36.464529 31052 solver.cpp:406]     Test net output #0: accuracy = 0.6526
I0113 15:56:36.464568 31052 solver.cpp:406]     Test net output #1: loss = 0.891275 (* 1 = 0.891275 loss)
I0113 15:56:36.582702 31052 solver.cpp:229] Iteration 85000, loss = 0.35365
I0113 15:56:36.582741 31052 solver.cpp:245]     Train net output #0: loss = 0.35365 (* 1 = 0.35365 loss)
I0113 15:56:36.582756 31052 sgd_solver.cpp:106] Iteration 85000, lr = 1e-05
I0113 15:57:14.014806 31052 solver.cpp:229] Iteration 85200, loss = 0.670746
I0113 15:57:14.014880 31052 solver.cpp:245]     Train net output #0: loss = 0.670746 (* 1 = 0.670746 loss)
I0113 15:57:14.014889 31052 sgd_solver.cpp:106] Iteration 85200, lr = 1e-05
I0113 15:57:51.459844 31052 solver.cpp:229] Iteration 85400, loss = 0.492523
I0113 15:57:51.459900 31052 solver.cpp:245]     Train net output #0: loss = 0.492523 (* 1 = 0.492523 loss)
I0113 15:57:51.459906 31052 sgd_solver.cpp:106] Iteration 85400, lr = 1e-05
I0113 15:58:28.883965 31052 solver.cpp:229] Iteration 85600, loss = 0.777399
I0113 15:58:28.884018 31052 solver.cpp:245]     Train net output #0: loss = 0.777399 (* 1 = 0.777399 loss)
I0113 15:58:28.884026 31052 sgd_solver.cpp:106] Iteration 85600, lr = 1e-05
I0113 15:59:12.108980 31052 solver.cpp:229] Iteration 85800, loss = 1.20827
I0113 15:59:12.109046 31052 solver.cpp:245]     Train net output #0: loss = 1.20827 (* 1 = 1.20827 loss)
I0113 15:59:12.109052 31052 sgd_solver.cpp:106] Iteration 85800, lr = 1e-05
I0113 15:59:55.448267 31052 solver.cpp:338] Iteration 86000, Testing net (#0)
I0113 16:00:03.609122 31052 solver.cpp:406]     Test net output #0: accuracy = 0.7144
I0113 16:00:03.609164 31052 solver.cpp:406]     Test net output #1: loss = 0.796101 (* 1 = 0.796101 loss)
I0113 16:00:03.734642 31052 solver.cpp:229] Iteration 86000, loss = 0.361603
I0113 16:00:03.734678 31052 solver.cpp:245]     Train net output #0: loss = 0.361603 (* 1 = 0.361603 loss)
I0113 16:00:03.734683 31052 sgd_solver.cpp:106] Iteration 86000, lr = 1e-05
I0113 16:00:47.305369 31052 solver.cpp:229] Iteration 86200, loss = 0.97279
I0113 16:00:47.305419 31052 solver.cpp:245]     Train net output #0: loss = 0.972789 (* 1 = 0.972789 loss)
I0113 16:00:47.305424 31052 sgd_solver.cpp:106] Iteration 86200, lr = 1e-05
I0113 16:01:30.888283 31052 solver.cpp:229] Iteration 86400, loss = 1.44673
I0113 16:01:30.888341 31052 solver.cpp:245]     Train net output #0: loss = 1.44673 (* 1 = 1.44673 loss)
I0113 16:01:30.888347 31052 sgd_solver.cpp:106] Iteration 86400, lr = 1e-05
I0113 16:02:13.457932 31052 solver.cpp:229] Iteration 86600, loss = 0.36536
I0113 16:02:13.458015 31052 solver.cpp:245]     Train net output #0: loss = 0.365361 (* 1 = 0.365361 loss)
I0113 16:02:13.458022 31052 sgd_solver.cpp:106] Iteration 86600, lr = 1e-05
I0113 16:02:50.902678 31052 solver.cpp:229] Iteration 86800, loss = 0.777398
I0113 16:02:50.902734 31052 solver.cpp:245]     Train net output #0: loss = 0.777398 (* 1 = 0.777398 loss)
I0113 16:02:50.902740 31052 sgd_solver.cpp:106] Iteration 86800, lr = 1e-05
I0113 16:03:28.159898 31052 solver.cpp:338] Iteration 87000, Testing net (#0)
I0113 16:03:34.966661 31052 solver.cpp:406]     Test net output #0: accuracy = 0.7077
I0113 16:03:34.966701 31052 solver.cpp:406]     Test net output #1: loss = 0.807519 (* 1 = 0.807519 loss)
I0113 16:03:35.065312 31052 solver.cpp:229] Iteration 87000, loss = 1.4683
I0113 16:03:35.065358 31052 solver.cpp:245]     Train net output #0: loss = 1.4683 (* 1 = 1.4683 loss)
I0113 16:03:35.065364 31052 sgd_solver.cpp:106] Iteration 87000, lr = 1e-05
I0113 16:04:14.297562 31052 solver.cpp:229] Iteration 87200, loss = 0.691726
I0113 16:04:14.297617 31052 solver.cpp:245]     Train net output #0: loss = 0.691726 (* 1 = 0.691726 loss)
I0113 16:04:14.297623 31052 sgd_solver.cpp:106] Iteration 87200, lr = 1e-05
I0113 16:04:57.837376 31052 solver.cpp:229] Iteration 87400, loss = 0.944662
I0113 16:04:57.837435 31052 solver.cpp:245]     Train net output #0: loss = 0.944661 (* 1 = 0.944661 loss)
I0113 16:04:57.837442 31052 sgd_solver.cpp:106] Iteration 87400, lr = 1e-05
I0113 16:05:41.381563 31052 solver.cpp:229] Iteration 87600, loss = 1.7207
I0113 16:05:41.381623 31052 solver.cpp:245]     Train net output #0: loss = 1.7207 (* 1 = 1.7207 loss)
I0113 16:05:41.381629 31052 sgd_solver.cpp:106] Iteration 87600, lr = 1e-05
I0113 16:06:24.923658 31052 solver.cpp:229] Iteration 87800, loss = 0.366383
I0113 16:06:24.923789 31052 solver.cpp:245]     Train net output #0: loss = 0.366382 (* 1 = 0.366382 loss)
I0113 16:06:24.923797 31052 sgd_solver.cpp:106] Iteration 87800, lr = 1e-05
I0113 16:07:08.251956 31052 solver.cpp:338] Iteration 88000, Testing net (#0)
I0113 16:07:16.395692 31052 solver.cpp:406]     Test net output #0: accuracy = 0.5951
I0113 16:07:16.395735 31052 solver.cpp:406]     Test net output #1: loss = 0.975205 (* 1 = 0.975205 loss)
I0113 16:07:16.525131 31052 solver.cpp:229] Iteration 88000, loss = 1.52446
I0113 16:07:16.525172 31052 solver.cpp:245]     Train net output #0: loss = 1.52446 (* 1 = 1.52446 loss)
I0113 16:07:16.525178 31052 sgd_solver.cpp:106] Iteration 88000, lr = 1e-05
I0113 16:07:56.938616 31052 solver.cpp:229] Iteration 88200, loss = 1.3375
I0113 16:07:56.938674 31052 solver.cpp:245]     Train net output #0: loss = 1.3375 (* 1 = 1.3375 loss)
I0113 16:07:56.938680 31052 sgd_solver.cpp:106] Iteration 88200, lr = 1e-05
I0113 16:08:34.368513 31052 solver.cpp:229] Iteration 88400, loss = 1.65218
I0113 16:08:34.368569 31052 solver.cpp:245]     Train net output #0: loss = 1.65217 (* 1 = 1.65217 loss)
I0113 16:08:34.368577 31052 sgd_solver.cpp:106] Iteration 88400, lr = 1e-05
I0113 16:09:11.788055 31052 solver.cpp:229] Iteration 88600, loss = 1.23202
I0113 16:09:11.788108 31052 solver.cpp:245]     Train net output #0: loss = 1.23202 (* 1 = 1.23202 loss)
I0113 16:09:11.788115 31052 sgd_solver.cpp:106] Iteration 88600, lr = 1e-05
I0113 16:09:51.824954 31052 solver.cpp:229] Iteration 88800, loss = 0.491514
I0113 16:09:51.825002 31052 solver.cpp:245]     Train net output #0: loss = 0.491513 (* 1 = 0.491513 loss)
I0113 16:09:51.825007 31052 sgd_solver.cpp:106] Iteration 88800, lr = 1e-05
I0113 16:10:35.148954 31052 solver.cpp:338] Iteration 89000, Testing net (#0)
I0113 16:10:43.299875 31052 solver.cpp:406]     Test net output #0: accuracy = 0.6975
I0113 16:10:43.299921 31052 solver.cpp:406]     Test net output #1: loss = 0.821534 (* 1 = 0.821534 loss)
I0113 16:10:43.425577 31052 solver.cpp:229] Iteration 89000, loss = 1.58628
I0113 16:10:43.425616 31052 solver.cpp:245]     Train net output #0: loss = 1.58628 (* 1 = 1.58628 loss)
I0113 16:10:43.425621 31052 sgd_solver.cpp:106] Iteration 89000, lr = 1e-05
I0113 16:11:27.000473 31052 solver.cpp:229] Iteration 89200, loss = 1.3427
I0113 16:11:27.000521 31052 solver.cpp:245]     Train net output #0: loss = 1.3427 (* 1 = 1.3427 loss)
I0113 16:11:27.000526 31052 sgd_solver.cpp:106] Iteration 89200, lr = 1e-05
I0113 16:12:10.574795 31052 solver.cpp:229] Iteration 89400, loss = 0.404681
I0113 16:12:10.574903 31052 solver.cpp:245]     Train net output #0: loss = 0.40468 (* 1 = 0.40468 loss)
I0113 16:12:10.574909 31052 sgd_solver.cpp:106] Iteration 89400, lr = 1e-05
I0113 16:12:54.154968 31052 solver.cpp:229] Iteration 89600, loss = 0.734793
I0113 16:12:54.155022 31052 solver.cpp:245]     Train net output #0: loss = 0.734792 (* 1 = 0.734792 loss)
I0113 16:12:54.155028 31052 sgd_solver.cpp:106] Iteration 89600, lr = 1e-05
I0113 16:13:33.763600 31052 solver.cpp:229] Iteration 89800, loss = 0.376836
I0113 16:13:33.763656 31052 solver.cpp:245]     Train net output #0: loss = 0.376835 (* 1 = 0.376835 loss)
I0113 16:13:33.763662 31052 sgd_solver.cpp:106] Iteration 89800, lr = 1e-05
I0113 16:14:11.007562 31052 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_90000.caffemodel.h5
I0113 16:14:11.092968 31052 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_90000.solverstate.h5
I0113 16:14:11.093721 31052 solver.cpp:338] Iteration 90000, Testing net (#0)
I0113 16:14:17.824641 31052 solver.cpp:406]     Test net output #0: accuracy = 0.7298
I0113 16:14:17.824684 31052 solver.cpp:406]     Test net output #1: loss = 0.775088 (* 1 = 0.775088 loss)
I0113 16:14:17.938874 31052 solver.cpp:229] Iteration 90000, loss = 0.373726
I0113 16:14:17.938915 31052 solver.cpp:245]     Train net output #0: loss = 0.373725 (* 1 = 0.373725 loss)
I0113 16:14:17.938921 31052 sgd_solver.cpp:106] Iteration 90000, lr = 1e-05
I0113 16:14:55.340488 31052 solver.cpp:229] Iteration 90200, loss = 1.65497
I0113 16:14:55.340543 31052 solver.cpp:245]     Train net output #0: loss = 1.65497 (* 1 = 1.65497 loss)
I0113 16:14:55.340549 31052 sgd_solver.cpp:106] Iteration 90200, lr = 1e-05
I0113 16:15:37.512236 31052 solver.cpp:229] Iteration 90400, loss = 1.39225
I0113 16:15:37.512293 31052 solver.cpp:245]     Train net output #0: loss = 1.39225 (* 1 = 1.39225 loss)
I0113 16:15:37.512300 31052 sgd_solver.cpp:106] Iteration 90400, lr = 1e-05
I0113 16:16:21.045445 31052 solver.cpp:229] Iteration 90600, loss = 1.2825
I0113 16:16:21.045503 31052 solver.cpp:245]     Train net output #0: loss = 1.2825 (* 1 = 1.2825 loss)
I0113 16:16:21.045509 31052 sgd_solver.cpp:106] Iteration 90600, lr = 1e-05
I0113 16:17:04.599396 31052 solver.cpp:229] Iteration 90800, loss = 1.59548
I0113 16:17:04.599462 31052 solver.cpp:245]     Train net output #0: loss = 1.59548 (* 1 = 1.59548 loss)
I0113 16:17:04.599468 31052 sgd_solver.cpp:106] Iteration 90800, lr = 1e-05
I0113 16:17:47.955109 31052 solver.cpp:338] Iteration 91000, Testing net (#0)
I0113 16:17:56.108043 31052 solver.cpp:406]     Test net output #0: accuracy = 0.6482
I0113 16:17:56.108083 31052 solver.cpp:406]     Test net output #1: loss = 0.894547 (* 1 = 0.894547 loss)
I0113 16:17:56.226531 31052 solver.cpp:229] Iteration 91000, loss = 1.41723
I0113 16:17:56.226572 31052 solver.cpp:245]     Train net output #0: loss = 1.41723 (* 1 = 1.41723 loss)
I0113 16:17:56.226577 31052 sgd_solver.cpp:106] Iteration 91000, lr = 1e-05
I0113 16:18:39.793648 31052 solver.cpp:229] Iteration 91200, loss = 1.14655
I0113 16:18:39.793699 31052 solver.cpp:245]     Train net output #0: loss = 1.14655 (* 1 = 1.14655 loss)
I0113 16:18:39.793704 31052 sgd_solver.cpp:106] Iteration 91200, lr = 1e-05
I0113 16:19:17.272200 31052 solver.cpp:229] Iteration 91400, loss = 0.890063
I0113 16:19:17.272259 31052 solver.cpp:245]     Train net output #0: loss = 0.890062 (* 1 = 0.890062 loss)
I0113 16:19:17.272265 31052 sgd_solver.cpp:106] Iteration 91400, lr = 1e-05
I0113 16:19:54.690629 31052 solver.cpp:229] Iteration 91600, loss = 0.379517
I0113 16:19:54.690703 31052 solver.cpp:245]     Train net output #0: loss = 0.379515 (* 1 = 0.379515 loss)
I0113 16:19:54.690709 31052 sgd_solver.cpp:106] Iteration 91600, lr = 1e-05
I0113 16:20:32.106173 31052 solver.cpp:229] Iteration 91800, loss = 1.53861
I0113 16:20:32.106232 31052 solver.cpp:245]     Train net output #0: loss = 1.53861 (* 1 = 1.53861 loss)
I0113 16:20:32.106240 31052 sgd_solver.cpp:106] Iteration 91800, lr = 1e-05
I0113 16:21:14.870683 31052 solver.cpp:338] Iteration 92000, Testing net (#0)
I0113 16:21:23.023267 31052 solver.cpp:406]     Test net output #0: accuracy = 0.6384
I0113 16:21:23.023310 31052 solver.cpp:406]     Test net output #1: loss = 0.908042 (* 1 = 0.908042 loss)
I0113 16:21:23.148794 31052 solver.cpp:229] Iteration 92000, loss = 0.667597
I0113 16:21:23.148831 31052 solver.cpp:245]     Train net output #0: loss = 0.667595 (* 1 = 0.667595 loss)
I0113 16:21:23.148836 31052 sgd_solver.cpp:106] Iteration 92000, lr = 1e-05
I0113 16:22:06.704074 31052 solver.cpp:229] Iteration 92200, loss = 0.71169
I0113 16:22:06.704149 31052 solver.cpp:245]     Train net output #0: loss = 0.711689 (* 1 = 0.711689 loss)
I0113 16:22:06.704154 31052 sgd_solver.cpp:106] Iteration 92200, lr = 1e-05
I0113 16:22:50.270632 31052 solver.cpp:229] Iteration 92400, loss = 1.39591
I0113 16:22:50.270681 31052 solver.cpp:245]     Train net output #0: loss = 1.39591 (* 1 = 1.39591 loss)
I0113 16:22:50.270687 31052 sgd_solver.cpp:106] Iteration 92400, lr = 1e-05
I0113 16:23:33.830485 31052 solver.cpp:229] Iteration 92600, loss = 1.63248
I0113 16:23:33.830534 31052 solver.cpp:245]     Train net output #0: loss = 1.63248 (* 1 = 1.63248 loss)
I0113 16:23:33.830540 31052 sgd_solver.cpp:106] Iteration 92600, lr = 1e-05
I0113 16:24:16.617410 31052 solver.cpp:229] Iteration 92800, loss = 1.3107
I0113 16:24:16.617461 31052 solver.cpp:245]     Train net output #0: loss = 1.3107 (* 1 = 1.3107 loss)
I0113 16:24:16.617467 31052 sgd_solver.cpp:106] Iteration 92800, lr = 1e-05
I0113 16:24:53.839324 31052 solver.cpp:338] Iteration 93000, Testing net (#0)
I0113 16:25:00.646203 31052 solver.cpp:406]     Test net output #0: accuracy = 0.7131
I0113 16:25:00.646245 31052 solver.cpp:406]     Test net output #1: loss = 0.799556 (* 1 = 0.799556 loss)
I0113 16:25:00.744951 31052 solver.cpp:229] Iteration 93000, loss = 0.93922
I0113 16:25:00.744995 31052 solver.cpp:245]     Train net output #0: loss = 0.939218 (* 1 = 0.939218 loss)
I0113 16:25:00.745002 31052 sgd_solver.cpp:106] Iteration 93000, lr = 1e-05
I0113 16:25:38.162308 31052 solver.cpp:229] Iteration 93200, loss = 0.801773
I0113 16:25:38.162369 31052 solver.cpp:245]     Train net output #0: loss = 0.80177 (* 1 = 0.80177 loss)
I0113 16:25:38.162374 31052 sgd_solver.cpp:106] Iteration 93200, lr = 1e-05
I0113 16:26:17.141002 31052 solver.cpp:229] Iteration 93400, loss = 1.39796
I0113 16:26:17.141052 31052 solver.cpp:245]     Train net output #0: loss = 1.39796 (* 1 = 1.39796 loss)
I0113 16:26:17.141057 31052 sgd_solver.cpp:106] Iteration 93400, lr = 1e-05
I0113 16:27:00.703114 31052 solver.cpp:229] Iteration 93600, loss = 1.0718
I0113 16:27:00.703164 31052 solver.cpp:245]     Train net output #0: loss = 1.0718 (* 1 = 1.0718 loss)
I0113 16:27:00.703171 31052 sgd_solver.cpp:106] Iteration 93600, lr = 1e-05
I0113 16:27:44.246170 31052 solver.cpp:229] Iteration 93800, loss = 0.383047
I0113 16:27:44.246233 31052 solver.cpp:245]     Train net output #0: loss = 0.383045 (* 1 = 0.383045 loss)
I0113 16:27:44.246240 31052 sgd_solver.cpp:106] Iteration 93800, lr = 1e-05
I0113 16:28:27.575183 31052 solver.cpp:338] Iteration 94000, Testing net (#0)
I0113 16:28:35.724951 31052 solver.cpp:406]     Test net output #0: accuracy = 0.7112
I0113 16:28:35.724992 31052 solver.cpp:406]     Test net output #1: loss = 0.803348 (* 1 = 0.803348 loss)
I0113 16:28:35.854529 31052 solver.cpp:229] Iteration 94000, loss = 0.51543
I0113 16:28:35.854570 31052 solver.cpp:245]     Train net output #0: loss = 0.515428 (* 1 = 0.515428 loss)
I0113 16:28:35.854578 31052 sgd_solver.cpp:106] Iteration 94000, lr = 1e-05
I0113 16:29:19.430137 31052 solver.cpp:229] Iteration 94200, loss = 0.770134
I0113 16:29:19.430224 31052 solver.cpp:245]     Train net output #0: loss = 0.770131 (* 1 = 0.770131 loss)
I0113 16:29:19.430233 31052 sgd_solver.cpp:106] Iteration 94200, lr = 1e-05
I0113 16:30:00.104631 31052 solver.cpp:229] Iteration 94400, loss = 0.40988
I0113 16:30:00.104687 31052 solver.cpp:245]     Train net output #0: loss = 0.409877 (* 1 = 0.409877 loss)
I0113 16:30:00.104693 31052 sgd_solver.cpp:106] Iteration 94400, lr = 1e-05
I0113 16:30:37.518451 31052 solver.cpp:229] Iteration 94600, loss = 0.426547
I0113 16:30:37.518546 31052 solver.cpp:245]     Train net output #0: loss = 0.426545 (* 1 = 0.426545 loss)
I0113 16:30:37.518553 31052 sgd_solver.cpp:106] Iteration 94600, lr = 1e-05
I0113 16:31:14.939987 31052 solver.cpp:229] Iteration 94800, loss = 0.48338
I0113 16:31:14.940059 31052 solver.cpp:245]     Train net output #0: loss = 0.483377 (* 1 = 0.483377 loss)
I0113 16:31:14.940065 31052 sgd_solver.cpp:106] Iteration 94800, lr = 1e-05
I0113 16:31:54.486054 31052 solver.cpp:466] Snapshotting to HDF5 file krnet_full_iter_95000.caffemodel.h5
I0113 16:31:54.576418 31052 sgd_solver.cpp:283] Snapshotting solver state to HDF5 file krnet_full_iter_95000.solverstate.h5
I0113 16:31:54.661860 31052 solver.cpp:318] Iteration 95000, loss = 0.515962
I0113 16:31:54.661893 31052 solver.cpp:338] Iteration 95000, Testing net (#0)
I0113 16:32:02.726817 31052 solver.cpp:406]     Test net output #0: accuracy = 0.599
I0113 16:32:02.726866 31052 solver.cpp:406]     Test net output #1: loss = 0.965745 (* 1 = 0.965745 loss)
I0113 16:32:02.726871 31052 solver.cpp:323] Optimization Done.
I0113 16:32:02.726872 31052 caffe.cpp:216] Optimization Done.
